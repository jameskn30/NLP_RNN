{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from d2l import torch as d2l\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens = [], min_freq = 0, reserved_tokens = []):\n",
    "        self._build(tokens, min_freq, reserved_tokens)\n",
    "    \n",
    "    def _build(self, tokens, min_freq, reserved_tokens):\n",
    "        print(f'building vocab from {len(tokens)} tokens')\n",
    "        counter = Counter(tokens)\n",
    "        self.token_freq = sorted(counter.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        self.idx_to_tokens = list(sorted(set(['<unk>'] + reserved_tokens \\\n",
    "        + [ token for token, freq in self.token_freq if freq > min_freq])))\n",
    "    \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_tokens)}\n",
    "\n",
    "        print('built vocab object')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_tokens)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        #if not type list or tuple\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(t) for t in tokens]\n",
    "    \n",
    "    def to_tokens(self, idx):\n",
    "        if not isinstance(idx, (list, tuple)):\n",
    "            return self.idx_to_tokens[idx]\n",
    "        return [self.to_tokens(i) for i in idx]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx['<unk>']\n",
    "\n",
    "class ProjectGutenbergDataset(Dataset):\n",
    "    def _download(self, links_path: str = 'links.txt', output_dir: str = 'dataset') -> None:\n",
    "        '''\n",
    "        iterate through links in links.txt in Project Gutenberg to download books\n",
    "        '''\n",
    "\n",
    "\n",
    "        #read links from file\n",
    "        if os.path.exists(output_dir) == False:\n",
    "            print('Downloading books ... ')\n",
    "            books = []\n",
    "            try:\n",
    "                with open(links_path, 'r') as file:\n",
    "                    errors = []\n",
    "                    for link in file.readlines():\n",
    "                        link = link.rstrip()\n",
    "\n",
    "                        res = requests.get(link)\n",
    "                        if res.status_code != 200:\n",
    "                            raise Exception(f\"Failed to Fetch, Error code {res.status_code}\")\n",
    "                        books.append(res.text)\n",
    "                        print(f\"SUCCESS {link}\")\n",
    "\n",
    "                if os.path.exists(output_dir) == False:\n",
    "                    os.mkdir(output_dir) \n",
    "\n",
    "                for id, book in enumerate(books):\n",
    "                    output_path = os.path.join(output_dir, f'book{id}.txt')\n",
    "                    with open(output_path, 'w') as file:\n",
    "                        file.write(book)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error while downloading books, error = ', e)\n",
    "        else:\n",
    "            print(\"PG dataset loaded\")\n",
    "\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        #remove digits and anything but letters and space\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    def _tokenize(self, tokenizer, text: str, save_to_file = False) -> list[str]:\n",
    "        tokens = tokenizer(self._preprocess(text))\n",
    "        # save tokens\n",
    "        if save_to_file:\n",
    "            with open('tokens.txt', 'w') as file:\n",
    "                for t in tokens:\n",
    "                    file.writelines(f'{t} \\n')\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _build(self, dataset_path):\n",
    "        '''\n",
    "        @param:\n",
    "            dataset_path: str, path to PG dataset\n",
    "        @return\n",
    "            corpus: list[int] \n",
    "            vocab: Vocab object\n",
    "        '''\n",
    "        alltext = ''\n",
    "        for file in os.listdir(dataset_path):\n",
    "            filepath = os.path.join(dataset_path, file)\n",
    "            with open(filepath, 'r') as file:\n",
    "                alltext += file.read()\n",
    "    \n",
    "        #init tokenizer\n",
    "        tokenizer = word_tokenize\n",
    "\n",
    "        tokens = self._tokenize(tokenizer, alltext, save_to_file=True)\n",
    "        \n",
    "        vocab = Vocab(tokens, min_freq = 2)\n",
    "        \n",
    "        #build corpus, list of indices, [1, 2,100,44,33,...] \n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "\n",
    "        return corpus, vocab\n",
    "\n",
    "    def __init__(self, dataset_path = './dataset/', num_steps = 100):\n",
    "        self.num_steps = num_steps \n",
    "\n",
    "        self._download()\n",
    "        corpus, vocab = self._build(dataset_path)\n",
    "\n",
    "        #save the corpus and vocab for inspection later\n",
    "        with open('corpus.pkl', 'wb') as file:\n",
    "            pickle.dump(corpus, file)\n",
    "\n",
    "        with open('corpus.txt', 'w') as file:\n",
    "            for token in corpus: \n",
    "                file.write(str(token) + \" \")\n",
    "        \n",
    "        with open('vocab_obj.pkl', 'wb') as file:\n",
    "            pickle.dump(vocab, file)\n",
    "\n",
    "        with open('vocab_obj.txt', 'w') as file:\n",
    "            for token, idx in vocab.token_to_idx.items(): \n",
    "                file.writelines(f'{token}:{idx}\\n')\n",
    "\n",
    "        N = len(corpus)\n",
    "\n",
    "        array = torch.tensor([corpus[i : i + num_steps + 1] for i in range(N - num_steps)])\n",
    "        self.X = array[:,:-1] \n",
    "        self.Y = array[:,1:] \n",
    "        self.vocab = vocab\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "# dataset = ProjectGutenbergDataset()\n",
    "\n",
    "# feature, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset))\n",
    "# print(feature)\n",
    "# print(len(feature))\n",
    "# print(label)\n",
    "# print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab_obj.pkl', 'rb') as file:\n",
    "#     vocab = pickle.load(file)\n",
    "\n",
    "# print(vocab.to_tokens(feature.tolist()))\n",
    "# print(vocab.to_tokens(label.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try loading the corpus pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('corpus.pkl', 'rb')  as file:\n",
    "#     corpus = pickle.load(file)\n",
    "\n",
    "# print(type(corpus))\n",
    "# print(corpus[:10])\n",
    "# print(vocab.to_tokens(corpus[500:600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and LM from d2l to establish baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 2, 4])\n",
      "torch.Size([2, 2, 4])\n",
      "tensor([[[1, 9],\n",
      "         [2, 8],\n",
      "         [3, 7],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[2, 7],\n",
      "         [3, 6],\n",
      "         [4, 5],\n",
      "         [5, 4]]])\n",
      "torch.Size([2, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
    "print(a.shape)\n",
    "b = torch.tensor([[9,8,7,6], [7,6,5,4]])\n",
    "print(b.shape)\n",
    "c = torch.stack((a,b), 0)\n",
    "print(c.shape)\n",
    "c = torch.stack((a,b), 1)\n",
    "print(c.shape)\n",
    "c = torch.stack((a,b), 2)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.device_count() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample input shape =  torch.Size([100, 8, 16])\n",
      "torch.Size([100, 8, 32])\n",
      "torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens,sigma = 0.001):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.device = get_device()\n",
    "        self.Wxh = nn.Parameter(torch.rand((num_inputs, num_hiddens), dtype = torch.float, device = self.device) * sigma)\n",
    "        self.Whh = nn.Parameter(torch.rand((num_hiddens, num_hiddens), dtype = torch.float, device = self.device) * sigma)\n",
    "        self.bh = nn.Parameter(torch.rand((1, num_hiddens), dtype = torch.float, device = self.device) * sigma)\n",
    "    \n",
    "    def forward(self, inputs, state = None):\n",
    "        # N is num steps\n",
    "        # n is batch size\n",
    "        # d is num inputs\n",
    "        N, n, d = inputs.shape\n",
    "        if state == None:\n",
    "            #state is not Parameter and will not be used for backprop, initalize state with 0s\n",
    "            state = torch.zeros((n, self.num_hiddens), device = self.device)\n",
    "        else:\n",
    "            state, = state\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for X in inputs:\n",
    "            \n",
    "            state = torch.tanh(X @ self.Wxh + state @ self.Whh + self.bh)\n",
    "            outputs.append(state)\n",
    "        \n",
    "        outputs = torch.stack(outputs, 0)\n",
    "        return outputs, state\n",
    "\n",
    "#params\n",
    "num_steps = 100\n",
    "batch_size = 8\n",
    "num_inputs = 16\n",
    "num_hiddens = 32\n",
    "\n",
    "sample_inputs = torch.rand((num_steps, batch_size, num_inputs), device = get_device())\n",
    "print('sample input shape = ',sample_inputs.shape)\n",
    "#Testing RNN correctness\n",
    "rnn = RNN(num_inputs, num_hiddens)\n",
    "outputs, state = rnn(sample_inputs)\n",
    "print(outputs.shape)\n",
    "print(state.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([345, 345, 345, 345, 345, 345, 345, 345, 345, 345, 345, 345, 345, 345,\n",
      "        345, 345], device='cuda:0')\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn, vocab_size, sigma = 0.001):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigma = sigma\n",
    "        self.device = get_device()\n",
    "\n",
    "        #init params\n",
    "        rnn_num_hiddens = self.rnn.num_hiddens\n",
    "        self.Whq = nn.Parameter(torch.rand((rnn_num_hiddens, self.vocab_size), device= self.device) * sigma)\n",
    "        self.bq = nn.Parameter(torch.rand((1, self.vocab_size), device= self.device) * sigma)\n",
    "    \n",
    "    def one_hot(self, X):\n",
    "        # original X shape is (batch_size, num_steps)\n",
    "        # we want to encode its shape to (num_steps, batch_size, vocab_size)\n",
    "        # TODO: WHY WE TRANSPOSE LIKE THIS?\n",
    "        # https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#one-hot-encoding\n",
    "        # We often transpose the input so that we will obtain an output of \n",
    "        # shape (number of time steps, batch size, vocabulary size). \n",
    "        # This will allow us to loop more conveniently through the \n",
    "        # outermost dimension for updating hidden states of a minibatch, time step by time step\n",
    "        return F.one_hot(X.T, self.vocab_size).type(torch.float)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # print('inputs shape = ', inputs.shape)\n",
    "        embedding = self.one_hot(inputs)\n",
    "        # print('embedding shaep = ', embedding.shape)\n",
    "\n",
    "        rnn_outputs, state = self.rnn(embedding)\n",
    "        # print('rnn output shape = ', rnn_outputs.shape)\n",
    "        # print('state output shape = ', state.shape)\n",
    "\n",
    "        return self.output_layer(rnn_outputs)\n",
    "    \n",
    "    def output_layer(self, rnn_outputs):\n",
    "        outputs = torch.stack([H @ self.Whq + self.bq for H in rnn_outputs], dim = 1)\n",
    "        # print('LM output shape = ', outputs.shape)\n",
    "        return outputs\n",
    "    \n",
    "    def train_step(self, input):\n",
    "        pass\n",
    "\n",
    "    def valid_step(self, input):\n",
    "        pass\n",
    "\n",
    "#Test correctness of LM\n",
    "\n",
    "batch_size = 8\n",
    "num_steps = 16\n",
    "vocab_size = 1000\n",
    "num_hiddens = 32\n",
    "\n",
    "sample_inputs = torch.randint(0, vocab_size, (batch_size, num_steps), device = get_device())\n",
    "rnn = RNN(vocab_size, num_hiddens)\n",
    "lm  = LanguageModel(rnn, vocab_size)\n",
    "\n",
    "outputs = lm(sample_inputs)\n",
    "pred = torch.argmax(outputs[0], dim = 1)\n",
    "\n",
    "print(pred)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG dataset loaded\n",
      "building vocab from 772176 tokens\n",
      "built vocab object\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"dataset_object.pkl\"):\n",
    "    with open(\"dataset_object.pkl\", \"rb\") as file:\n",
    "        dataset = pickle.load(file)\n",
    "else:\n",
    "    dataset = ProjectGutenbergDataset(num_steps=1000)\n",
    "    with open(\"dataset_object.pkl\", \"wb\") as file:\n",
    "        pickle.dump(dataset, file)\n",
    "\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG dataset loaded\n",
      "building vocab from 772176 tokens\n",
      "built vocab object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m num_hiddens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      4\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mProjectGutenbergDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m rnn \u001b[38;5;241m=\u001b[39m RNN(vocab_size, num_hiddens)\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(rnn, vocab_size)\n",
      "Cell \u001b[1;32mIn[4], line 133\u001b[0m, in \u001b[0;36mProjectGutenbergDataset.__init__\u001b[1;34m(self, dataset_path, num_steps)\u001b[0m\n\u001b[0;32m    129\u001b[0m         file\u001b[38;5;241m.\u001b[39mwritelines(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    131\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(corpus)\n\u001b[1;32m--> 133\u001b[0m array \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m array[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY \u001b[38;5;241m=\u001b[39m array[:,\u001b[38;5;241m1\u001b[39m:] \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = len(dataset.vocab)\n",
    "batch_size = 128 \n",
    "num_hiddens = 32\n",
    "num_steps = 1000\n",
    "\n",
    "\n",
    "rnn = RNN(vocab_size, num_hiddens)\n",
    "model = LanguageModel(rnn, vocab_size)\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle= True)\n",
    "print(len(train_dataloader))\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f'epoch = {e}')\n",
    "    for batch_id, (X, y) in enumerate(tqdm(train_dataloader)):\n",
    "        X = X.to(device = get_device())\n",
    "        y = y.to(device = get_device(), dtype= torch.float32)\n",
    "        y_pred = model(X)\n",
    "        y_pred = torch.argmax(y_pred, dim =2).to(torch.float32)\n",
    "\n",
    "        # print(y.shape)\n",
    "        # print(y_pred.shape)\n",
    "\n",
    "        l = torch.tensor(loss(y_pred, y), requires_grad=True, device = get_device())\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        if batch_id % 1000 == 0:\n",
    "            print(l.item())\n",
    "        optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset num steps =  100\n",
      "vocab size  13361\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, max_epochs = 100, lr = 0.001):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = lr\n",
    "        self.device = 'cuda' if torch.cuda.device_count() else 'cpu'\n",
    "\n",
    "    def fit(self, model, train_dataloader, valid_dataloader = None, optim = None, loss = None):\n",
    "        if optim == None:\n",
    "            optim = torch.optim.SGD(model.parameters(), lr = self.lr)\n",
    "        \n",
    "        if loss == None:\n",
    "            def loss_function(y_hat, y):\n",
    "                return F.cross_entropy(y_hat, y)\n",
    "\n",
    "            loss = loss_function\n",
    "\n",
    "        for batch_id, (X, y) in enumerate(train_dataloader):\n",
    "            X = X.to(device = self.device)\n",
    "            y = y.to(device = self.device, dtype= torch.float32)\n",
    "            y_pred = model(X)\n",
    "            y_pred = torch.argmax(y_pred, dim =2).to(torch.float32)\n",
    "            print(y_pred.grad_fn)\n",
    "            print(y.grad_fn)\n",
    "\n",
    "            # print('batch id ', batch_id)\n",
    "            # print(f'X: {X.shape}, device = {X.device}')\n",
    "            # print(f'y: {y.shape}, device = {y.device}')\n",
    "            # print('output shape = ', y_pred.shape)\n",
    "\n",
    "            # train_loss = loss(y_pred, y)\n",
    "            # print(type(train_loss))\n",
    "            # train_loss.backward()\n",
    "            # print(train_loss.item())\n",
    "\n",
    "            break\n",
    "\n",
    "trainer = Trainer()\n",
    "print('dataset num steps = ', dataset.num_steps)\n",
    "\n",
    "vocab_size = len(dataset.vocab)\n",
    "print('vocab size ', vocab_size)\n",
    "batch_size = 8 \n",
    "num_hiddens = 16\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle= True)\n",
    "\n",
    "rnn = RNN(vocab_size, num_hiddens)\n",
    "model = LanguageModel(rnn, vocab_size)\n",
    "\n",
    "trainer.fit(model, train_dataloader=train_dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 13361])\n",
      "torch.float32\n",
      "torch.Size([2, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# l = F.cross_entropy(y_hat, y)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# l.backward()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 13\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(l))\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1169\u001b[0m, in \u001b[0;36mCrossEntropyLoss.__init__\u001b[1;34m(self, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m   1168\u001b[0m              reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, label_smoothing: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index \u001b[38;5;241m=\u001b[39m ignore_index\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing \u001b[38;5;241m=\u001b[39m label_smoothing\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:30\u001b[0m, in \u001b[0;36m_WeightedLoss.__init__\u001b[1;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, weight)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight: Optional[Tensor]\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:23\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[1;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_get_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\_reduction.py:35\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[1;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[0;32m     36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "y_hat = torch.rand((2,10,13361), dtype=torch.float32)\n",
    "# print(y_hat)\n",
    "# y_hat = torch.argmax(y_hat, dim = 2).to(torch.float32)\n",
    "y = torch.rand((2,10), dtype=torch.float32)\n",
    "print(y_hat.shape)\n",
    "print(y_hat.dtype)\n",
    "print(y.shape)\n",
    "\n",
    "# l = F.cross_entropy(y_hat, y)\n",
    "# l.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    l = torch.nn.CrossEntropyLoss(y_hat, y)\n",
    "\n",
    "    print(type(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
