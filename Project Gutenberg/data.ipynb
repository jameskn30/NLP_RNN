{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torch\n",
    "import pickle\n",
    "import tqdm\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG dataset loaded\n",
      "building vocab from 772176 tokens\n",
      "built vocab object\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Vocab' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 144\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY[index]\n\u001b[1;32m--> 144\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mProjectGutenbergDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m feature, label \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[16], line 125\u001b[0m, in \u001b[0;36mProjectGutenbergDataset.__init__\u001b[1;34m(self, dataset_path, num_steps)\u001b[0m\n\u001b[0;32m    122\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(vocab, file)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[0;32m    126\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    132\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(corpus)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens = [], min_freq = 0, reserved_tokens = []):\n",
    "        self._build(tokens, min_freq, reserved_tokens)\n",
    "    \n",
    "    def _build(self, tokens, min_freq, reserved_tokens):\n",
    "        print(f'building vocab from {len(tokens)} tokens')\n",
    "        counter = Counter(tokens)\n",
    "        self.token_freq = sorted(counter.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        self.idx_to_tokens = list(sorted(set(['<unk>'] + reserved_tokens \\\n",
    "        + [ token for token, freq in self.token_freq if freq > min_freq])))\n",
    "    \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_tokens)}\n",
    "\n",
    "        print('built vocab object')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_tokens)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        #if not type list or tuple\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(t) for t in tokens]\n",
    "    \n",
    "    def to_tokens(self, idx):\n",
    "        if not isinstance(idx, (list, tuple)):\n",
    "            return self.idx_to_tokens[idx]\n",
    "        return [self.to_tokens(i) for i in idx]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx['<unk>']\n",
    "\n",
    "class ProjectGutenbergDataset(Dataset):\n",
    "    def _download(self, links_path: str = 'links.txt', output_dir: str = 'dataset') -> None:\n",
    "        '''\n",
    "        iterate through links in links.txt in Project Gutenberg to download books\n",
    "        '''\n",
    "        #read links from file\n",
    "        if os.path.exists(output_dir) == False:\n",
    "            print('Downloading books ... ')\n",
    "            books = []\n",
    "            try:\n",
    "                with open(links_path, 'r') as file:\n",
    "                    errors = []\n",
    "                    for link in file.readlines():\n",
    "                        link = link.rstrip()\n",
    "\n",
    "                        res = requests.get(link)\n",
    "                        if res.status_code != 200:\n",
    "                            raise Exception(f\"Failed to Fetch, Error code {res.status_code}\")\n",
    "                        books.append(res.text)\n",
    "                        print(f\"SUCCESS {link}\")\n",
    "\n",
    "                if os.path.exists(output_dir) == False:\n",
    "                    os.mkdir(output_dir) \n",
    "\n",
    "                for id, book in enumerate(books):\n",
    "                    output_path = os.path.join(output_dir, f'book{id}.txt')\n",
    "                    with open(output_path, 'w') as file:\n",
    "                        file.write(book)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error while downloading books, error = ', e)\n",
    "        else:\n",
    "            print(\"PG dataset loaded\")\n",
    "\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        #remove digits and anything but letters and space\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    def _tokenize(self, tokenizer, text: str, save_to_file = False) -> list[str]:\n",
    "        tokens = tokenizer(self._preprocess(text))\n",
    "        # save tokens\n",
    "        if save_to_file:\n",
    "            with open('tokens.txt', 'w') as file:\n",
    "                for t in tokens:\n",
    "                    file.writelines(f'{t} \\n')\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _build(self, dataset_path):\n",
    "        '''\n",
    "        @param:\n",
    "            dataset_path: str, path to PG dataset\n",
    "        @return\n",
    "            corpus: list[int] \n",
    "            vocab: Vocab object\n",
    "        '''\n",
    "        alltext = ''\n",
    "        for file in os.listdir(dataset_path):\n",
    "            filepath = os.path.join(dataset_path, file)\n",
    "            with open(filepath, 'r') as file:\n",
    "                alltext += file.read()\n",
    "    \n",
    "        #init tokenizer\n",
    "        tokenizer = word_tokenize\n",
    "\n",
    "        tokens = self._tokenize(tokenizer, alltext, save_to_file=True)\n",
    "        \n",
    "        vocab = Vocab(tokens, min_freq = 2)\n",
    "        \n",
    "        #build corpus, list of indices, [1, 2,100,44,33,...] \n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "\n",
    "        return corpus, vocab\n",
    "\n",
    "    def __init__(self, dataset_path = './dataset/', num_steps = 100):\n",
    "        self._download()\n",
    "        corpus, vocab = self._build(dataset_path)\n",
    "\n",
    "        with open('corpus.pkl', 'wb') as file:\n",
    "            pickle.dump(corpus, file)\n",
    "\n",
    "        with open('corpus.txt', 'w') as file:\n",
    "            for token in corpus: \n",
    "                file.write(str(token) + \" \")\n",
    "        \n",
    "        with open('vocab_obj.pkl', 'wb') as file:\n",
    "            pickle.dump(vocab, file)\n",
    "\n",
    "        N = len(corpus)\n",
    "\n",
    "        array = torch.tensor([corpus[i : i + num_steps + 1] for i in range(N - num_steps)])\n",
    "        self.X = array[:,:-1] \n",
    "        self.Y = array[:,1:] \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "dataset = ProjectGutenbergDataset()\n",
    "\n",
    "feature, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7512, 5746, 3312, 2300, 5085, 4740, 1995, 5149, 7512, 8225, 7548, 2300,\n",
      "        4007, 2923, 7512, 7962, 5085,  338,  340, 3769, 7512, 7892, 7081,  293,\n",
      "        4788, 5169, 5291, 5085, 7512, 8390,  475, 4964, 1621,  293, 8351,  237,\n",
      "        4964, 6199, 8248, 8466, 4598, 1596, 4020, 3149, 4020,  539, 5149, 6221,\n",
      "        4020, 7851, 7512, 7490, 5085, 7512, 5746, 3312, 4315, 3788, 8351, 7548,\n",
      "        2300, 5149, 5127,  475, 8431, 3697, 8466,  391, 4990, 4393, 3769, 7512,\n",
      "        7892, 7081, 8466, 8319, 3407, 7622, 1175, 7512, 4239, 5085, 7512, 1639,\n",
      "        8258, 8466,  391, 4393,  650, 7968, 7548, 2300, 7621, 4740, 1995, 5149,\n",
      "        7512, 8225,  520,    0])\n",
      "100\n",
      "tensor([5746, 3312, 2300, 5085, 4740, 1995, 5149, 7512, 8225, 7548, 2300, 4007,\n",
      "        2923, 7512, 7962, 5085,  338,  340, 3769, 7512, 7892, 7081,  293, 4788,\n",
      "        5169, 5291, 5085, 7512, 8390,  475, 4964, 1621,  293, 8351,  237, 4964,\n",
      "        6199, 8248, 8466, 4598, 1596, 4020, 3149, 4020,  539, 5149, 6221, 4020,\n",
      "        7851, 7512, 7490, 5085, 7512, 5746, 3312, 4315, 3788, 8351, 7548, 2300,\n",
      "        5149, 5127,  475, 8431, 3697, 8466,  391, 4990, 4393, 3769, 7512, 7892,\n",
      "        7081, 8466, 8319, 3407, 7622, 1175, 7512, 4239, 5085, 7512, 1639, 8258,\n",
      "        8466,  391, 4393,  650, 7968, 7548, 2300, 7621, 4740, 1995, 5149, 7512,\n",
      "        8225,  520,    0,    0])\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(feature)\n",
    "print(len(feature))\n",
    "print(label)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'moby', 'dick', 'or', 'the', 'whale', 'author', '<unk>']\n",
      "['project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'moby', 'dick', 'or', 'the', 'whale', 'author', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "with open('vocab.pkl', 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "\n",
    "print(vocab.to_tokens(feature.tolist()))\n",
    "print(vocab.to_tokens(label.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try loading the corpus pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[11772, 9081, 5269, 3670, 8036, 7519, 3145, 8122, 11772, 12952]\n",
      "['chapter', 'stowing', 'down', 'and', 'clearing', 'up', 'chapter', 'the', 'doubloon', 'chapter', 'leg', 'and', 'arm', 'chapter', 'the', 'decanter', 'chapter', 'a', 'bower', 'in', 'the', 'arsacides', 'chapter', 'measurement', 'of', 'the', 'whales', 'skeleton', 'chapter', 'the', 'fossil', 'whale', 'chapter', 'does', 'the', 'whales', 'magnitude', '<unk>', 'he', 'perish', 'chapter', 'ahabs', 'leg', 'chapter', 'the', 'carpenter', 'chapter', 'ahab', 'and', 'the', 'carpenter', 'chapter', 'ahab', 'and', 'starbuck', 'in', 'the', 'cabin', 'chapter', 'queequeg', 'in', 'his', 'coffin', 'chapter', 'the', 'pacific', 'chapter', 'the', 'blacksmith', 'chapter', 'the', 'forge', 'chapter', 'the', '<unk>', 'chapter', 'the', 'pequod', 'meets', 'the', 'bachelor', 'chapter', 'the', 'dying', 'whale', 'chapter', 'the', 'whale', 'watch', 'chapter', 'the', 'quadrant', 'chapter', 'the', 'candles', 'chapter', 'the', 'deck', 'towards', 'the']\n",
      "772176\n"
     ]
    }
   ],
   "source": [
    "with open('corpus.pkl', 'rb')  as file:\n",
    "    corpus = pickle.load(file)\n",
    "\n",
    "print(type(corpus))\n",
    "print(corpus[:10])\n",
    "print(vocab.to_tokens(corpus[500:600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10802\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32, shuffle= True)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4990, 3504, 2952,  ..., 4020, 5074, 7622],\n",
      "        [6507, 7622,  614,  ..., 7510, 4598,    0],\n",
      "        [6117, 7795, 7678,  ..., 6952, 5085, 4836],\n",
      "        ...,\n",
      "        [3415, 8157, 8033,  ..., 4990, 5196,  293],\n",
      "        [6860,  975, 3677,  ...,  614, 8033,   32],\n",
      "        [7042,  975, 3415,  ...,  293, 8358,    1]])\n",
      "32\n",
      "tensor([[3504, 2952, 7622,  ..., 5074, 7622, 2352],\n",
      "        [7622,  614, 7963,  ..., 4598,    0, 8401],\n",
      "        [7795, 7678, 7516,  ..., 5085, 4836, 7522],\n",
      "        ...,\n",
      "        [8157, 8033, 4376,  ..., 5196,  293, 7622],\n",
      "        [ 975, 3677, 1771,  ..., 8033,   32, 5122],\n",
      "        [ 975, 3415, 7377,  ..., 8358,    1,    0]])\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "feature, label = next(iter(train_dataloader))\n",
    "print(feature)\n",
    "print(len(feature))\n",
    "print(label)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, model, train_dataloader, valid_dataloader):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and RNN LM from d2l to establish baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
