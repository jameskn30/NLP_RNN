{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG dataset loaded\n",
      "building vocab from 772176 tokens\n",
      "built vocab object\n",
      "saving vocab object to vocab.csv\n",
      "[11772, 9081, 5269, 3670, 8036, 7519, 3145, 8122, 11772, 12952]\n"
     ]
    }
   ],
   "source": [
    "class ProjectGutenbergDataset(Dataset):\n",
    "    def _download(self, links_path: str = 'links.txt', output_dir: str = 'dataset') -> None:\n",
    "        '''\n",
    "        iterate through links in links.txt in Project Gutenberg to download books\n",
    "        '''\n",
    "        #read links from file\n",
    "        if os.path.exists(output_dir) == False:\n",
    "            print('Downloading books ... ')\n",
    "            books = []\n",
    "            try:\n",
    "                with open(links_path, 'r') as file:\n",
    "                    errors = []\n",
    "                    for link in file.readlines():\n",
    "                        link = link.rstrip()\n",
    "\n",
    "                        res = requests.get(link)\n",
    "                        if res.status_code != 200:\n",
    "                            raise Exception(f\"Failed to Fetch, Error code {res.status_code}\")\n",
    "                        books.append(res.text)\n",
    "                        print(f\"SUCCESS {link}\")\n",
    "\n",
    "                if os.path.exists(output_dir) == False:\n",
    "                    os.mkdir(output_dir) \n",
    "\n",
    "                for id, book in enumerate(books):\n",
    "                    output_path = os.path.join(output_dir, f'book{id}.txt')\n",
    "                    with open(output_path, 'w') as file:\n",
    "                        file.write(book)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error while downloading books, error = ', e)\n",
    "        else:\n",
    "            print(\"PG dataset loaded\")\n",
    "\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    def _tokenize(self, tokenizer, text: str, save_to_file = False) -> list[str]:\n",
    "        tokens = tokenizer(self._preprocess(text))\n",
    "        # save tokens\n",
    "        if save_to_file:\n",
    "            with open('tokens.txt', 'w') as file:\n",
    "                for t in tokens:\n",
    "                    file.writelines(f'{t} ')\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _build(self, dataset_path):\n",
    "        #Merge the dataset\n",
    "        alltext = ''\n",
    "        for file in os.listdir(dataset_path):\n",
    "            filepath = os.path.join(dataset_path, file)\n",
    "            with open(filepath, 'r') as file:\n",
    "                alltext += file.read()\n",
    "        return alltext\n",
    "\n",
    "    def __init__(self, dataset_path = './dataset/', num_steps = 100, batch_sie = 32):\n",
    "        self._download()\n",
    "        alltext = self._build(dataset_path)\n",
    "        self.vocab = None\n",
    "\n",
    "        #init tokenizer\n",
    "        tokenizer = word_tokenize\n",
    "\n",
    "        tokens = self._tokenize(tokenizer, alltext, save_to_file=True)\n",
    "        \n",
    "        if not self.vocab:\n",
    "            self.vocab = Vocab(tokens, min_freq = 2)\n",
    "            self.vocab.save_to_file()\n",
    "        \n",
    "        #build corpus, list of indices, [1, 2,100,44,33,...] \n",
    "        self.corpus = [self.vocab[token] for token in tokens]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        pass\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, tokens = [], min_freq = 0, reserved_tokens = []):\n",
    "        self._build(tokens, min_freq, reserved_tokens)\n",
    "    \n",
    "    def _build(self, tokens, min_freq, reserved_tokens):\n",
    "        print(f'building vocab from {len(tokens)} tokens')\n",
    "        counter = Counter(tokens)\n",
    "        self.token_freq = sorted(counter.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        self.idx_to_tokens = list(sorted(set(['<unk>'] + reserved_tokens \\\n",
    "        + [ token for token, freq in self.token_freq if freq > min_freq])))\n",
    "    \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_tokens)}\n",
    "\n",
    "        print('built vocab object')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_tokens)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        #if not type list or tuple\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(t) for t in tokens]\n",
    "    \n",
    "    def to_tokens(self, idx):\n",
    "        if not isinstance(idx, (list, tuple)):\n",
    "            return self.idx_to_tokens.get(idx, self.unk)\n",
    "        return [self.to_tokens(i) for i in idx]\n",
    "    \n",
    "    def save_to_file(self, path = 'vocab.csv'):\n",
    "        print(f'saving vocab object to {path}')\n",
    "        with open(path,'w') as file:\n",
    "            file.writelines(f'tokens,idx\\n')\n",
    "            for token, index in self.token_to_idx.items():\n",
    "                file.writelines(f'{token},{index}\\n')\n",
    "        \n",
    "    # def load_from_file(self, path = 'vocab.csv'):\n",
    "    #     print(f'loading vocab object from {path}')\n",
    "    #     with open(path,'r') as file:\n",
    "    #         for line in file.readlines():\n",
    "    #             token, idx = line.rstrip().split(',')\n",
    "\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx['<unk>']\n",
    "\n",
    "dataset = ProjectGutenbergDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gutenberg.org/ebooks/2701.txt.utf-8\n",
      "ï»¿The Project Gutenberg eBook of Moby Dick; Or, The Whale\n",
      "    \n",
      "This ebook is for the use of anyone \n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.gutenberg.org/ebooks/2701.txt.utf-8'\n",
    "res = requests.get(link)\n",
    "print(link)\n",
    "print(res.text[:100])\n",
    "with open('sample.txt', 'w') as file:\n",
    "    file.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
