{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "import tqdm\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG dataset loaded\n",
      "building vocab from 772176 tokens\n",
      "built vocab object\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens = [], min_freq = 0, reserved_tokens = []):\n",
    "        self._build(tokens, min_freq, reserved_tokens)\n",
    "    \n",
    "    def _build(self, tokens, min_freq, reserved_tokens):\n",
    "        print(f'building vocab from {len(tokens)} tokens')\n",
    "        counter = Counter(tokens)\n",
    "        self.token_freq = sorted(counter.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        self.idx_to_tokens = list(sorted(set(['<unk>'] + reserved_tokens \\\n",
    "        + [ token for token, freq in self.token_freq if freq > min_freq])))\n",
    "    \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_tokens)}\n",
    "\n",
    "        print('built vocab object')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_tokens)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        #if not type list or tuple\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(t) for t in tokens]\n",
    "    \n",
    "    def to_tokens(self, idx):\n",
    "        if not isinstance(idx, (list, tuple)):\n",
    "            return self.idx_to_tokens[idx]\n",
    "        return [self.to_tokens(i) for i in idx]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx['<unk>']\n",
    "\n",
    "class ProjectGutenbergDataset(Dataset):\n",
    "    def _download(self, links_path: str = 'links.txt', output_dir: str = 'dataset') -> None:\n",
    "        '''\n",
    "        iterate through links in links.txt in Project Gutenberg to download books\n",
    "        '''\n",
    "        #read links from file\n",
    "        if os.path.exists(output_dir) == False:\n",
    "            print('Downloading books ... ')\n",
    "            books = []\n",
    "            try:\n",
    "                with open(links_path, 'r') as file:\n",
    "                    errors = []\n",
    "                    for link in file.readlines():\n",
    "                        link = link.rstrip()\n",
    "\n",
    "                        res = requests.get(link)\n",
    "                        if res.status_code != 200:\n",
    "                            raise Exception(f\"Failed to Fetch, Error code {res.status_code}\")\n",
    "                        books.append(res.text)\n",
    "                        print(f\"SUCCESS {link}\")\n",
    "\n",
    "                if os.path.exists(output_dir) == False:\n",
    "                    os.mkdir(output_dir) \n",
    "\n",
    "                for id, book in enumerate(books):\n",
    "                    output_path = os.path.join(output_dir, f'book{id}.txt')\n",
    "                    with open(output_path, 'w') as file:\n",
    "                        file.write(book)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error while downloading books, error = ', e)\n",
    "        else:\n",
    "            print(\"PG dataset loaded\")\n",
    "\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        #remove digits and anything but letters and space\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    def _tokenize(self, tokenizer, text: str, save_to_file = False) -> list[str]:\n",
    "        tokens = tokenizer(self._preprocess(text))\n",
    "        # save tokens\n",
    "        if save_to_file:\n",
    "            with open('tokens.txt', 'w') as file:\n",
    "                for t in tokens:\n",
    "                    file.writelines(f'{t} \\n')\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _build(self, dataset_path):\n",
    "        '''\n",
    "        @param:\n",
    "            dataset_path: str, path to PG dataset\n",
    "        @return\n",
    "            corpus: list[int] \n",
    "            vocab: Vocab object\n",
    "        '''\n",
    "        alltext = ''\n",
    "        for file in os.listdir(dataset_path):\n",
    "            filepath = os.path.join(dataset_path, file)\n",
    "            with open(filepath, 'r') as file:\n",
    "                alltext += file.read()\n",
    "    \n",
    "        #init tokenizer\n",
    "        tokenizer = word_tokenize\n",
    "\n",
    "        tokens = self._tokenize(tokenizer, alltext, save_to_file=True)\n",
    "        \n",
    "        vocab = Vocab(tokens, min_freq = 2)\n",
    "        \n",
    "        #build corpus, list of indices, [1, 2,100,44,33,...] \n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "\n",
    "        return corpus, vocab\n",
    "\n",
    "    def __init__(self, dataset_path = './dataset/', num_steps = 100):\n",
    "        self._download()\n",
    "        corpus, vocab = self._build(dataset_path)\n",
    "\n",
    "        with open('corpus.pkl', 'wb') as file:\n",
    "            pickle.dump(corpus, file)\n",
    "\n",
    "        with open('corpus.txt', 'w') as file:\n",
    "            for token in corpus: \n",
    "                file.write(str(token) + \" \")\n",
    "        \n",
    "        with open('vocab_obj.pkl', 'wb') as file:\n",
    "            pickle.dump(vocab, file)\n",
    "\n",
    "        N = len(corpus)\n",
    "\n",
    "        array = torch.tensor([corpus[i : i + num_steps + 1] for i in range(N - num_steps)])\n",
    "        self.X = array[:,:-1] \n",
    "        self.Y = array[:,1:] \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "dataset = ProjectGutenbergDataset()\n",
    "\n",
    "feature, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7512, 5746, 3312, 2300, 5085, 4740, 1995, 5149, 7512, 8225, 7548, 2300,\n",
      "        4007, 2923, 7512, 7962, 5085,  338,  340, 3769, 7512, 7892, 7081,  293,\n",
      "        4788, 5169, 5291, 5085, 7512, 8390,  475, 4964, 1621,  293, 8351,  237,\n",
      "        4964, 6199, 8248, 8466, 4598, 1596, 4020, 3149, 4020,  539, 5149, 6221,\n",
      "        4020, 7851, 7512, 7490, 5085, 7512, 5746, 3312, 4315, 3788, 8351, 7548,\n",
      "        2300, 5149, 5127,  475, 8431, 3697, 8466,  391, 4990, 4393, 3769, 7512,\n",
      "        7892, 7081, 8466, 8319, 3407, 7622, 1175, 7512, 4239, 5085, 7512, 1639,\n",
      "        8258, 8466,  391, 4393,  650, 7968, 7548, 2300, 7621, 4740, 1995, 5149,\n",
      "        7512, 8225,  520,    0])\n",
      "100\n",
      "tensor([5746, 3312, 2300, 5085, 4740, 1995, 5149, 7512, 8225, 7548, 2300, 4007,\n",
      "        2923, 7512, 7962, 5085,  338,  340, 3769, 7512, 7892, 7081,  293, 4788,\n",
      "        5169, 5291, 5085, 7512, 8390,  475, 4964, 1621,  293, 8351,  237, 4964,\n",
      "        6199, 8248, 8466, 4598, 1596, 4020, 3149, 4020,  539, 5149, 6221, 4020,\n",
      "        7851, 7512, 7490, 5085, 7512, 5746, 3312, 4315, 3788, 8351, 7548, 2300,\n",
      "        5149, 5127,  475, 8431, 3697, 8466,  391, 4990, 4393, 3769, 7512, 7892,\n",
      "        7081, 8466, 8319, 3407, 7622, 1175, 7512, 4239, 5085, 7512, 1639, 8258,\n",
      "        8466,  391, 4393,  650, 7968, 7548, 2300, 7621, 4740, 1995, 5149, 7512,\n",
      "        8225,  520,    0,    0])\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(feature)\n",
    "print(len(feature))\n",
    "print(label)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'moby', 'dick', 'or', 'the', 'whale', 'author', '<unk>']\n",
      "['project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'moby', 'dick', 'or', 'the', 'whale', 'author', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "with open('vocab_obj.pkl', 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "\n",
    "print(vocab.to_tokens(feature.tolist()))\n",
    "print(vocab.to_tokens(label.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try loading the corpus pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[11772, 9081, 5269, 3670, 8036, 7519, 3145, 8122, 11772, 12952]\n",
      "['chapter', 'stowing', 'down', 'and', 'clearing', 'up', 'chapter', 'the', 'doubloon', 'chapter', 'leg', 'and', 'arm', 'chapter', 'the', 'decanter', 'chapter', 'a', 'bower', 'in', 'the', 'arsacides', 'chapter', 'measurement', 'of', 'the', 'whales', 'skeleton', 'chapter', 'the', 'fossil', 'whale', 'chapter', 'does', 'the', 'whales', 'magnitude', '<unk>', 'he', 'perish', 'chapter', 'ahabs', 'leg', 'chapter', 'the', 'carpenter', 'chapter', 'ahab', 'and', 'the', 'carpenter', 'chapter', 'ahab', 'and', 'starbuck', 'in', 'the', 'cabin', 'chapter', 'queequeg', 'in', 'his', 'coffin', 'chapter', 'the', 'pacific', 'chapter', 'the', 'blacksmith', 'chapter', 'the', 'forge', 'chapter', 'the', '<unk>', 'chapter', 'the', 'pequod', 'meets', 'the', 'bachelor', 'chapter', 'the', 'dying', 'whale', 'chapter', 'the', 'whale', 'watch', 'chapter', 'the', 'quadrant', 'chapter', 'the', 'candles', 'chapter', 'the', 'deck', 'towards', 'the']\n"
     ]
    }
   ],
   "source": [
    "with open('corpus.pkl', 'rb')  as file:\n",
    "    corpus = pickle.load(file)\n",
    "\n",
    "print(type(corpus))\n",
    "print(corpus[:10])\n",
    "print(vocab.to_tokens(corpus[500:600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10802\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32, shuffle= True)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4990, 3504, 2952,  ..., 4020, 5074, 7622],\n",
      "        [6507, 7622,  614,  ..., 7510, 4598,    0],\n",
      "        [6117, 7795, 7678,  ..., 6952, 5085, 4836],\n",
      "        ...,\n",
      "        [3415, 8157, 8033,  ..., 4990, 5196,  293],\n",
      "        [6860,  975, 3677,  ...,  614, 8033,   32],\n",
      "        [7042,  975, 3415,  ...,  293, 8358,    1]])\n",
      "32\n",
      "tensor([[3504, 2952, 7622,  ..., 5074, 7622, 2352],\n",
      "        [7622,  614, 7963,  ..., 4598,    0, 8401],\n",
      "        [7795, 7678, 7516,  ..., 5085, 4836, 7522],\n",
      "        ...,\n",
      "        [8157, 8033, 4376,  ..., 5196,  293, 7622],\n",
      "        [ 975, 3677, 1771,  ..., 8033,   32, 5122],\n",
      "        [ 975, 3415, 7377,  ..., 8358,    1,    0]])\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "feature, label = next(iter(train_dataloader))\n",
    "print(feature)\n",
    "print(len(feature))\n",
    "print(label)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, model, train_dataloader, valid_dataloader):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and RNN LM from d2l to establish baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.device_count() else 'cpu'\n",
    "print(f\"DEVICE = {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3, 4],\n",
      "         [2, 3, 4, 5]],\n",
      "\n",
      "        [[9, 8, 7, 6],\n",
      "         [7, 6, 5, 4]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
    "b = torch.tensor([[9,8,7,6], [7,6,5,4]])\n",
    "\n",
    "c = torch.stack((a,b), 0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample input shape =  torch.Size([100, 8, 16])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample input shape =  torch.Size([100, 8, 16])\n",
      "torch.Size([100, 8, 32])\n",
      "torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens,sigma = 0.001):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.Wxh = nn.Parameter(torch.rand((num_inputs, num_hiddens), dtype = torch.float, device = DEVICE) * sigma)\n",
    "        self.Whh = nn.Parameter(torch.rand((num_hiddens, num_hiddens), dtype = torch.float, device = DEVICE) * sigma)\n",
    "        self.bh = nn.Parameter(torch.rand((1, num_hiddens), dtype = torch.float, device = DEVICE) * sigma)\n",
    "    \n",
    "    def forward(self, inputs, state = None):\n",
    "        # N is num steps\n",
    "        # n is batch size\n",
    "        # d is num inputs\n",
    "        N, n, d = inputs.shape\n",
    "        if state == None:\n",
    "            #state is not Parameter and will not be used for backprop, initalize state with 0s\n",
    "            state = torch.zeros((n, self.num_hiddens), device = DEVICE)\n",
    "        else:\n",
    "            state, = state\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for X in inputs:\n",
    "            \n",
    "            state = torch.tanh(X @ self.Wxh + state @ self.Whh + self.bh)\n",
    "            outputs.append(state)\n",
    "        \n",
    "        outputs = torch.stack(outputs, 0)\n",
    "        return outputs, state\n",
    "\n",
    "#params\n",
    "num_steps = 100\n",
    "batch_size = 8\n",
    "num_inputs = 16\n",
    "num_hiddens = 32\n",
    "\n",
    "sample_inputs = torch.rand((num_steps, batch_size, num_inputs), device = DEVICE)\n",
    "print('sample input shape = ',sample_inputs.shape)\n",
    "#Testing RNN correctness\n",
    "rnn = RNN(num_inputs, num_hiddens)\n",
    "outputs, state = rnn(sample_inputs)\n",
    "print(outputs.shape)\n",
    "print(state.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shaep =  torch.Size([16, 8, 1000])\n",
      "rnn output shape =  torch.Size([16, 8, 32])\n",
      "state output shape =  torch.Size([8, 32])\n",
      "LM output shape =  torch.Size([8, 16, 1000])\n",
      "tensor([259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 251, 259, 259, 259,\n",
      "        259, 259], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn, vocab_size, sigma = 0.001):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigma = sigma\n",
    "\n",
    "        #init params\n",
    "        rnn_num_hiddens = self.rnn.num_hiddens\n",
    "        self.Whq = nn.Parameter(torch.rand((rnn_num_hiddens, self.vocab_size), device= DEVICE) * sigma)\n",
    "        self.bq = nn.Parameter(torch.rand((1, self.vocab_size), device= DEVICE) * sigma)\n",
    "    \n",
    "    def one_hot(self, X):\n",
    "        #original X shape is (batch_size, num_steps)\n",
    "        #we want to encode its shape to (num_steps, batch_size, vocab_size)\n",
    "        return F.one_hot(X.T, self.vocab_size).type(torch.float)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.one_hot(inputs)\n",
    "\n",
    "        rnn_outputs, state = self.rnn(embedding)\n",
    "        print('embedding shaep = ', embedding.shape)\n",
    "        print('rnn output shape = ', rnn_outputs.shape)\n",
    "        print('state output shape = ', state.shape)\n",
    "\n",
    "        return self.output_layer(rnn_outputs)\n",
    "    \n",
    "    def output_layer(self, rnn_outputs):\n",
    "        outputs = torch.stack([H @ self.Whq + self.bq for H in rnn_outputs], dim = 1)\n",
    "        print('LM output shape = ', outputs.shape)\n",
    "        return outputs\n",
    "\n",
    "#Test correctness of LM\n",
    "\n",
    "batch_size = 8\n",
    "num_steps = 16\n",
    "vocab_size = 1000\n",
    "num_hiddens = 32\n",
    "\n",
    "sample_inputs = torch.randint(0, vocab_size, (batch_size, num_steps), device = DEVICE)\n",
    "rnn = RNN(vocab_size, num_hiddens)\n",
    "\n",
    "lm  = LanguageModel(rnn, vocab_size)\n",
    "\n",
    "outputs = lm(sample_inputs)\n",
    "pred = torch.argmax(outputs[0], dim = 1)\n",
    "\n",
    "print(pred)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
