{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "import tqdm\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG dataset loaded\n",
      "building vocab from 772176 tokens\n",
      "built vocab object\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens = [], min_freq = 0, reserved_tokens = []):\n",
    "        self._build(tokens, min_freq, reserved_tokens)\n",
    "    \n",
    "    def _build(self, tokens, min_freq, reserved_tokens):\n",
    "        print(f'building vocab from {len(tokens)} tokens')\n",
    "        counter = Counter(tokens)\n",
    "        self.token_freq = sorted(counter.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        self.idx_to_tokens = list(sorted(set(['<unk>'] + reserved_tokens \\\n",
    "        + [ token for token, freq in self.token_freq if freq > min_freq])))\n",
    "    \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_tokens)}\n",
    "\n",
    "        print('built vocab object')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_tokens)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        #if not type list or tuple\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(t) for t in tokens]\n",
    "    \n",
    "    def to_tokens(self, idx):\n",
    "        if not isinstance(idx, (list, tuple)):\n",
    "            return self.idx_to_tokens[idx]\n",
    "        return [self.to_tokens(i) for i in idx]\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx['<unk>']\n",
    "\n",
    "class ProjectGutenbergDataset(Dataset):\n",
    "    def _download(self, links_path: str = 'links.txt', output_dir: str = 'dataset') -> None:\n",
    "        '''\n",
    "        iterate through links in links.txt in Project Gutenberg to download books\n",
    "        '''\n",
    "        #read links from file\n",
    "        if os.path.exists(output_dir) == False:\n",
    "            print('Downloading books ... ')\n",
    "            books = []\n",
    "            try:\n",
    "                with open(links_path, 'r') as file:\n",
    "                    errors = []\n",
    "                    for link in file.readlines():\n",
    "                        link = link.rstrip()\n",
    "\n",
    "                        res = requests.get(link)\n",
    "                        if res.status_code != 200:\n",
    "                            raise Exception(f\"Failed to Fetch, Error code {res.status_code}\")\n",
    "                        books.append(res.text)\n",
    "                        print(f\"SUCCESS {link}\")\n",
    "\n",
    "                if os.path.exists(output_dir) == False:\n",
    "                    os.mkdir(output_dir) \n",
    "\n",
    "                for id, book in enumerate(books):\n",
    "                    output_path = os.path.join(output_dir, f'book{id}.txt')\n",
    "                    with open(output_path, 'w') as file:\n",
    "                        file.write(book)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error while downloading books, error = ', e)\n",
    "        else:\n",
    "            print(\"PG dataset loaded\")\n",
    "\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        #remove digits and anything but letters and space\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    def _tokenize(self, tokenizer, text: str, save_to_file = False) -> list[str]:\n",
    "        tokens = tokenizer(self._preprocess(text))\n",
    "        # save tokens\n",
    "        if save_to_file:\n",
    "            with open('tokens.txt', 'w') as file:\n",
    "                for t in tokens:\n",
    "                    file.writelines(f'{t} \\n')\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _build(self, dataset_path):\n",
    "        '''\n",
    "        @param:\n",
    "            dataset_path: str, path to PG dataset\n",
    "        @return\n",
    "            corpus: list[int] \n",
    "            vocab: Vocab object\n",
    "        '''\n",
    "        alltext = ''\n",
    "        for file in os.listdir(dataset_path):\n",
    "            filepath = os.path.join(dataset_path, file)\n",
    "            with open(filepath, 'r') as file:\n",
    "                alltext += file.read()\n",
    "    \n",
    "        #init tokenizer\n",
    "        tokenizer = word_tokenize\n",
    "\n",
    "        tokens = self._tokenize(tokenizer, alltext, save_to_file=True)\n",
    "        \n",
    "        vocab = Vocab(tokens, min_freq = 2)\n",
    "        \n",
    "        #build corpus, list of indices, [1, 2,100,44,33,...] \n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "\n",
    "        return corpus, vocab\n",
    "\n",
    "    def __init__(self, dataset_path = './dataset/', num_steps = 100):\n",
    "        self.num_steps = num_steps \n",
    "\n",
    "        self._download()\n",
    "        corpus, vocab = self._build(dataset_path)\n",
    "\n",
    "        #save the corpus and vocab for inspection later\n",
    "        with open('corpus.pkl', 'wb') as file:\n",
    "            pickle.dump(corpus, file)\n",
    "\n",
    "        with open('corpus.txt', 'w') as file:\n",
    "            for token in corpus: \n",
    "                file.write(str(token) + \" \")\n",
    "        \n",
    "        with open('vocab_obj.pkl', 'wb') as file:\n",
    "            pickle.dump(vocab, file)\n",
    "\n",
    "        with open('vocab_obj.txt', 'w') as file:\n",
    "            for token, idx in vocab.token_to_idx.items(): \n",
    "                file.writelines(f'{token}:{idx}\\n')\n",
    "\n",
    "        N = len(corpus)\n",
    "\n",
    "        array = torch.tensor([corpus[i : i + num_steps + 1] for i in range(N - num_steps)])\n",
    "        self.X = array[:,:-1] \n",
    "        self.Y = array[:,1:] \n",
    "        self.vocab = vocab\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "dataset = ProjectGutenbergDataset()\n",
    "\n",
    "feature, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772076\n",
      "tensor([11772,  9081,  5269,  3670,  8036,  7519,  3145,  8122, 11772, 12952,\n",
      "        11831,  3670,  6381,  4646, 11772, 12524,  8036,   505,   507,  5963,\n",
      "        11772, 12399, 11137,   434,  7599,  8153,  8338,  8036, 11772, 13181,\n",
      "          727,  7855,  2547,   434, 13113,   362,  7855,  9800, 12976, 13333,\n",
      "         7283,  2507,  6398,  5003,  6398,   828,  8122,  9836,  6398, 12316,\n",
      "        11772, 11735,  8036, 11772,  9081,  5269,  6847,  5997, 13113, 11831,\n",
      "         3670,  8122,  8089,   727, 13247,  5828, 13333,   596,  7896,  6959,\n",
      "         5963, 11772, 12399, 11137, 13333, 13069,  5415, 11944,  1822, 11772,\n",
      "         6738,  8036, 11772,  2578, 12987, 13333,   596,  6959,  1009, 12532,\n",
      "        11831,  3670, 11943,  7519,  3145,  8122, 11772, 12952,   790,     0])\n",
      "100\n",
      "tensor([ 9081,  5269,  3670,  8036,  7519,  3145,  8122, 11772, 12952, 11831,\n",
      "         3670,  6381,  4646, 11772, 12524,  8036,   505,   507,  5963, 11772,\n",
      "        12399, 11137,   434,  7599,  8153,  8338,  8036, 11772, 13181,   727,\n",
      "         7855,  2547,   434, 13113,   362,  7855,  9800, 12976, 13333,  7283,\n",
      "         2507,  6398,  5003,  6398,   828,  8122,  9836,  6398, 12316, 11772,\n",
      "        11735,  8036, 11772,  9081,  5269,  6847,  5997, 13113, 11831,  3670,\n",
      "         8122,  8089,   727, 13247,  5828, 13333,   596,  7896,  6959,  5963,\n",
      "        11772, 12399, 11137, 13333, 13069,  5415, 11944,  1822, 11772,  6738,\n",
      "         8036, 11772,  2578, 12987, 13333,   596,  6959,  1009, 12532, 11831,\n",
      "         3670, 11943,  7519,  3145,  8122, 11772, 12952,   790,     0,     0])\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(feature)\n",
    "print(len(feature))\n",
    "print(label)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'moby', 'dick', 'or', 'the', 'whale', 'author', '<unk>']\n",
      "['project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'moby', 'dick', 'or', 'the', 'whale', 'author', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "with open('vocab_obj.pkl', 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "\n",
    "print(vocab.to_tokens(feature.tolist()))\n",
    "print(vocab.to_tokens(label.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try loading the corpus pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[11772, 9081, 5269, 3670, 8036, 7519, 3145, 8122, 11772, 12952]\n",
      "['chapter', 'stowing', 'down', 'and', 'clearing', 'up', 'chapter', 'the', 'doubloon', 'chapter', 'leg', 'and', 'arm', 'chapter', 'the', 'decanter', 'chapter', 'a', 'bower', 'in', 'the', 'arsacides', 'chapter', 'measurement', 'of', 'the', 'whales', 'skeleton', 'chapter', 'the', 'fossil', 'whale', 'chapter', 'does', 'the', 'whales', 'magnitude', '<unk>', 'he', 'perish', 'chapter', 'ahabs', 'leg', 'chapter', 'the', 'carpenter', 'chapter', 'ahab', 'and', 'the', 'carpenter', 'chapter', 'ahab', 'and', 'starbuck', 'in', 'the', 'cabin', 'chapter', 'queequeg', 'in', 'his', 'coffin', 'chapter', 'the', 'pacific', 'chapter', 'the', 'blacksmith', 'chapter', 'the', 'forge', 'chapter', 'the', '<unk>', 'chapter', 'the', 'pequod', 'meets', 'the', 'bachelor', 'chapter', 'the', 'dying', 'whale', 'chapter', 'the', 'whale', 'watch', 'chapter', 'the', 'quadrant', 'chapter', 'the', 'candles', 'chapter', 'the', 'deck', 'towards', 'the']\n"
     ]
    }
   ],
   "source": [
    "with open('corpus.pkl', 'rb')  as file:\n",
    "    corpus = pickle.load(file)\n",
    "\n",
    "print(type(corpus))\n",
    "print(corpus[:10])\n",
    "print(vocab.to_tokens(corpus[500:600]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN and LM from d2l to establish baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.device_count() else 'cpu'\n",
    "print(f\"DEVICE = {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 2, 4])\n",
      "torch.Size([2, 2, 4])\n",
      "tensor([[[1, 9],\n",
      "         [2, 8],\n",
      "         [3, 7],\n",
      "         [4, 6]],\n",
      "\n",
      "        [[2, 7],\n",
      "         [3, 6],\n",
      "         [4, 5],\n",
      "         [5, 4]]])\n",
      "torch.Size([2, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4], [2,3,4,5]])\n",
    "print(a.shape)\n",
    "b = torch.tensor([[9,8,7,6], [7,6,5,4]])\n",
    "print(b.shape)\n",
    "c = torch.stack((a,b), 0)\n",
    "print(c.shape)\n",
    "c = torch.stack((a,b), 1)\n",
    "print(c.shape)\n",
    "c = torch.stack((a,b), 2)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample input shape =  torch.Size([100, 8, 16])\n",
      "torch.Size([100, 8, 32])\n",
      "torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens,sigma = 0.001):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.Wxh = nn.Parameter(torch.rand((num_inputs, num_hiddens), dtype = torch.float, device = DEVICE) * sigma)\n",
    "        self.Whh = nn.Parameter(torch.rand((num_hiddens, num_hiddens), dtype = torch.float, device = DEVICE) * sigma)\n",
    "        self.bh = nn.Parameter(torch.rand((1, num_hiddens), dtype = torch.float, device = DEVICE) * sigma)\n",
    "    \n",
    "    def forward(self, inputs, state = None):\n",
    "        # N is num steps\n",
    "        # n is batch size\n",
    "        # d is num inputs\n",
    "        N, n, d = inputs.shape\n",
    "        if state == None:\n",
    "            #state is not Parameter and will not be used for backprop, initalize state with 0s\n",
    "            state = torch.zeros((n, self.num_hiddens), device = DEVICE)\n",
    "        else:\n",
    "            state, = state\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for X in inputs:\n",
    "            \n",
    "            state = torch.tanh(X @ self.Wxh + state @ self.Whh + self.bh)\n",
    "            outputs.append(state)\n",
    "        \n",
    "        outputs = torch.stack(outputs, 0)\n",
    "        return outputs, state\n",
    "\n",
    "#params\n",
    "num_steps = 100\n",
    "batch_size = 8\n",
    "num_inputs = 16\n",
    "num_hiddens = 32\n",
    "\n",
    "sample_inputs = torch.rand((num_steps, batch_size, num_inputs), device = DEVICE)\n",
    "print('sample input shape = ',sample_inputs.shape)\n",
    "#Testing RNN correctness\n",
    "rnn = RNN(num_inputs, num_hiddens)\n",
    "outputs, state = rnn(sample_inputs)\n",
    "print(outputs.shape)\n",
    "print(state.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape =  torch.Size([8, 16])\n",
      "embedding shaep =  torch.Size([16, 8, 1000])\n",
      "rnn output shape =  torch.Size([16, 8, 32])\n",
      "state output shape =  torch.Size([8, 32])\n",
      "LM output shape =  torch.Size([8, 16, 1000])\n",
      "tensor([256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
      "        256, 256], device='cuda:0')\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn, vocab_size, sigma = 0.001):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigma = sigma\n",
    "\n",
    "        #init params\n",
    "        rnn_num_hiddens = self.rnn.num_hiddens\n",
    "        self.Whq = nn.Parameter(torch.rand((rnn_num_hiddens, self.vocab_size), device= DEVICE) * sigma)\n",
    "        self.bq = nn.Parameter(torch.rand((1, self.vocab_size), device= DEVICE) * sigma)\n",
    "    \n",
    "    def one_hot(self, X):\n",
    "        # original X shape is (batch_size, num_steps)\n",
    "        # we want to encode its shape to (num_steps, batch_size, vocab_size)\n",
    "        # TODO: WHY WE TRANSPOSE LIKE THIS?\n",
    "        # https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#one-hot-encoding\n",
    "        # We often transpose the input so that we will obtain an output of \n",
    "        # shape (number of time steps, batch size, vocabulary size). \n",
    "        # This will allow us to loop more conveniently through the \n",
    "        # outermost dimension for updating hidden states of a minibatch, time step by time step\n",
    "        return F.one_hot(X.T, self.vocab_size).type(torch.float)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print('inputs shape = ', inputs.shape)\n",
    "        embedding = self.one_hot(inputs)\n",
    "        print('embedding shaep = ', embedding.shape)\n",
    "\n",
    "        rnn_outputs, state = self.rnn(embedding)\n",
    "        print('rnn output shape = ', rnn_outputs.shape)\n",
    "        print('state output shape = ', state.shape)\n",
    "\n",
    "        return self.output_layer(rnn_outputs)\n",
    "    \n",
    "    def output_layer(self, rnn_outputs):\n",
    "        outputs = torch.stack([H @ self.Whq + self.bq for H in rnn_outputs], dim = 1)\n",
    "        print('LM output shape = ', outputs.shape)\n",
    "        return outputs\n",
    "    \n",
    "    def train_step(self, input):\n",
    "        pass\n",
    "\n",
    "    def valid_step(self, input):\n",
    "        pass\n",
    "\n",
    "#Test correctness of LM\n",
    "\n",
    "batch_size = 8\n",
    "num_steps = 16\n",
    "vocab_size = 1000\n",
    "num_hiddens = 32\n",
    "\n",
    "sample_inputs = torch.randint(0, vocab_size, (batch_size, num_steps), device = DEVICE)\n",
    "rnn = RNN(vocab_size, num_hiddens)\n",
    "\n",
    "lm  = LanguageModel(rnn, vocab_size)\n",
    "\n",
    "outputs = lm(sample_inputs)\n",
    "pred = torch.argmax(outputs[0], dim = 1)\n",
    "\n",
    "print(pred)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total len of dataset =  24128\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32, shuffle= True)\n",
    "print('total len of dataset = ', len(train_dataloader))\n",
    "\n",
    "feature, label = next(iter(train_dataloader))\n",
    "# print(feature)\n",
    "print(feature.shape)\n",
    "# print(label)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset num steps =  100\n",
      "vocab size  13361\n",
      "batch id  0\n",
      "X: torch.Size([8, 100]), device = cuda:0\n",
      "y: torch.Size([8, 100]), device = cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m rnn \u001b[38;5;241m=\u001b[39m RNN(vocab_size, num_hiddens)\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m LanguageModel(rnn, vocab_size)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloader, valid_dataloader)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, device = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, device = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     26\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot(inputs)\n\u001b[1;32m---> 28\u001b[0m     rnn_outputs, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs shape = \u001b[39m\u001b[38;5;124m'\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding shaep = \u001b[39m\u001b[38;5;124m'\u001b[39m, embedding\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, inputs, state)\u001b[0m\n\u001b[0;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m---> 25\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWxh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWhh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbh\u001b[49m)\n\u001b[0;32m     26\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[0;32m     28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(outputs, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, max_epochs = 100, lr = 0.001):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = lr\n",
    "        self.device = 'cuda' if torch.cuda.device_count() else 'cpu'\n",
    "\n",
    "    def fit(self, model, train_dataloader, valid_dataloader = None):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = self.lr)\n",
    "\n",
    "        for batch_id, (X, y) in enumerate(train_dataloader):\n",
    "            X = X.to(device = self.device)\n",
    "            y = y.to(device = self.device)\n",
    "            print('batch id ', batch_id)\n",
    "            print(f'X: {X.shape}, device = {X.device}')\n",
    "            print(f'y: {y.shape}, device = {y.device}')\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            break\n",
    "\n",
    "trainer = Trainer()\n",
    "print('dataset num steps = ', dataset.num_steps)\n",
    "\n",
    "vocab_size = len(dataset.vocab)\n",
    "print('vocab size ', vocab_size)\n",
    "batch_size = 8 \n",
    "num_hiddens = 16\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle= True)\n",
    "\n",
    "rnn = RNN(vocab_size, num_hiddens)\n",
    "model = LanguageModel(rnn, vocab_size)\n",
    "\n",
    "trainer.fit(lm, train_dataloader=train_dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
