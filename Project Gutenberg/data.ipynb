{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Any\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import torch\n",
    "import pickle\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG dataset loaded\n",
      "building vocab from 345763 tokens\n",
      "built vocab object\n",
      "saving vocab object to vocab.csv\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens = [], min_freq = 0, reserved_tokens = []):\n",
    "        self._build(tokens, min_freq, reserved_tokens)\n",
    "    \n",
    "    def _build(self, tokens, min_freq, reserved_tokens):\n",
    "        print(f'building vocab from {len(tokens)} tokens')\n",
    "        counter = Counter(tokens)\n",
    "        self.token_freq = sorted(counter.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        self.idx_to_tokens = list(sorted(set(['<unk>'] + reserved_tokens \\\n",
    "        + [ token for token, freq in self.token_freq if freq > min_freq])))\n",
    "    \n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_tokens)}\n",
    "\n",
    "        print('built vocab object')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_tokens)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        #if not type list or tuple\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(t) for t in tokens]\n",
    "    \n",
    "    def to_tokens(self, idx):\n",
    "        if not isinstance(idx, (list, tuple)):\n",
    "            return self.idx_to_tokens[idx]\n",
    "        return [self.to_tokens(i) for i in idx]\n",
    "    \n",
    "    # def save_to_file(self, path = 'vocab.csv'):\n",
    "    #     print(f'saving vocab object to {path}')\n",
    "    #     with open(path,'w') as file:\n",
    "    #         file.writelines(f'tokens,idx\\n')\n",
    "    #         for token, index in self.token_to_idx.items():\n",
    "    #             file.writelines(f'{token},{index}\\n')\n",
    "        \n",
    "    # def load_from_file(self, path = 'vocab.csv'):\n",
    "    #     print(f'loading vocab object from {path}')\n",
    "    #     with open(path,'r') as file:\n",
    "    #         for line in file.readlines():\n",
    "    #             token, idx = line.rstrip().split(',')\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self.token_to_idx['<unk>']\n",
    "\n",
    "class ProjectGutenbergDataset(Dataset):\n",
    "    def _download(self, links_path: str = 'links.txt', output_dir: str = 'dataset') -> None:\n",
    "        '''\n",
    "        iterate through links in links.txt in Project Gutenberg to download books\n",
    "        '''\n",
    "        #read links from file\n",
    "        if os.path.exists(output_dir) == False:\n",
    "            print('Downloading books ... ')\n",
    "            books = []\n",
    "            try:\n",
    "                with open(links_path, 'r') as file:\n",
    "                    errors = []\n",
    "                    for link in file.readlines():\n",
    "                        link = link.rstrip()\n",
    "\n",
    "                        res = requests.get(link)\n",
    "                        if res.status_code != 200:\n",
    "                            raise Exception(f\"Failed to Fetch, Error code {res.status_code}\")\n",
    "                        books.append(res.text)\n",
    "                        print(f\"SUCCESS {link}\")\n",
    "\n",
    "                if os.path.exists(output_dir) == False:\n",
    "                    os.mkdir(output_dir) \n",
    "\n",
    "                for id, book in enumerate(books):\n",
    "                    output_path = os.path.join(output_dir, f'book{id}.txt')\n",
    "                    with open(output_path, 'w') as file:\n",
    "                        file.write(book)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error while downloading books, error = ', e)\n",
    "        else:\n",
    "            print(\"PG dataset loaded\")\n",
    "\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text).lower()\n",
    "    \n",
    "    def _tokenize(self, tokenizer, text: str, save_to_file = False) -> list[str]:\n",
    "        tokens = tokenizer(self._preprocess(text))\n",
    "        # save tokens\n",
    "        if save_to_file:\n",
    "            with open('tokens.txt', 'w') as file:\n",
    "                for t in tokens:\n",
    "                    file.writelines(f'{t} ')\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _build(self, dataset_path):\n",
    "        '''\n",
    "        @param:\n",
    "            dataset_path: str, path to PG dataset\n",
    "        @return\n",
    "            corpus: list[int] \n",
    "            vocab: Vocab object\n",
    "        '''\n",
    "        alltext = ''\n",
    "        for file in os.listdir(dataset_path):\n",
    "            filepath = os.path.join(dataset_path, file)\n",
    "            with open(filepath, 'r') as file:\n",
    "                alltext += file.read()\n",
    "    \n",
    "        #init tokenizer\n",
    "        tokenizer = word_tokenize\n",
    "\n",
    "        tokens = self._tokenize(tokenizer, alltext, save_to_file=True)\n",
    "        \n",
    "        vocab = Vocab(tokens, min_freq = 2)\n",
    "        vocab.save_to_file()\n",
    "        \n",
    "        #build corpus, list of indices, [1, 2,100,44,33,...] \n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "\n",
    "        return corpus, vocab\n",
    "\n",
    "    def __init__(self, dataset_path = './dataset/', num_steps = 100):\n",
    "        self._download()\n",
    "        corpus, vocab = self._build(dataset_path)\n",
    "\n",
    "        # Testing to decode the corpus\n",
    "        # decoded = vocab.to_tokens(corpus[:10])\n",
    "        # print(decoded)\n",
    "        N = len(corpus)\n",
    "\n",
    "        array = torch.tensor([corpus[i : i + num_steps + 1] for i in range(N - num_steps)])\n",
    "        self.X = array[:,:-1] \n",
    "        self.Y = array[:,1:] \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "dataset = ProjectGutenbergDataset()\n",
    "\n",
    "feature, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345663\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7512, 5746, 3312, 2300, 5085, 4740, 1995, 5149, 7512, 8225, 7548, 2300,\n",
      "        4007, 2923, 7512, 7962, 5085,  338,  340, 3769, 7512, 7892, 7081,  293,\n",
      "        4788, 5169, 5291, 5085, 7512, 8390,  475, 4964, 1621,  293, 8351,  237,\n",
      "        4964, 6199, 8248, 8466, 4598, 1596, 4020, 3149, 4020,  539, 5149, 6221,\n",
      "        4020, 7851, 7512, 7490, 5085, 7512, 5746, 3312, 4315, 3788, 8351, 7548,\n",
      "        2300, 5149, 5127,  475, 8431, 3697, 8466,  391, 4990, 4393, 3769, 7512,\n",
      "        7892, 7081, 8466, 8319, 3407, 7622, 1175, 7512, 4239, 5085, 7512, 1639,\n",
      "        8258, 8466,  391, 4393,  650, 7968, 7548, 2300, 7621, 4740, 1995, 5149,\n",
      "        7512, 8225,  520,    0])\n",
      "100\n",
      "tensor([5746, 3312, 2300, 5085, 4740, 1995, 5149, 7512, 8225, 7548, 2300, 4007,\n",
      "        2923, 7512, 7962, 5085,  338,  340, 3769, 7512, 7892, 7081,  293, 4788,\n",
      "        5169, 5291, 5085, 7512, 8390,  475, 4964, 1621,  293, 8351,  237, 4964,\n",
      "        6199, 8248, 8466, 4598, 1596, 4020, 3149, 4020,  539, 5149, 6221, 4020,\n",
      "        7851, 7512, 7490, 5085, 7512, 5746, 3312, 4315, 3788, 8351, 7548, 2300,\n",
      "        5149, 5127,  475, 8431, 3697, 8466,  391, 4990, 4393, 3769, 7512, 7892,\n",
      "        7081, 8466, 8319, 3407, 7622, 1175, 7512, 4239, 5085, 7512, 1639, 8258,\n",
      "        8466,  391, 4393,  650, 7968, 7548, 2300, 7621, 4740, 1995, 5149, 7512,\n",
      "        8225,  520,    0,    0])\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(feature)\n",
    "print(len(feature))\n",
    "print(label)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gutenberg.org/ebooks/2701.txt.utf-8\n",
      "ï»¿The Project Gutenberg eBook of Moby Dick; Or, The Whale\n",
      "    \n",
      "This ebook is for the use of anyone \n"
     ]
    }
   ],
   "source": [
    "link = 'https://www.gutenberg.org/ebooks/2701.txt.utf-8'\n",
    "res = requests.get(link)\n",
    "print(link)\n",
    "print(res.text[:100])\n",
    "with open('sample.txt', 'w') as file:\n",
    "    file.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
