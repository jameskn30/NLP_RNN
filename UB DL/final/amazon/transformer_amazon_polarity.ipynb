{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontractions\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\seaborn\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import seaborn objects\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcmod\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\seaborn\\rcmod.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m palettes\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_theme\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreset_defaults\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreset_orig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxes_style\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_style\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplotting_context\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_context\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_palette\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     13\u001b[0m _style_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxes.facecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\seaborn\\palettes.py:9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m husl\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m desaturate, get_color_cycle\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xkcd_rgb, crayons\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_colormap\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\seaborn\\utils.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_rgb\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_kwargs\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\matplotlib\\pyplot.py:55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rcsetup, style\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pylab_helpers, interactive\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cbook\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\matplotlib\\style\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m available, context, library, reload_library, use\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreload_library\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\matplotlib\\style\\core.py:271\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# Load style library\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# ==================\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m _base_library \u001b[38;5;241m=\u001b[39m \u001b[43mread_style_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_LIBRARY_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m library \u001b[38;5;241m=\u001b[39m _StyleLibrary()\n\u001b[0;32m    273\u001b[0m available \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\matplotlib\\style\\core.py:238\u001b[0m, in \u001b[0;36mread_style_directory\u001b[1;34m(style_dir)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return dictionary of styles defined in *style_dir*.\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m styles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m Path(style_dir)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTYLE_EXTENSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m warns:\n\u001b[0;32m    240\u001b[0m         styles[path\u001b[38;5;241m.\u001b[39mstem] \u001b[38;5;241m=\u001b[39m _rc_params_in_file(path)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\pathlib.py:1177\u001b[0m, in \u001b[0;36mPath.glob\u001b[1;34m(self, pattern)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-relative patterns are unsupported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1176\u001b[0m selector \u001b[38;5;241m=\u001b[39m _make_selector(\u001b[38;5;28mtuple\u001b[39m(pattern_parts), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour)\n\u001b[1;32m-> 1177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_from\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\pathlib.py:523\u001b[0m, in \u001b[0;36m_Selector.select_from\u001b[1;34m(self, parent_path)\u001b[0m\n\u001b[0;32m    521\u001b[0m exists \u001b[38;5;241m=\u001b[39m path_cls\u001b[38;5;241m.\u001b[39mexists\n\u001b[0;32m    522\u001b[0m scandir \u001b[38;5;241m=\u001b[39m parent_path\u001b[38;5;241m.\u001b[39m_accessor\u001b[38;5;241m.\u001b[39mscandir\n\u001b[1;32m--> 523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([])\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_from(parent_path, is_dir, exists, scandir)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\pathlib.py:1439\u001b[0m, in \u001b[0;36mPath.is_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;124;03mWhether this path is a directory.\u001b[39;00m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m S_ISDIR(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_mode)\n\u001b[0;32m   1440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ignore_error(e):\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\pathlib.py:1232\u001b[0m, in \u001b[0;36mPath.stat\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstat\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;124;03m    Return the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;124;03m    os.stat() does.\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import re\n",
    "import contractions\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split \n",
    "from torch import nn\n",
    "import torch \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device = ', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "dataset =  os.path.join(root, 'dataset', 'amazon_polarity')\n",
    "#yelp polarity has 2 labels, \n",
    "#yelp dataset has 5 labels, they have the same text content\n",
    "train_datapath = os.path.join(dataset, 'train.csv') \n",
    "test_datapath = os.path.join(dataset, 'test.csv') \n",
    "\n",
    "processed_train_dataset = os.path.join(dataset, 'processed_train_datset.pickle')\n",
    "processed_test_dataset = os.path.join(dataset, 'processed_test_dataset.pickle')\n",
    "vocab_save_path = os.path.join(dataset, 'vocab.pickle')\n",
    "\n",
    "model_save_root = os.path.join(root, 'output', 'transformer')\n",
    "\n",
    "if os.path.exists(model_save_root) == False:\n",
    "    os.makedirs(model_save_root)\n",
    "\n",
    "assert os.path.exists(train_datapath),  f\"train dataset path {train_datapath} not found\"\n",
    "assert os.path.exists(test_datapath),   f\"test dataset path {test_datapath} not found\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "sample_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_datapath, names = [\"review\",\"title\", \"text\"])\n",
    "train_df.drop('title', axis =1, inplace=True)\n",
    "train_df = train_df.sample(frac = sample_size)\n",
    "print(train_df.shape)\n",
    "label_map = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    # 3: 2,\n",
    "    # 4: 3,\n",
    "    # 5: 4,\n",
    "}\n",
    "train_df['review'] = train_df['review'].map(label_map)\n",
    "train_df.reset_index(inplace=True, drop = True)\n",
    "print(train_df['review'].unique())\n",
    "print(train_df['review'].value_counts())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_datapath, names = [\"review\",'title', \"text\"])\n",
    "test_df.drop('title', axis =1, inplace=True)\n",
    "test_df = test_df.sample(frac = sample_size)\n",
    "label_map = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    # 3: 2,\n",
    "    # 4: 3,\n",
    "    # 5: 4,\n",
    "}\n",
    "test_df['review'] = test_df['review'].map(label_map)\n",
    "print(test_df.shape)\n",
    "test_df.reset_index(inplace=True, drop = True)\n",
    "print(test_df['review'].value_counts())\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Text\n",
    "\n",
    "def remove_urls(text):\n",
    "    #if there's link in text, like www.something.com, https://www.something.com,\n",
    "    # replace it with the <url> token\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = pattern.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(\"\\d\", ' ', text)\n",
    "\n",
    "def remove_punctation(text):\n",
    "    return re.sub(r'[^\\w\\s]',' ',text)\n",
    "\n",
    "def expand_contraction(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split(' ') if word not in STOPWORDS])\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    extract feature and label from line and process the text\n",
    "    @params:\n",
    "        text: string, format: __label__2 some text.\n",
    "    @return:\n",
    "        feature: string\n",
    "        label: int, 0: bad review, 1 good review\n",
    "    '''\n",
    "    #Each line has format: __label__2 some text.\n",
    "    #The first part is label, the rest is text feature\n",
    "    #lower case the features\n",
    "    text = text.lower()\n",
    "    #start cleaning\n",
    "\n",
    "    #remove urls in text\n",
    "    text = remove_urls(text)\n",
    "    #remove digits\n",
    "    text = remove_digits(text)\n",
    "    # # #expand contractions\n",
    "    text = expand_contraction(text)\n",
    "    # # #remove punctuations\n",
    "    text = remove_punctation(text)\n",
    "    # # #remove stop words\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    #after cleaning, there's a letter n that occur most frequently\n",
    "    #this don't make sense so remove a standalone letter n\n",
    "    text = ' '.join(t for t in text.split() if t != '' and t != 'n')\n",
    "    return text.strip()\n",
    "\n",
    "test_string = '''This is a test string. Here are some special characters: &,#,$. How about some punctuations? !@#$%^&*()_+=-`~{[]}|:;'<,>.?/\"|https://www.example.com'''\n",
    "\n",
    "clean_text(test_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = train_df.copy()\n",
    "eda_df['text'] = eda_df['text'].apply(lambda s: clean_text(s))\n",
    "eda_df['lengths'] = eda_df['text'].apply(lambda s: len(s))\n",
    "MAX_LENGTH = eda_df['lengths'].max()\n",
    "print(MAX_LENGTH)\n",
    "eda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "eda_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(13,5))\n",
    "sns.histplot(eda_df, x = 'lengths', ax = ax1)\n",
    "ax1.set_title('histogram of lengths')\n",
    "sns.boxplot(eda_df, x = 'lengths', ax = ax2)\n",
    "ax2.set_title('box plot of lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wordcloud of common words\n",
    "# freq = collections.Counter()\n",
    "\n",
    "# for row in eda_df.iterrows():\n",
    "#     label, text = row[1]\n",
    "#     freq.update(text.split())\n",
    "\n",
    "# print('most frequent words')\n",
    "# print(freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_reviews = eda_df.loc[eda_df['review'] == 1]['text'].apply(lambda s: len(s)).values\n",
    "# good_reviews = eda_df.loc[eda_df['review'] == 2]['text'].apply(lambda s: len(s)).values\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(13,5))\n",
    "sns.countplot(train_df, x = 'review', ax = ax1)\n",
    "ax1.set_title('train df')\n",
    "sns.countplot(test_df, x = 'review', ax = ax2)\n",
    "ax2.set_title('test df')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_array(X, maxlength = 800):\n",
    "    X_tokens = []\n",
    "    X_lengths = []\n",
    "    max_len = 0\n",
    "    for text in tqdm(X):\n",
    "        tokens = tokenizer(text)[:maxlength]\n",
    "        max_len = max(max_len, len(tokens))\n",
    "        X_tokens.append(tokens)\n",
    "        X_lengths.append(len(tokens))\n",
    "\n",
    "    print('max len = ', max_len)\n",
    "    return X_tokens, X_lengths\n",
    "\n",
    "\n",
    "def get_ids(tokens, vocab):\n",
    "    ids = vocab.lookup_indices(tokens)\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "def build_train_test_data(feature_train, label_train, min_vocab_freq = 2, **kwargs):\n",
    "    train_tokens, train_lengths = build_array(feature_train)\n",
    "\n",
    "    unk_token = '<unk>'\n",
    "    pad_token = '<pad>'\n",
    "    special_tokens = [unk_token, pad_token]\n",
    "\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "        train_tokens,\n",
    "        min_freq=min_vocab_freq,\n",
    "        specials=special_tokens,\n",
    "    )\n",
    "\n",
    "    unk_id = vocab[unk_token]\n",
    "    pad_id = vocab[unk_token]\n",
    "\n",
    "    vocab.set_default_index(unk_id)\n",
    "\n",
    "    print('vocab len = ', len(vocab))\n",
    "\n",
    "    def convert_to_ids_labels_lengths(token_list, labels, lengths):\n",
    "        id_list = []\n",
    "\n",
    "        for tokens in tqdm(token_list):\n",
    "            ids = get_ids(tokens, vocab)\n",
    "            id_list.append(ids)\n",
    "\n",
    "        #convert y to tensor\n",
    "        labels = torch.tensor(labels)\n",
    "        #convert X lengths to tensor\n",
    "        lengths = torch.tensor(lengths)\n",
    "\n",
    "        return id_list, labels, lengths\n",
    "\n",
    "    train_ids, train_y, train_lengths = convert_to_ids_labels_lengths(train_tokens, label_train, train_lengths)\n",
    "\n",
    "    return (train_tokens, train_ids, train_y, train_lengths), vocab, pad_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df, test_df])\n",
    "all_df['text'] = all_df['text'].apply(lambda s: clean_text(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_df.copy()\n",
    "df['text'] = df['text'].apply(lambda s: clean_text(s))\n",
    "df['lengths'] = df['text'].apply(lambda s: len(s))\n",
    "MAX_LENGTH = df['lengths'].max()\n",
    "print(MAX_LENGTH)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(13,5))\n",
    "sns.histplot(df, x = 'lengths', ax = ax1)\n",
    "ax1.set_title('histogram of lengths')\n",
    "sns.boxplot(df, x = 'lengths', ax = ax2)\n",
    "ax2.set_title('box plot of lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_array = all_df['text'].values\n",
    "y_array = all_df['review'].values\n",
    "\n",
    "(train_tokens, train_ids, train_y, train_lengths) ,vocab, pad_id = build_train_test_data(X_array, y_array)\n",
    "\n",
    "train_ids = pad_sequence(train_ids, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "print('ids shape = ', train_ids.shape )\n",
    "print('y values = ', set(train_y.tolist()) )\n",
    "print('y shape = ', train_y.shape )\n",
    "print('lengths shape = ', train_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, token, id, length in zip(train_y[:1], train_tokens[:1], train_ids[:1], train_lengths[:1]):\n",
    "    print(f'label={label}\\ntokens={token}\\nlength={length}\\nids={id}')\n",
    "\n",
    "print('vocab len = ', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, num_hiddens: int, dropout:float = 0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        assert num_hiddens % 2 == 0, f'num hiddens ({num_hiddens}),has to be even'\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        x = torch.arange(max_len).float().unsqueeze(1)\n",
    "        #N = 10000 as defined in the Attention is All You Need paper\n",
    "        denom = torch.pow(10000, torch.arange(0, num_hiddens,2).float()/num_hiddens)\n",
    "        x = x/denom\n",
    "        self.P[:,:,0::2] = torch.sin(x)\n",
    "        self.P[:,:,1::2] = torch.cos(x)\n",
    "\n",
    "        # self.P = self.P.to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # assert x.device == self.P.device, 'in positional encoding, X should have the same device with P'\n",
    "        batch_size, length, num_hiddens = x.shape\n",
    "        x = x + self.P[:,:length,:].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_class, \n",
    "    num_heads, dim_fc, num_tokens, \n",
    "    dropout = 0.2, num_encoder_layers = 2, batch_first = True):\n",
    "\n",
    "        '''\n",
    "        @params:\n",
    "            input_size: input features or embedding size\n",
    "            num_heads: number of multi attentino heads \n",
    "            dim_fc:  dimension of feedforward layer\n",
    "            dropout: drop out rate\n",
    "            num_tokens: number of tokens in vocabulary (some call vocab_size)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(input_size, dropout)\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model = input_size,\n",
    "            nhead = num_heads,\n",
    "            dim_feedforward= dim_fc,\n",
    "            dropout = dropout,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layers, num_layers = num_encoder_layers)\n",
    "        self.embedding = nn.Embedding(num_tokens, input_size)\n",
    "        self.input_size = input_size \n",
    "\n",
    "        self.fc = nn.Linear(input_size, num_class) \n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def forward(self, src, mask = None):\n",
    "        batch_size, input_len = src.shape\n",
    "        \n",
    "        src = self.embedding(src)\n",
    "        src = src * math.sqrt(self.input_size)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        if mask == None:\n",
    "            mask = nn.Transformer.generate_square_subsequent_mask(input_len).to(src.device)\n",
    "\n",
    "        transformer_output = self.encoder(src, mask)\n",
    "\n",
    "        # print('transformer output shape = ', transformer_output.shape)\n",
    "        output = self.fc(transformer_output[:,-1]) \n",
    "        # print('fc output shape = ', output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100 \n",
    "num_class = 2\n",
    "num_heads = 5\n",
    "fc_hidden_size = 5\n",
    "num_tokens = 1000\n",
    "    \n",
    "clf = TransformerClassifier(input_size, num_class, num_heads, fc_hidden_size, num_tokens)\n",
    "clf.to(DEVICE)\n",
    "# clf.convert_to_device(DEVICE)\n",
    "X = torch.randint(0,1000,(10,5)).to(DEVICE)\n",
    "\n",
    "y = clf(X, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReview(Dataset):\n",
    "\n",
    "    def __init__(self, ids, labels, lengths):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids[idx], self.labels[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YelpReview(train_ids, train_y, train_lengths)\n",
    "# test_dataset = YelpReview(test_ids, train_y, train_lengths)\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "# Save the YelpDataset\n",
    "\n",
    "with open(processed_train_dataset, 'wb') as file:\n",
    "    pickle.dump(train_dataset, file)\n",
    "\n",
    "# with open(processed_test_dataset, 'wb') as file:\n",
    "#     pickle.dump(test_dataset, file)\n",
    "\n",
    "with open(vocab_save_path, 'wb') as file:\n",
    "    pickle.dump(vocab, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open(processed_train_dataset, 'rb') as file:\n",
    "    train_dataset = pickle.load(file)\n",
    "\n",
    "# with open(processed_test_dataset, 'rb') as file:\n",
    "#     test_dataset = pickle.load(file)\n",
    "\n",
    "with open(vocab_save_path, 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "\n",
    "print(train_dataset)\n",
    "print(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "NUM_CLASSES = len(set(train_dataset.labels.tolist()))\n",
    "print('num classes = ', NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model running on dataloader\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_len = int(train_ratio * len(train_dataset))\n",
    "test_val_len = (len(train_dataset) - train_len)//2\n",
    "train_dataset, val_dataset, test_dataset = random_split(train_dataset,[train_len, test_val_len, test_val_len])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "\n",
    "print('num class = ', NUM_CLASSES)\n",
    "print('train dataset len = ', len(train_dataset))\n",
    "print('train dataloader len = ', len(train_dataloader))\n",
    "print('val dataset len = ', len(val_dataset))\n",
    "print('val dataloader len = ', len(val_dataloader))\n",
    "print('test dataset len = ', len(test_dataset))\n",
    "print('test dataloader len = ', len(test_dataloader))\n",
    "\n",
    "(sample_ids, sample_y, sample_lengths) = next(iter(train_dataloader)) \n",
    "print('id shape = ', sample_ids.shape)\n",
    "print('label shape = ', sample_y.shape)\n",
    "print('lengths shape = ', sample_lengths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    for ids, label, length in tqdm(dataloader, desc=\"training...\"):\n",
    "        ids = ids.to(device)\n",
    "        label = label.to(device)\n",
    "        length = length\n",
    "        # prediction = model(ids, length)\n",
    "        prediction = model(ids)\n",
    "\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "        \n",
    "def pickle_load(path):\n",
    "    with open(path, 'wb') as file:\n",
    "        obj = pickle.load(file)\n",
    "    return obj\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    \n",
    "    for ids, label, length in tqdm(dataloader, desc=\"evaluating...\"):\n",
    "        ids = ids.to(device)\n",
    "        length = length\n",
    "        label = label.to(device)\n",
    "        # prediction = model(ids, length)\n",
    "        prediction = model(ids)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "            \n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "def tune(model,train_dataloader, val_dataloader, test_dataloader,  \n",
    "         optimizer, criterion, device, epochs = 10, label = 'baseline', history = None):\n",
    "    print(f\"The model has {model.count_parameters()} trainable parameters\")\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "    best_score = -float(\"inf\")\n",
    "\n",
    "    save_path = os.path.join(model_save_root, label)\n",
    "    if os.path.exists(save_path) == False:\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    if history == None:\n",
    "        history = collections.defaultdict(list)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(val_dataloader, model, criterion, device)\n",
    "        test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"valid_losses\"].append(val_loss)\n",
    "        history[\"valid_accs\"].append(val_acc)\n",
    "        history[\"test_losses\"].append(test_loss)\n",
    "        history[\"test_accs\"].append(test_acc)\n",
    "\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "        print(f\"val_loss: {val_loss:.3f}, valid_acc: {val_acc:.3f}\")\n",
    "        print(f\"test_loss: {test_loss:.3f}, test_acc: {test_acc:.3f}\")\n",
    "        pickle_dump(history, os.path.join(save_path, 'history.pickle'))\n",
    "        torch.save(model, os.path.join(save_path, f\"yelp.transformers.checkpoint.pt\"))\n",
    "\n",
    "        plot(history, save_path = os.path.join(save_path, 'plot.png'))\n",
    "\n",
    "    if test_acc < best_score:\n",
    "        best_score = test_loss\n",
    "        torch.save(model, os.path.join(model_save_root, f\"yelp.transformers.best.pt\"))\n",
    "    \n",
    "    plot(history, save_path = os.path.join(save_path, 'plot.png'), show = True)\n",
    "    return history\n",
    "\n",
    "def plot(history, save_path = None, show = False):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "    epochs = list(range(len(history['train_accs'])))\n",
    "    sns.lineplot(y = history[\"train_accs\"],   label ='train accuracy',  x = epochs, ax = ax1)\n",
    "    sns.lineplot(y = history[\"valid_accs\"],   label ='val accuracy',    x = epochs, ax = ax1)\n",
    "    sns.lineplot(y = history[\"test_accs\"],   label ='test accuracy',    x = epochs, ax = ax1)\n",
    "    ax1.set_title(\"Accuracy\")\n",
    "\n",
    "    sns.lineplot(y = history[\"train_losses\"], label ='train loss', x = epochs, ax = ax2)\n",
    "    sns.lineplot(y = history[\"valid_losses\"],   label ='val loss', x = epochs, ax = ax2)\n",
    "    sns.lineplot(y = history[\"test_losses\"],   label ='test loss', x = epochs, ax = ax2)\n",
    "    ax2.set_title(\"Loss\")\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.GloVe()\n",
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "output_dim = NUM_CLASSES\n",
    "print('output dim = ', output_dim)\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size = embedding_dim,\n",
    "    num_class=  NUM_CLASSES,\n",
    "    num_heads = 5,\n",
    "    dim_fc = 128,\n",
    "    num_tokens = vocab_size,\n",
    "    dropout = dropout_rate\n",
    ")\n",
    "\n",
    "model.embedding.weight.data = pretrained_embedding\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = tune(model,  train_dataloader, val_dataloader, test_dataloader, \n",
    "               optimizer, criterion, epochs = 10, device = DEVICE)\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "test accuracy not improving. Perhaps the train vocab and test vocab is too different. Try merging train and test tokens. See what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
