{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import re\n",
    "import contractions\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split \n",
    "from torch import nn\n",
    "import torch \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device = ', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "#yelp polarity has 2 labels, \n",
    "#yelp dataset has 5 labels, they have the same text content\n",
    "train_datapath = os.path.join(root, 'dataset', 'yelp_review_polarity_csv', 'train.csv') \n",
    "test_datapath = os.path.join(root,  'dataset', 'yelp_review_polarity_csv', 'test.csv') \n",
    "\n",
    "processed_train_dataset = os.path.join(root, 'dataset', 'yelp_review_polarity_csv', 'processed_train_datset.pickle')\n",
    "processed_test_dataset = os.path.join(root, 'dataset', 'yelp_review_polarity_csv', 'processed_test_dataset.pickle')\n",
    "vocab_save_path = os.path.join(root, 'dataset', 'yelp_review_polarity_csv', 'vocab.pickle')\n",
    "\n",
    "model_save_root = os.path.join(root, 'output')\n",
    "\n",
    "assert os.path.exists(train_datapath),  f\"train dataset path {train_datapath} not found\"\n",
    "assert os.path.exists(test_datapath),   f\"test dataset path {test_datapath} not found\"\n",
    "\n",
    "sample_size = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Who pays attention to health code violations a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Cold, slow, overpriced dont waste your $$ orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>When I heard Brand was opening, it meant Black...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm not sure what it is about Brewery guys tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Very nervous finding a new stylist. Read revie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   review                                               text\n",
       "0       2  Who pays attention to health code violations a...\n",
       "1       1  Cold, slow, overpriced dont waste your $$ orde...\n",
       "2       2  When I heard Brand was opening, it meant Black...\n",
       "3       2  I'm not sure what it is about Brewery guys tha...\n",
       "4       2  Very nervous finding a new stylist. Read revie..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_datapath, names = [\"review\",\"text\"])\n",
    "train_df = train_df.sample(frac = sample_size)\n",
    "print(train_df.shape)\n",
    "train_df.reset_index(inplace=True, drop = True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19000, 2)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_datapath, names = [\"review\",\"text\"])\n",
    "test_df = test_df.sample(frac = sample_size)\n",
    "print(test_df.shape)\n",
    "test_df.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test string special characters punctuations _'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Text\n",
    "\n",
    "def remove_urls(text):\n",
    "    #if there's link in text, like www.something.com, https://www.something.com,\n",
    "    # replace it with the <url> token\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = pattern.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(\"\\d\", ' ', text)\n",
    "\n",
    "def remove_punctation(text):\n",
    "    return re.sub(r'[^\\w\\s]',' ',text)\n",
    "\n",
    "def expand_contraction(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split(' ') if word not in STOPWORDS])\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    extract feature and label from line and process the text\n",
    "    @params:\n",
    "        text: string, format: __label__2 some text.\n",
    "    @return:\n",
    "        feature: string\n",
    "        label: int, 0: bad review, 1 good review\n",
    "    '''\n",
    "    #Each line has format: __label__2 some text.\n",
    "    #The first part is label, the rest is text feature\n",
    "    #lower case the features\n",
    "    text = text.lower()\n",
    "    #start cleaning\n",
    "\n",
    "    #remove urls in text\n",
    "    text = remove_urls(text)\n",
    "    #remove digits\n",
    "    text = remove_digits(text)\n",
    "    # # #expand contractions\n",
    "    text = expand_contraction(text)\n",
    "    # # #remove punctuations\n",
    "    text = remove_punctation(text)\n",
    "    # # #remove stop words\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    #after cleaning, there's a letter n that occur most frequently\n",
    "    #this don't make sense so remove a standalone letter n\n",
    "    text = ' '.join(t for t in text.split() if t != '' and t != 'n')\n",
    "    return text.strip()\n",
    "\n",
    "test_string = '''This is a test string. Here are some special characters: &,#,$. How about some punctuations? !@#$%^&*()_+=-`~{[]}|:;'<,>.?/\"|https://www.example.com'''\n",
    "\n",
    "clean_text(test_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = train_df.copy()\n",
    "eda_df['text'] = eda_df['text'].apply(lambda s: clean_text(s))\n",
    "eda_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud of common words\n",
    "freq = collections.Counter()\n",
    "\n",
    "for row in eda_df.iterrows():\n",
    "    label, text = row[1]\n",
    "    freq.update(text.split())\n",
    "\n",
    "print('most frequent words')\n",
    "print(freq.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews = eda_df.loc[eda_df['review'] == 1]['text'].apply(lambda s: len(s)).values\n",
    "good_reviews = eda_df.loc[eda_df['review'] == 2]['text'].apply(lambda s: len(s)).values\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "sns.histplot(x = bad_reviews, label = 'bad reviews', ax = ax1)\n",
    "ax1.set_title('bad reviews')\n",
    "sns.histplot(x = good_reviews, label = 'good reviews', ax = ax2)\n",
    "ax2.set_title('good reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eda_df['review'].value_counts())\n",
    "sns.countplot(x = eda_df['review'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_array(X, maxlength = 256):\n",
    "    X_tokens = []\n",
    "    X_lengths = []\n",
    "    for text in tqdm(X):\n",
    "        tokens = tokenizer(text)[:maxlength]\n",
    "        X_tokens.append(tokens)\n",
    "        X_lengths.append(len(tokens))\n",
    "\n",
    "    return X_tokens, X_lengths\n",
    "\n",
    "\n",
    "def get_ids(tokens, vocab):\n",
    "    ids = vocab.lookup_indices(tokens)\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "def build_train_test_data(feature_train, label_train, min_vocab_freq = 5, **kwargs):\n",
    "    train_tokens, train_lengths = build_array(feature_train)\n",
    "\n",
    "    unk_token = '<unk>'\n",
    "    pad_token = '<pad>'\n",
    "    special_tokens = [unk_token, pad_token]\n",
    "\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "        train_tokens,\n",
    "        min_freq=min_vocab_freq,\n",
    "        specials=special_tokens,\n",
    "    )\n",
    "\n",
    "    unk_id = vocab[unk_token]\n",
    "    pad_id = vocab[unk_token]\n",
    "\n",
    "    vocab.set_default_index(unk_id)\n",
    "\n",
    "    print('vocab len = ', len(vocab))\n",
    "\n",
    "    def convert_to_ids_labels_lengths(token_list, labels, lengths):\n",
    "        id_list = []\n",
    "\n",
    "        for tokens in tqdm(token_list):\n",
    "            ids = get_ids(tokens, vocab)\n",
    "            id_list.append(ids)\n",
    "\n",
    "        #convert y to tensor\n",
    "        labels = torch.tensor([0 if label == 1 else 1 for label in labels])\n",
    "        #convert X lengths to tensor\n",
    "        lengths = torch.tensor(lengths)\n",
    "\n",
    "        return id_list, labels, lengths\n",
    "\n",
    "    train_ids, train_y, train_lengths = convert_to_ids_labels_lengths(train_tokens, label_train, train_lengths)\n",
    "\n",
    "    return (train_tokens, train_ids, train_y, train_lengths), vocab, pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280000/280000 [00:10<00:00, 27753.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len =  46855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280000/280000 [00:07<00:00, 35681.00it/s]\n",
      "100%|██████████| 19000/19000 [00:00<00:00, 28298.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len =  12853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19000/19000 [00:00<00:00, 44462.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids shape =  torch.Size([280000, 256])\n",
      "y shape =  torch.Size([280000])\n",
      "lengths shape =  torch.Size([280000])\n",
      "test\n",
      "ids shape =  torch.Size([19000, 256])\n",
      "y shape =  torch.Size([19000])\n",
      "lengths shape =  torch.Size([19000])\n"
     ]
    }
   ],
   "source": [
    "X_array = train_df['text'].apply(lambda s: clean_text(s)).values\n",
    "y_array = train_df['review'].values\n",
    "\n",
    "X_array_test = test_df['text'].apply(lambda s: clean_text(s)).values\n",
    "y_array_test = test_df['review'].values\n",
    "\n",
    "(train_tokens, train_ids, train_y, train_lengths), vocab, pad_id = build_train_test_data(X_array, y_array)\n",
    "(test_tokens, test_ids, test_y, test_lengths), _, _= build_train_test_data(X_array_test, y_array_test)\n",
    "\n",
    "train_ids = pad_sequence(train_ids, batch_first=True, padding_value=pad_id)\n",
    "test_ids = pad_sequence(test_ids, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "print('ids shape = ', train_ids.shape )\n",
    "print('y shape = ', train_y.shape )\n",
    "print('lengths shape = ', train_lengths.shape)\n",
    "\n",
    "print('test')\n",
    "print('ids shape = ', test_ids.shape )\n",
    "print('y shape = ', test_y.shape )\n",
    "print('lengths shape = ', test_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=1\n",
      "tokens=['pays', 'attention', 'health', 'code', 'violations', 'anyway', 'tell', 'know', 'place', 'good', 'every', 'single', 'time', 'go', 'dinner', 'non', 'asian', 'place', 'good', 'enough', 'locals', 'good', 'enough', 'brazillon', 'pres', 'g', 'w', 'bush', 'would', 'say', 'chinese', 'restaurants', 'back', 'east', 'including', 'philly', 'nyc', 'place', 'definitely', 'hold', 'yes', 'little', 'dirty', 'yes', 'service', 'spotty', 'constantly', 'coming', 'around', 'refilling', 'water', 'everytime', 'take', 'sip', 'food', 'first', 'rate', 'recommend', 'crispy', 'spinach', 'dish', 'menu', 'house', 'specialties', 'bed', 'fried', 'spinach', 'spicy', 'chicken', 'shrimp', 'vegetable', 'medley', 'top', 'fantastic', 'wonton', 'soup', 'also', 'good', 'little', 'different', 'used', 'wontons', 'filled', 'meat', 'seafood', 'seafood', 'c', 'fu', 'wonderful', 'fresh', 'prepared', 'way', 'ask', 'keeping', 'friday', 'night', 'chinese', 'food', 'night', 'tradition', 'dad', 'bestowed', 'upon', 'kid', 'typically', 'find', 'eating', 'least', 'getting', 'take', 'week', 'give', 'c', 'fu', 'two', 'thumbs']\n",
      "length=116\n",
      "ids=tensor([ 5542,   712,  1466,  2499,  8545,   742,   212,    36,     3,     4,\n",
      "          100,   749,     9,    13,   112,   439,   734,     3,     4,   131,\n",
      "         1610,     4,   131,     0, 31130,  1777,   678, 11572,     5,    61,\n",
      "          494,   284,    12,  1337,   664,  2262,  2323,     3,    82,   888,\n",
      "          325,    37,   492,   325,    11,  4836,  1793,   234,    68,  5052,\n",
      "          188,  2491,    66,  3338,     2,    32,   992,   133,   631,  1109,\n",
      "          221,    52,   239,  6428,   644,   224,  1109,   343,    44,   288,\n",
      "         1896,  9902,   166,   420,  3401,   244,    21,     4,    37,   137,\n",
      "          192,  4414,   697,   155,   622,   622,   674,  5870,   413,   111,\n",
      "          656,    39,   162,  1837,   561,    51,   494,     2,    51,  4681,\n",
      "         1721, 33103,   685,   973,  1758,    91,   251,   191,   183,    66,\n",
      "          278,    77,   674,  5870,    40,  2592,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n",
      "vocab len =  46855\n"
     ]
    }
   ],
   "source": [
    "for label, token, id, length in zip(train_y[:1], train_tokens[:1], train_ids[:1], train_lengths[:1]):\n",
    "    print(f'label={label}\\ntokens={token}\\nlength={length}\\nids={id}')\n",
    "\n",
    "print('vocab len = ', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, num_hiddens: int, dropout:float = 0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        assert num_hiddens % 2 == 0, f'num hiddens ({num_hiddens}),has to be even'\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        x = torch.arange(max_len).float().unsqueeze(1)\n",
    "        #N = 10000 as defined in the Attention is All You Need paper\n",
    "        denom = torch.pow(10000, torch.arange(0, num_hiddens,2).float()/num_hiddens)\n",
    "        x = x/denom\n",
    "        self.P[:,:,0::2] = torch.sin(x)\n",
    "        self.P[:,:,1::2] = torch.cos(x)\n",
    "\n",
    "        # self.P = self.P.to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # assert x.device == self.P.device, 'in positional encoding, X should have the same device with P'\n",
    "        batch_size, length, num_hiddens = x.shape\n",
    "        x = x + self.P[:,:length,:].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_class, \n",
    "    num_heads, dim_fc, num_tokens, \n",
    "    dropout = 0.2, num_encoder_layers = 2, batch_first = True):\n",
    "\n",
    "        '''\n",
    "        @params:\n",
    "            input_size: input features or embedding size\n",
    "            num_heads: number of multi attentino heads \n",
    "            dim_fc:  dimension of feedforward layer\n",
    "            dropout: drop out rate\n",
    "            num_tokens: number of tokens in vocabulary (some call vocab_size)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(input_size, dropout)\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model = input_size,\n",
    "            nhead = num_heads,\n",
    "            dim_feedforward= dim_fc,\n",
    "            dropout = dropout,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layers, num_layers = num_encoder_layers)\n",
    "        self.embedding = nn.Embedding(num_tokens, input_size)\n",
    "        self.input_size = input_size \n",
    "\n",
    "        self.fc = nn.Linear(input_size, num_class) \n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def forward(self, src, mask = None):\n",
    "        batch_size, input_len = src.shape\n",
    "        \n",
    "        src = self.embedding(src)\n",
    "        src = src * math.sqrt(self.input_size)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        if mask == None:\n",
    "            mask = nn.Transformer.generate_square_subsequent_mask(input_len).to(src.device)\n",
    "\n",
    "        transformer_output = self.encoder(src, mask)\n",
    "\n",
    "        # print('transformer output shape = ', transformer_output.shape)\n",
    "        output = self.fc(transformer_output[:,-1]) \n",
    "        # print('fc output shape = ', output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100 \n",
    "num_class = 2\n",
    "num_heads = 5\n",
    "fc_hidden_size = 5\n",
    "num_tokens = 1000\n",
    "    \n",
    "clf = TransformerClassifier(input_size, num_class, num_heads, fc_hidden_size, num_tokens, device = DEVICE)\n",
    "clf.to(DEVICE)\n",
    "# clf.convert_to_device(DEVICE)\n",
    "X = torch.randint(0,1000,(10,5)).to(DEVICE)\n",
    "\n",
    "y = clf(X, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReview(Dataset):\n",
    "\n",
    "    def __init__(self, ids, labels, lengths):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids[idx], self.labels[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m YelpReview(\u001b[43mtrain_ids\u001b[49m, train_y, train_lengths)\n\u001b[0;32m      2\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m YelpReview(test_ids, train_y, train_lengths)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_ids' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = YelpReview(train_ids, train_y, train_lengths)\n",
    "test_dataset = YelpReview(test_ids, train_y, train_lengths)\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "# Save the YelpDataset\n",
    "\n",
    "with open(processed_train_dataset, 'wb') as file:\n",
    "    pickle.dump(train_dataset, file)\n",
    "\n",
    "with open(processed_test_dataset, 'wb') as file:\n",
    "    pickle.dump(test_dataset, file)\n",
    "\n",
    "with open(vocab_save_path, 'wb') as file:\n",
    "    pickle.dump(vocab, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.YelpReview object at 0x0000020C62707850>\n",
      "Vocab()\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "with open(processed_train_dataset, 'rb') as file:\n",
    "    train_dataset = pickle.load(file)\n",
    "\n",
    "with open(processed_test_dataset, 'rb') as file:\n",
    "    test_dataset = pickle.load(file)\n",
    "\n",
    "with open(vocab_save_path, 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "\n",
    "print(train_dataset)\n",
    "print(vocab)\n",
    "\n",
    "NUM_CLASSES = len(set(train_dataset.labels.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num class =  2\n",
      "train dataset len =  179200\n",
      "train dataloader len =  1400\n",
      "val dataset len =  44800\n",
      "val dataloader len =  350\n",
      "test dataset len =  19000\n",
      "test dataloader len =  149\n",
      "id shape =  torch.Size([128, 256])\n",
      "label shape =  torch.Size([128])\n",
      "lengths shape =  torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# test model running on dataloader\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_len = int(train_ratio * len(train_dataset))\n",
    "val_len = len(train_dataset) - train_len\n",
    "train_dataset, val_dataset = random_split(train_dataset,[train_len, val_len])\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "\n",
    "print('num class = ', NUM_CLASSES)\n",
    "print('train dataset len = ', len(train_dataset))\n",
    "print('train dataloader len = ', len(train_dataloader))\n",
    "print('val dataset len = ', len(val_dataset))\n",
    "print('val dataloader len = ', len(val_dataloader))\n",
    "print('test dataset len = ', len(test_dataset))\n",
    "print('test dataloader len = ', len(test_dataloader))\n",
    "\n",
    "(sample_ids, sample_y, sample_lengths) = next(iter(train_dataloader)) \n",
    "print('id shape = ', sample_ids.shape)\n",
    "print('label shape = ', sample_y.shape)\n",
    "print('lengths shape = ', sample_lengths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    for ids, label, length in tqdm(dataloader, desc=\"training...\"):\n",
    "        ids = ids.to(device)\n",
    "        label = label.to(device)\n",
    "        length = length\n",
    "        # prediction = model(ids, length)\n",
    "        prediction = model(ids)\n",
    "\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "        \n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    \n",
    "    for ids, label, length in tqdm(dataloader, desc=\"evaluating...\"):\n",
    "        ids = ids.to(device)\n",
    "        length = length\n",
    "        label = label.to(device)\n",
    "        # prediction = model(ids, length)\n",
    "        prediction = model(ids)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "\n",
    "            \n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "def tune(model,train_dataloader, val_dataloader, test_dataloader,  optimizer, criterion, device, epochs = 10, earlyStopping=False):\n",
    "    print(f\"The model has {model.count_parameters()} trainable parameters\")\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    history = []\n",
    "\n",
    "    history = collections.defaultdict(list)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(val_dataloader, model, criterion, device)\n",
    "        test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"valid_losses\"].append(val_loss)\n",
    "        history[\"valid_accs\"].append(val_acc)\n",
    "        history[\"test_losses\"].append(test_loss)\n",
    "        history[\"test_accs\"].append(test_acc)\n",
    "\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "        print(f\"val_loss: {val_loss:.3f}, valid_acc: {val_acc:.3f}\")\n",
    "        print(f\"test_loss: {test_loss:.3f}, test_acc: {test_acc:.3f}\")\n",
    "\n",
    "    if test_loss < best_valid_loss:\n",
    "        best_valid_loss = test_loss\n",
    "        torch.save(os.path.join(model_save_root, f\"transformers.checkpoint.pt\"))\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "    epochs = list(range(len(history['train_accs'])))\n",
    "    sns.lineplot(y = history[\"train_accs\"],   label ='train accuracy',  x = epochs, ax = ax1)\n",
    "    sns.lineplot(y = history[\"valid_accs\"],   label ='val accuracy',    x = epochs, ax = ax1)\n",
    "    sns.lineplot(y = history[\"testaccs\"],   label ='test accuracy',    x = epochs, ax = ax1)\n",
    "    ax1.set_title(\"Accuracy\")\n",
    "\n",
    "    sns.lineplot(y = history[\"train_losses\"], label ='train loss', x = epochs, ax = ax2)\n",
    "    sns.lineplot(y = history[\"valid_losses\"],   label ='val loss', x = epochs, ax = ax2)\n",
    "    sns.lineplot(y = history[\"test_losses\"],   label ='test loss', x = epochs, ax = ax2)\n",
    "    ax2.set_title(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.GloVe()\n",
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dim =  2\n",
      "The model has 15260594 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   9%|▉         | 127/1400 [00:36<06:01,  3.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     23\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 25\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m               \u001b[49m\u001b[43mearlyStopping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m plot(history)\n",
      "Cell \u001b[1;32mIn[13], line 61\u001b[0m, in \u001b[0;36mtune\u001b[1;34m(model, train_dataloader, val_dataloader, test_dataloader, optimizer, criterion, device, epochs, earlyStopping)\u001b[0m\n\u001b[0;32m     58\u001b[0m history \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 61\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(val_dataloader, model, criterion, device)\n\u001b[0;32m     63\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(test_dataloader, model, criterion, device)\n",
      "Cell \u001b[1;32mIn[13], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m get_accuracy(prediction, label)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 5e-4\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "output_dim = NUM_CLASSES\n",
    "print('output dim = ', output_dim)\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size = embedding_dim,\n",
    "    num_class=  NUM_CLASSES,\n",
    "    num_heads = 5,\n",
    "    dim_fc = 64,\n",
    "    num_tokens = vocab_size,\n",
    "    dropout = dropout_rate\n",
    ")\n",
    "\n",
    "model.embedding.weight.data = pretrained_embedding\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = tune(model,  train_dataloader, val_dataloader, test_dataloader, \n",
    "               optimizer, criterion, epochs = 10, device = DEVICE, \n",
    "               earlyStopping = False)\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
