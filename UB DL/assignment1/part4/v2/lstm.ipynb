{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords =  {'same', \"weren't\", \"she's\", 'were', 'not', 'mightn', 'where', 'under', 'between', 'hasn', 'shan', 'what', 'weren', 'yours', 'too', 'who', 'am', \"isn't\", \"mustn't\", 'an', 'or', \"don't\", 'ourselves', 'herself', 'in', 'off', 'doesn', \"it's\", 'below', 'after', 'the', 'is', 'while', \"you'd\", 'he', 'him', 'because', 'did', 'a', 'had', 'nor', \"that'll\", 'has', 'as', 'isn', \"mightn't\", 'her', 'again', \"wasn't\", \"won't\", 'have', 'wouldn', 'against', 'my', \"hadn't\", 'was', 'they', 'with', \"didn't\", 'i', 'which', 'himself', 'such', 'up', \"needn't\", 'ain', 'that', 'at', 'should', 'those', 'some', 'no', 'most', 'but', 'each', 'don', 'his', \"hasn't\", 'it', 'down', 'more', \"you've\", \"shouldn't\", 'wasn', \"wouldn't\", 'during', 'our', 'do', 'shouldn', 'than', 'can', \"you're\", 'over', 'needn', 'themselves', 'by', \"haven't\", 'be', 'are', 'other', 'being', \"couldn't\", 'aren', 'both', 'its', 'couldn', 'them', 'll', 're', 'y', 'any', 'won', 'myself', 'through', 'there', 'once', \"you'll\", 'how', 'doing', 'me', 'about', 'to', 'yourself', 'why', 'from', 'haven', 'this', 'will', \"aren't\", 'ma', 'few', 've', 't', 'until', 'itself', 'above', 'hadn', 'all', 'yourselves', 'your', 'then', 'here', 'having', \"doesn't\", 'been', 'and', 'd', 'she', 'before', 'of', 's', 'now', 'theirs', 'for', 'on', 'ours', 'does', \"shan't\", 'so', 'own', 'these', 'you', 'we', 'their', 'out', 'further', 'very', 'into', 'just', \"should've\", 'whom', 'hers', 'if', 'didn', 'mustn', 'when', 'o', 'm', 'only'}\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import contractions \n",
    "\n",
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#The essentials\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "print('stopwords = ', STOPWORDS)\n",
    "\n",
    "root = os.getcwd()\n",
    "datasetpath = os.path.join(root, 'dataset', 'amazon_review_ft')\n",
    "assert os.path.exists(datasetpath), f'dataset path does not exist {datasetpath}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_TO_IDX = os.path.join(root, 'output', 'word2idx.pkl')\n",
    "IDX_TO_WORD = os.path.join(root, 'output', 'idx2word.pkl')\n",
    "with open(WORD_TO_IDX, 'rb') as file:\n",
    "    word2idx = pickle.load(file)\n",
    "with open(IDX_TO_WORD, 'rb') as file:\n",
    "    idx2word = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of dataset =  (288000, 173)\n"
     ]
    }
   ],
   "source": [
    "#load checkpoint after tokens np\n",
    "X_TOKENS_TRAIN_OUTPUT_PATH = os.path.join(root, 'output', 'X_tokens.npy')\n",
    "Y_TOKENS_TRAIN_OUTPUT_PATH = os.path.join(root, 'output', 'y_tokens.npy')\n",
    "with open(X_TOKENS_TRAIN_OUTPUT_PATH, 'rb') as file:\n",
    "    X_train = np.load(file, allow_pickle = True)\n",
    "with open(Y_TOKENS_TRAIN_OUTPUT_PATH, 'rb') as file:\n",
    "    y_train = np.load(file)\n",
    "\n",
    "print(\"total length of dataset = \", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288000\n",
      "train dataloader shape =  2880\n",
      "X shape =  torch.Size([100, 173])\n",
      "y shape =  torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "print(len(train_dataset))\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_dataloder = DataLoader(train_dataset, shuffle = True, batch_size = BATCH_SIZE)\n",
    "print('train dataloader shape = ', len(train_dataloder))\n",
    "sampleX, sampleY = next(iter(train_dataloder))\n",
    "print(\"X shape = \", sampleX.shape)\n",
    "print(\"y shape = \", sampleY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embed_size, hidden_size, num_layers, dropout_rate = 0.5):\n",
    "        super(LSTMClassifier, self).__init__() \n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout = dropout_rate, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dense = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def init_state(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device),\n",
    "                      weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device))\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        batch_size, length = X.shape\n",
    "        X = X.long()\n",
    "        embedding = self.embedding(X)\n",
    "        outputs, state = self.lstm(embedding, state)\n",
    "        # print('hidden state shape = ', state[0].shape)\n",
    "        # print('cell shape = ', state[1].shape)\n",
    "        # print('outputs shape = ', outputs.shape)\n",
    "        outputs = outputs.contiguous().view(-1, self.hidden_size)\n",
    "        # print('outputs after contiguous shape = ', outputs.shape)\n",
    "        outputs = self.dropout(outputs) \n",
    "        outputs = self.dense(outputs)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        # print('outputs after dense shape = ', outputs.shape)\n",
    "        y_pred = outputs.view(batch_size, -1) \n",
    "        # print('y_pred shape = ', y_pred.shape)\n",
    "        y_pred = y_pred[:, -1]\n",
    "        # print('last layer y_pred shape = ', y_pred.shape)\n",
    "        return y_pred, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(word2idx)\n",
    "# output_size =1\n",
    "# embed_size = 400\n",
    "# hidden_size = 500\n",
    "# num_layers = 2\n",
    "\n",
    "# test_model = LSTMClassifier(vocab_size, output_size, embed_size, hidden_size, num_layers)\n",
    "# sampleX = torch.rand(10, 20)\n",
    "# sampleY = torch.rand(10)\n",
    "\n",
    "# state = test_model.init_state(sampleX.shape[0], 'cpu')\n",
    "# print('hidden state shape = ', state[0].shape)\n",
    "# print('cell shape = ', state[1].shape)\n",
    "\n",
    "# sampleY_pred, state = test_model(sampleX, state)\n",
    "# print('final y_pred shape = ', sampleY.shape)\n",
    "\n",
    "# loss_fn = nn.BCELoss()\n",
    "\n",
    "# loss = loss_fn(sampleY, sampleY)\n",
    "# print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "# del model\n",
    "# del train_dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using  cuda\n",
      "vocab len =  1002\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#clear cuda memory\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(vocab_size, output_size, embed_size, hidden_size, num_layers)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\cuda\\memory.py:133\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 133\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('using ', device)\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "print('vocab len = ', vocab_size)\n",
    "output_size =1\n",
    "embed_size = 100\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "GRAD_CLIP = 5\n",
    "EPOCHS = 10\n",
    "LR = 0.1\n",
    "\n",
    "#clear cuda memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model = LSTMClassifier(vocab_size, output_size, embed_size, hidden_size, num_layers)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "model.train()\n",
    "history = []\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "CHECKPOINT_OUTPUT = os.path.join(root, 'output', 'lstm.checkpoint.torch')\n",
    "HISTORY_OUTPUT = os.path.join(root, 'output', 'lstm.history.pkl')\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    state = None\n",
    "    running_loss =0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loop = tqdm(train_dataloder)\n",
    "    model.train()\n",
    "\n",
    "    for X,y in loop:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        batch_size = X.shape[0]\n",
    "        total += batch_size\n",
    "        # DANGER: this batch size must evenly divisible by total len of dataset. \n",
    "        # else it will FAIL, configure BATCH_SIZE in dataloader\n",
    "        if state == None:\n",
    "            state = model.init_state(batch_size, device)\n",
    "\n",
    "        state = [state[0].data, state[1].data]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, state = model(X, state)\n",
    "        loss = loss_fn(y_pred, y.float())\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        #Get training accuracy\n",
    "        y_pred = y_pred.round().cpu().detach().numpy()\n",
    "        y = y.cpu().numpy()\n",
    "        correct += (y_pred == y).sum()\n",
    "\n",
    "        loop.set_description(f'running loss = {running_loss}, accuracy = {correct / total:.5f}')\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    history.append({\n",
    "        'train_loss': running_loss,\n",
    "        'train_acc': accuracy\n",
    "    })\n",
    "\n",
    "    if best_score < accuracy:\n",
    "        torch.save(model, f'lstm.checkpoint.{accuracy:.3f}.torch')\n",
    "        best_score = accuracy\n",
    "\n",
    "    with open(HISTORY_OUTPUT, 'wb') as file:\n",
    "        pickle.dump(history, file)\n",
    "\n",
    "\n",
    "    # print('acc = ', correct / total)\n",
    "    # print('loss value = ', running_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
