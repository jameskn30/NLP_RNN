{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-29T20:16:04.640195Z",
     "iopub.status.busy": "2021-01-29T20:16:04.639501Z",
     "iopub.status.idle": "2021-01-29T20:16:04.649001Z",
     "shell.execute_reply": "2021-01-29T20:16:04.648422Z"
    },
    "papermill": {
     "duration": 0.03699,
     "end_time": "2021-01-29T20:16:04.649181",
     "exception": false,
     "start_time": "2021-01-29T20:16:04.612191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:16:04.695378Z",
     "iopub.status.busy": "2021-01-29T20:16:04.694727Z",
     "iopub.status.idle": "2021-01-29T20:16:06.687846Z",
     "shell.execute_reply": "2021-01-29T20:16:06.688420Z"
    },
    "papermill": {
     "duration": 2.018377,
     "end_time": "2021-01-29T20:16:06.688622",
     "exception": false,
     "start_time": "2021-01-29T20:16:04.670245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "datasetpath = os.path.join(root, 'dataset', 'amazon_review_ft')\n",
    "assert os.path.exists(datasetpath), f'dataset path does not exist {datasetpath}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:16:06.729571Z",
     "iopub.status.busy": "2021-01-29T20:16:06.728918Z",
     "iopub.status.idle": "2021-01-29T20:16:06.733550Z",
     "shell.execute_reply": "2021-01-29T20:16:06.733129Z"
    },
    "papermill": {
     "duration": 0.026353,
     "end_time": "2021-01-29T20:16:06.733746",
     "exception": false,
     "start_time": "2021-01-29T20:16:06.707393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File(os.path.join(datasetpath, 'train.ft.txt.bz2'))\n",
    "test_file = bz2.BZ2File(os.path.join(datasetpath, 'test.ft.txt.bz2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:16:06.781965Z",
     "iopub.status.busy": "2021-01-29T20:16:06.781158Z",
     "iopub.status.idle": "2021-01-29T20:17:51.847135Z",
     "shell.execute_reply": "2021-01-29T20:17:51.846596Z"
    },
    "papermill": {
     "duration": 105.094045,
     "end_time": "2021-01-29T20:17:51.847277",
     "exception": false,
     "start_time": "2021-01-29T20:16:06.753232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018106,
     "end_time": "2021-01-29T20:17:51.884530",
     "exception": false,
     "start_time": "2021-01-29T20:17:51.866424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Convert bytes object into utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:17:51.925808Z",
     "iopub.status.busy": "2021-01-29T20:17:51.925134Z",
     "iopub.status.idle": "2021-01-29T20:17:51.928165Z",
     "shell.execute_reply": "2021-01-29T20:17:51.927684Z"
    },
    "papermill": {
     "duration": 0.025387,
     "end_time": "2021-01-29T20:17:51.928293",
     "exception": false,
     "start_time": "2021-01-29T20:17:51.902906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pct of the data used for the models\n",
    "pct = 0.3\n",
    "num_train = int(len(train_file)*pct) #max 3600000\n",
    "num_test = int(len(test_file)*pct) #max 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:17:52.015832Z",
     "iopub.status.busy": "2021-01-29T20:17:52.010788Z",
     "iopub.status.idle": "2021-01-29T20:17:52.997846Z",
     "shell.execute_reply": "2021-01-29T20:17:52.997353Z"
    },
    "papermill": {
     "duration": 1.051638,
     "end_time": "2021-01-29T20:17:52.997973",
     "exception": false,
     "start_time": "2021-01-29T20:17:51.946335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_train]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:17:53.039887Z",
     "iopub.status.busy": "2021-01-29T20:17:53.039031Z",
     "iopub.status.idle": "2021-01-29T20:17:53.042397Z",
     "shell.execute_reply": "2021-01-29T20:17:53.042903Z"
    },
    "papermill": {
     "duration": 0.02626,
     "end_time": "2021-01-29T20:17:53.043022",
     "exception": false,
     "start_time": "2021-01-29T20:17:53.016762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"__label__2 The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n\",\n",
       " '__label__2 Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you\\'ve played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time\\'s Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer\\'s work (I haven\\'t heard the Xenogears soundtrack, so I can\\'t say for sure), and even if you\\'ve never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n',\n",
       " \"__label__2 Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\\n\",\n",
       " \"__label__2 Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\\n\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018652,
     "end_time": "2021-01-29T20:17:53.080427",
     "exception": false,
     "start_time": "2021-01-29T20:17:53.061775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll have to extract out the labels from the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:17:53.140695Z",
     "iopub.status.busy": "2021-01-29T20:17:53.130411Z",
     "iopub.status.idle": "2021-01-29T20:18:00.872005Z",
     "shell.execute_reply": "2021-01-29T20:18:00.871282Z"
    },
    "papermill": {
     "duration": 7.772936,
     "end_time": "2021-01-29T20:18:00.872191",
     "exception": false,
     "start_time": "2021-01-29T20:17:53.099255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting labels from sentences\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020924,
     "end_time": "2021-01-29T20:18:00.915323",
     "exception": false,
     "start_time": "2021-01-29T20:18:00.894399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:18:01.016577Z",
     "iopub.status.busy": "2021-01-29T20:18:00.980728Z",
     "iopub.status.idle": "2021-01-29T20:18:14.430779Z",
     "shell.execute_reply": "2021-01-29T20:18:14.430311Z"
    },
    "papermill": {
     "duration": 13.494382,
     "end_time": "2021-01-29T20:18:14.430913",
     "exception": false,
     "start_time": "2021-01-29T20:18:00.936531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018862,
     "end_time": "2021-01-29T20:18:14.469069",
     "exception": false,
     "start_time": "2021-01-29T20:18:14.450207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Modify URLs to ``<url>``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:18:14.540041Z",
     "iopub.status.busy": "2021-01-29T20:18:14.524356Z",
     "iopub.status.idle": "2021-01-29T20:18:17.864336Z",
     "shell.execute_reply": "2021-01-29T20:18:17.863835Z"
    },
    "papermill": {
     "duration": 3.376567,
     "end_time": "2021-01-29T20:18:17.864462",
     "exception": false,
     "start_time": "2021-01-29T20:18:14.487895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018893,
     "end_time": "2021-01-29T20:18:17.902741",
     "exception": false,
     "start_time": "2021-01-29T20:18:17.883848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tokenization of the sentenses with NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:18:17.946518Z",
     "iopub.status.busy": "2021-01-29T20:18:17.946027Z",
     "iopub.status.idle": "2021-01-29T20:40:46.801279Z",
     "shell.execute_reply": "2021-01-29T20:40:46.801761Z"
    },
    "papermill": {
     "duration": 1348.880173,
     "end_time": "2021-01-29T20:40:46.801919",
     "exception": false,
     "start_time": "2021-01-29T20:18:17.921746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080000/1080000 [11:05<00:00, 1623.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "for i, sentence in enumerate(tqdm(train_sentences)):\n",
    "    # The sentences will be stored as a list of words/tokens\n",
    "    train_sentences[i] = []\n",
    "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
    "        words.update([word.lower()])  # Converting all the words to lowercase\n",
    "        train_sentences[i].append(word)\n",
    "    # if i%20000 == 0:\n",
    "    #     print(str((i*100)/num_train) + \"% done\")\n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040458,
     "end_time": "2021-01-29T20:40:46.875153",
     "exception": false,
     "start_time": "2021-01-29T20:40:46.834695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "to remove typos and words that likely don't exist, we'll remove all words from the vocab that only appear once throughout\n",
    "\n",
    "To account for unknown words and padding, we'll have to add them to our vocabulary as well. Each word in the vocabulary will then be assigned an integer index and after that mapped to this integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:40:46.974226Z",
     "iopub.status.busy": "2021-01-29T20:40:46.964177Z",
     "iopub.status.idle": "2021-01-29T20:40:47.448503Z",
     "shell.execute_reply": "2021-01-29T20:40:47.447996Z"
    },
    "papermill": {
     "duration": 0.540316,
     "end_time": "2021-01-29T20:40:47.448657",
     "exception": false,
     "start_time": "2021-01-29T20:40:46.908341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing the words that only appear once\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "# Sorting the words according to the number of appearances, with the most common word being first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "words = ['_PAD','_UNK'] + words\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032922,
     "end_time": "2021-01-29T20:40:47.514874",
     "exception": false,
     "start_time": "2021-01-29T20:40:47.481952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "convert the words in the sentences to their corresponding indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:40:47.639438Z",
     "iopub.status.busy": "2021-01-29T20:40:47.613755Z",
     "iopub.status.idle": "2021-01-29T20:48:20.301648Z",
     "shell.execute_reply": "2021-01-29T20:48:20.300530Z"
    },
    "papermill": {
     "duration": 452.754121,
     "end_time": "2021-01-29T20:48:20.301810",
     "exception": false,
     "start_time": "2021-01-29T20:40:47.547689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(train_sentences):\n",
    "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    # For test sentences, we have to tokenize the sentences as well\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032688,
     "end_time": "2021-01-29T20:48:20.367994",
     "exception": false,
     "start_time": "2021-01-29T20:48:20.335306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "padding the sentences with 0s and shortening the lengthy sentences so that the data can be trained in batches to speed things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:48:20.441612Z",
     "iopub.status.busy": "2021-01-29T20:48:20.440697Z",
     "iopub.status.idle": "2021-01-29T20:48:53.011966Z",
     "shell.execute_reply": "2021-01-29T20:48:53.011444Z"
    },
    "papermill": {
     "duration": 32.611528,
     "end_time": "2021-01-29T20:48:53.012119",
     "exception": false,
     "start_time": "2021-01-29T20:48:20.400591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "test_sentences = pad_input(test_sentences, seq_len)\n",
    "\n",
    "# Converting our labels into numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032627,
     "end_time": "2021-01-29T20:48:53.078072",
     "exception": false,
     "start_time": "2021-01-29T20:48:53.045445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Split into train/Val/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:48:53.148170Z",
     "iopub.status.busy": "2021-01-29T20:48:53.147456Z",
     "iopub.status.idle": "2021-01-29T20:48:53.150324Z",
     "shell.execute_reply": "2021-01-29T20:48:53.149923Z"
    },
    "papermill": {
     "duration": 0.039631,
     "end_time": "2021-01-29T20:48:53.150428",
     "exception": false,
     "start_time": "2021-01-29T20:48:53.110797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_frac = 0.5 # 50% validation, 50% test\n",
    "split_id = int(split_frac * len(test_sentences))\n",
    "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032691,
     "end_time": "2021-01-29T20:48:53.215817",
     "exception": false,
     "start_time": "2021-01-29T20:48:53.183126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create model with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:48:53.287590Z",
     "iopub.status.busy": "2021-01-29T20:48:53.287090Z",
     "iopub.status.idle": "2021-01-29T20:48:54.808082Z",
     "shell.execute_reply": "2021-01-29T20:48:54.807005Z"
    },
    "papermill": {
     "duration": 1.559379,
     "end_time": "2021-01-29T20:48:54.808219",
     "exception": false,
     "start_time": "2021-01-29T20:48:53.248840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 400\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032917,
     "end_time": "2021-01-29T20:48:54.874709",
     "exception": false,
     "start_time": "2021-01-29T20:48:54.841792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:48:55.298284Z",
     "iopub.status.busy": "2021-01-29T20:48:55.297661Z",
     "iopub.status.idle": "2021-01-29T20:48:55.303196Z",
     "shell.execute_reply": "2021-01-29T20:48:55.302778Z"
    },
    "papermill": {
     "duration": 0.395174,
     "end_time": "2021-01-29T20:48:55.303328",
     "exception": false,
     "start_time": "2021-01-29T20:48:54.908154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU available')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Only CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:48:55.448172Z",
     "iopub.status.busy": "2021-01-29T20:48:55.447437Z",
     "iopub.status.idle": "2021-01-29T20:48:55.450318Z",
     "shell.execute_reply": "2021-01-29T20:48:55.449913Z"
    },
    "papermill": {
     "duration": 0.046088,
     "end_time": "2021-01-29T20:48:55.450428",
     "exception": false,
     "start_time": "2021-01-29T20:48:55.404340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:48:55.527728Z",
     "iopub.status.busy": "2021-01-29T20:48:55.527126Z",
     "iopub.status.idle": "2021-01-29T20:49:01.313206Z",
     "shell.execute_reply": "2021-01-29T20:49:01.312744Z"
    },
    "papermill": {
     "duration": 5.828373,
     "end_time": "2021-01-29T20:49:01.313344",
     "exception": false,
     "start_time": "2021-01-29T20:48:55.484971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:49:01.392083Z",
     "iopub.status.busy": "2021-01-29T20:49:01.391421Z",
     "iopub.status.idle": "2021-01-29T21:27:06.418273Z",
     "shell.execute_reply": "2021-01-29T21:27:06.417522Z"
    },
    "papermill": {
     "duration": 2285.07103,
     "end_time": "2021-01-29T21:27:06.418509",
     "exception": false,
     "start_time": "2021-01-29T20:49:01.347479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 500\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T21:27:06.535443Z",
     "iopub.status.busy": "2021-01-29T21:27:06.534766Z",
     "iopub.status.idle": "2021-01-29T21:27:56.730977Z",
     "shell.execute_reply": "2021-01-29T21:27:56.731389Z"
    },
    "papermill": {
     "duration": 50.243436,
     "end_time": "2021-01-29T21:27:56.731531",
     "exception": false,
     "start_time": "2021-01-29T21:27:06.488095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038299,
     "end_time": "2021-01-29T21:27:56.808213",
     "exception": false,
     "start_time": "2021-01-29T21:27:56.769914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T21:27:56.917804Z",
     "iopub.status.busy": "2021-01-29T21:27:56.917008Z",
     "iopub.status.idle": "2021-01-29T21:27:56.919309Z",
     "shell.execute_reply": "2021-01-29T21:27:56.919731Z"
    },
    "papermill": {
     "duration": 0.073144,
     "end_time": "2021-01-29T21:27:56.919870",
     "exception": false,
     "start_time": "2021-01-29T21:27:56.846726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        GRU_out, hidden = self.gru(embeds, hidden)\n",
    "        GRU_out = GRU_out.contiguous().view(-1,self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(GRU_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T21:27:57.002759Z",
     "iopub.status.busy": "2021-01-29T21:27:57.002068Z",
     "iopub.status.idle": "2021-01-29T21:27:58.295992Z",
     "shell.execute_reply": "2021-01-29T21:27:58.295501Z"
    },
    "papermill": {
     "duration": 1.337663,
     "end_time": "2021-01-29T21:27:58.296151",
     "exception": false,
     "start_time": "2021-01-29T21:27:56.958488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = GRUNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T21:27:58.385662Z",
     "iopub.status.busy": "2021-01-29T21:27:58.384738Z",
     "iopub.status.idle": "2021-01-29T21:59:17.963454Z",
     "shell.execute_reply": "2021-01-29T21:59:17.962620Z"
    },
    "papermill": {
     "duration": 1879.628369,
     "end_time": "2021-01-29T21:59:17.963619",
     "exception": false,
     "start_time": "2021-01-29T21:27:58.335250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 500\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = h.data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = val_h.data\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T21:59:18.065604Z",
     "iopub.status.busy": "2021-01-29T21:59:18.064781Z",
     "iopub.status.idle": "2021-01-29T21:59:59.607832Z",
     "shell.execute_reply": "2021-01-29T21:59:59.608487Z"
    },
    "papermill": {
     "duration": 41.600687,
     "end_time": "2021-01-29T21:59:59.608700",
     "exception": false,
     "start_time": "2021-01-29T21:59:18.008013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the best model GRU\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = h.data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6244.819011,
   "end_time": "2021-01-29T22:00:04.328777",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-29T20:15:59.509766",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
