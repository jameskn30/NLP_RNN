{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:16:04.695378Z",
     "iopub.status.busy": "2021-01-29T20:16:04.694727Z",
     "iopub.status.idle": "2021-01-29T20:16:06.687846Z",
     "shell.execute_reply": "2021-01-29T20:16:06.688420Z"
    },
    "papermill": {
     "duration": 2.018377,
     "end_time": "2021-01-29T20:16:06.688622",
     "exception": false,
     "start_time": "2021-01-29T20:16:04.670245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords =  {'here', 'i', 'd', 'been', \"wouldn't\", 'your', 'own', 'did', 'more', \"shan't\", 'hers', 'will', 'such', 'up', 'about', 'why', 'itself', 'aren', 'o', 'few', \"you'll\", 'only', 'doesn', \"shouldn't\", 'hasn', 'both', 'if', 've', 'under', 's', \"isn't\", 'him', 'once', 'haven', 'do', 'after', 'again', 'which', 'against', 'his', \"hasn't\", \"weren't\", \"you've\", 'as', 'had', 'before', \"wasn't\", \"that'll\", 'these', \"don't\", 'between', 'or', 'm', 'what', 'just', 'their', \"hadn't\", \"needn't\", 'until', \"didn't\", 'theirs', 'has', \"mustn't\", 'can', 'by', 'y', 'further', \"aren't\", 'her', 'it', 'very', 'does', 'is', 'those', 'nor', 'above', 'are', 'while', 'you', 'to', 'then', 'them', 'other', \"mightn't\", 'in', 'mustn', 'with', 'myself', 'the', 'down', 'now', \"haven't\", 'so', 'they', 'an', 'each', 're', 'have', 'he', 'through', 'our', 'all', 'themselves', 'my', 'off', 'whom', 'at', 'not', 'wasn', 'll', 'ma', 'we', 'weren', \"doesn't\", 'herself', 'too', 'most', 'where', 'couldn', 'shan', 'wouldn', 'but', 'out', \"it's\", \"you'd\", \"she's\", 'won', 'be', 'yourselves', 'were', 'from', 'no', 'don', 't', 'some', 'when', 'mightn', 'she', 'that', 'was', 'hadn', \"you're\", 'isn', 'am', 'doing', 'below', 'and', \"couldn't\", \"won't\", 'should', 'ourselves', 'on', 'how', 'who', 'yours', 'during', 'ain', 'a', 'of', 'its', 'than', 'being', 'didn', 'himself', 'because', 'over', 'any', 'for', 'me', 'yourself', 'there', 'needn', 'same', 'this', 'shouldn', 'into', 'ours', 'having', \"should've\"}\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import contractions \n",
    "\n",
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#The essentials\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "print('stopwords = ', STOPWORDS)\n",
    "\n",
    "root = os.getcwd()\n",
    "datasetpath = os.path.join(root, 'dataset', 'amazon_review_ft')\n",
    "assert os.path.exists(datasetpath), f'dataset path does not exist {datasetpath}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, clean, and process  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:16:06.729571Z",
     "iopub.status.busy": "2021-01-29T20:16:06.728918Z",
     "iopub.status.idle": "2021-01-29T20:16:06.733550Z",
     "shell.execute_reply": "2021-01-29T20:16:06.733129Z"
    },
    "papermill": {
     "duration": 0.026353,
     "end_time": "2021-01-29T20:16:06.733746",
     "exception": false,
     "start_time": "2021-01-29T20:16:06.707393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_txt = bz2.BZ2File(os.path.join(datasetpath, 'train.ft.txt.bz2'))\n",
    "# test_txt = bz2.BZ2File(os.path.join(datasetpath, 'test.ft.txt.bz2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:16:06.781965Z",
     "iopub.status.busy": "2021-01-29T20:16:06.781158Z",
     "iopub.status.idle": "2021-01-29T20:17:51.847135Z",
     "shell.execute_reply": "2021-01-29T20:17:51.846596Z"
    },
    "papermill": {
     "duration": 105.094045,
     "end_time": "2021-01-29T20:17:51.847277",
     "exception": false,
     "start_time": "2021-01-29T20:16:06.753232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines = train_txt.readlines()\n",
    "del train_txt\n",
    "gc.collect()\n",
    "# test_file = test_txteadlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n'\n",
      "b\"__label__2 The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n\"\n",
      "b'__label__2 Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you\\'ve played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time\\'s Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer\\'s work (I haven\\'t heard the Xenogears soundtrack, so I can\\'t say for sure), and even if you\\'ve never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n'\n",
      "b\"__label__2 Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\\n\"\n",
      "b\"__label__2 Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\\n\"\n",
      "b\"__label__2 an absolute masterpiece: I am quite sure any of you actually taking the time to read this have played the game at least once, and heard at least a few of the tracks here. And whether you were aware of it or not, Mitsuda's music contributed greatly to the mood of every single minute of the whole game.Composed of 3 CDs and quite a few songs (I haven't an exact count), all of which are heart-rendering and impressively remarkable, this soundtrack is one I assure you you will not forget. It has everything for every listener -- from fast-paced and energetic (Dancing the Tokage or Termina Home), to slower and more haunting (Dragon God), to purely beautifully composed (Time's Scar), to even some fantastic vocals (Radical Dreamers).This is one of the best videogame soundtracks out there, and surely Mitsuda's best ever. ^_^\\n\"\n",
      "b'__label__1 Buyer beware: This is a self-published book, and if you want to know why--read a few paragraphs! Those 5 star reviews must have been written by Ms. Haddon\\'s family and friends--or perhaps, by herself! I can\\'t imagine anyone reading the whole thing--I spent an evening with the book and a friend and we were in hysterics reading bits and pieces of it to one another. It is most definitely bad enough to be entered into some kind of a \"worst book\" contest. I can\\'t believe Amazon even sells this kind of thing. Maybe I can offer them my 8th grade term paper on \"To Kill a Mockingbird\"--a book I am quite sure Ms. Haddon never heard of. Anyway, unless you are in a mood to send a book to someone as a joke---stay far, far away from this one!\\n'\n",
      "b'__label__2 Glorious story: I loved Whisper of the wicked saints. The story was amazing and I was pleasantly surprised at the changes in the book. I am not normaly someone who is into romance novels, but the world was raving about this book and so I bought it. I loved it !! This is a brilliant story because it is so true. This book was so wonderful that I have told all of my friends to read it. It is not a typical romance, it is so much more. Not reading this book is a crime, becuase you are missing out on a heart warming story.\\n'\n",
      "b\"__label__2 A FIVE STAR BOOK: I just finished reading Whisper of the Wicked saints. I fell in love with the caracters. I expected an average romance read, but instead I found one of my favorite books of all time. Just when I thought I could predict the outcome I was shocked ! The writting was so descriptive that my heart broke when Julia's did and I felt as if I was there with them instead of just a distant reader. If you are a lover of romance novels then this is a must read. Don't let the cover fool you this book is spectacular!\\n\"\n",
      "b'__label__2 Whispers of the Wicked Saints: This was a easy to read book that made me want to keep reading on and on, not easy to put down.It left me wanting to read the follow on, which I hope is coming soon. I used to read a lot but have gotten away from it. This book made me want to read again. Very enjoyable.\\n'\n"
     ]
    }
   ],
   "source": [
    "for line in train_lines[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018106,
     "end_time": "2021-01-29T20:17:51.884530",
     "exception": false,
     "start_time": "2021-01-29T20:17:51.866424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Convert bytes object into utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:17:51.925808Z",
     "iopub.status.busy": "2021-01-29T20:17:51.925134Z",
     "iopub.status.idle": "2021-01-29T20:17:51.928165Z",
     "shell.execute_reply": "2021-01-29T20:17:51.927684Z"
    },
    "papermill": {
     "duration": 0.025387,
     "end_time": "2021-01-29T20:17:51.928293",
     "exception": false,
     "start_time": "2021-01-29T20:17:51.902906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pct of the data used for the models\n",
    "# pct = 0.3\n",
    "# num_train = int(len(train_file)*pct) #max 3600000\n",
    "# num_test = int(len(test_file)*pct) #max 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:17:52.015832Z",
     "iopub.status.busy": "2021-01-29T20:17:52.010788Z",
     "iopub.status.idle": "2021-01-29T20:17:52.997846Z",
     "shell.execute_reply": "2021-01-29T20:17:52.997353Z"
    },
    "papermill": {
     "duration": 1.051638,
     "end_time": "2021-01-29T20:17:52.997973",
     "exception": false,
     "start_time": "2021-01-29T20:17:51.946335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit =  360000\n",
      "train len =  288000\n",
      "val len =  72000\n"
     ]
    }
   ],
   "source": [
    "#only tkae 10% of this dataset\n",
    "LIMIT = int(0.1 * len(train_lines))\n",
    "print('limit = ', LIMIT)\n",
    "\n",
    "truncated_train_lines = train_lines[:LIMIT]\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "train_len = int(len(truncated_train_lines) * TRAIN_RATIO)\n",
    "\n",
    "train_array = [x.decode('utf-8') for x in truncated_train_lines[:train_len]]\n",
    "val_array = [x.decode('utf-8') for x in truncated_train_lines[train_len:]]\n",
    "\n",
    "#apply limit to a big dataset\n",
    "train_array = train_array[:LIMIT]\n",
    "\n",
    "print('train len = ', len(train_array))\n",
    "print('val len = ', len(val_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018652,
     "end_time": "2021-01-29T20:17:53.080427",
     "exception": false,
     "start_time": "2021-01-29T20:17:53.061775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We'll have to extract out the labels from the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "\n",
      "438\n",
      "__label__2\n",
      "['Stuning', 'even', 'for', 'the', 'non-gamer:', 'This', 'sound', 'track', 'was', 'beautiful!', 'It', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'I', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid.', 'game', 'music!', 'I', 'have', 'played', 'the', 'game', 'Chrono', 'Cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'I', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music!', 'It', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras.', 'It', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen!', '^_^']\n"
     ]
    }
   ],
   "source": [
    "print(train_array[0])\n",
    "print(len(train_array[0]))\n",
    "print(train_array[0].split()[0])\n",
    "print(train_array[0].split()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-29T20:17:53.140695Z",
     "iopub.status.busy": "2021-01-29T20:17:53.130411Z",
     "iopub.status.idle": "2021-01-29T20:18:00.872005Z",
     "shell.execute_reply": "2021-01-29T20:18:00.871282Z"
    },
    "papermill": {
     "duration": 7.772936,
     "end_time": "2021-01-29T20:18:00.872191",
     "exception": false,
     "start_time": "2021-01-29T20:17:53.099255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/288000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stuning', 'even', 'non', 'gamer', 'sound', 'track', 'beautiful', 'paints', 'senery', 'mind', 'well', 'would', 'recomend', 'even', 'people', 'hate', 'vid', 'game', 'music', 'played', 'game', 'chrono', 'cross', 'games', 'ever', 'played', 'best', 'music', 'backs', 'away', 'crude', 'keyboarding', 'takes', 'fresher', 'step', 'grate', 'guitars', 'soulful', 'orchestras', 'would', 'impress', 'anyone', 'cares', 'listen', '_']]\n",
      "X train len =  1\n",
      "[1]\n",
      "y train len =  1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    #if there's link in text, like www.something.com, https://www.something.com, \n",
    "    # replace it with the <url> token\n",
    "    if 'www.' in text or 'http:' in text or 'https:' in text or '.com' in text:\n",
    "        text = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", text)\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(\"\\d\", '', text)\n",
    "\n",
    "def remove_punctation(text):\n",
    "    return re.sub(r'[^\\w\\s]',' ',text)\n",
    "\n",
    "def expand_contraction(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split(' ') if word not in STOPWORDS])\n",
    "\n",
    "def process_line(line):\n",
    "    '''\n",
    "    extract feature and label from line and process the text\n",
    "    @params: \n",
    "        line: string, format: __label__2 some text.\n",
    "    @return:\n",
    "        feature: string\n",
    "        label: int, 0: bad review, 1 good review\n",
    "    '''\n",
    "    #Each line has format: __label__2 some text. \n",
    "    #The first part is label, the rest is text feature\n",
    "    parts = line.split()\n",
    "    label = parts[0]\n",
    "    feature = ' '.join(parts[1:])\n",
    "    #lower case the features\n",
    "    feature = feature.lower()\n",
    "    #remove urls in text\n",
    "    feature = remove_urls(feature)\n",
    "    #expand contractions\n",
    "    feature = expand_contraction(feature) \n",
    "    #remove punctuations\n",
    "    feature = remove_punctation(feature)\n",
    "    #remove digits\n",
    "    feature = remove_digits(feature)\n",
    "    #remove stop words\n",
    "    feature = remove_stopwords(feature)\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(feature)\n",
    "\n",
    "    #__label__1 = 0: bad review, __label__2 = 1,good review  \n",
    "    return tokens, 0 if label == '__label__1' else 1 \n",
    "\n",
    "# Extracting labels from sentences\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for line in tqdm(train_array):\n",
    "    feature, label = process_line(line)\n",
    "    X_train.append(feature)\n",
    "    y_train.append(label)\n",
    "    break\n",
    "\n",
    "print(X_train[:10])\n",
    "print('X train len = ', len(X_train))\n",
    "print(y_train[:10])\n",
    "print('y train len = ', len(y_train))\n",
    "\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\lib\\npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "#save checkpoint np\n",
    "X_TRAIN_OUTPUT_PATH = os.path.join(root, 'output', 'X_train.npy')\n",
    "Y_TRAIN_OUTPUT_PATH = os.path.join(root, 'output', 'y_train.npy')\n",
    "with open(X_TRAIN_OUTPUT_PATH, 'wb') as file:\n",
    "    np.save(file, X_train)\n",
    "with open(Y_TRAIN_OUTPUT_PATH, 'wb') as file:\n",
    "    np.save(file, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize feature and build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load checkpoint np\n",
    "#from this point down is for training. Only loading this part will relieve the memory\n",
    "X_TRAIN_OUTPUT_PATH = os.path.join(root, 'output', 'X_train.npy')\n",
    "Y_TRAIN_OUTPUT_PATH = os.path.join(root, 'output', 'y_train.npy')\n",
    "\n",
    "with open(X_TRAIN_OUTPUT_PATH, 'rb') as file:\n",
    "    X_train = np.load(file, allow_pickle=True)\n",
    "with open(Y_TRAIN_OUTPUT_PATH, 'rb') as file:\n",
    "    y_train = np.load(file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuning', 'even', 'for', 'the', 'non-gamer', ':', 'this', 'sound', 'track', 'was', 'beautiful', '!', 'it', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'i', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid', '.', 'game', 'music', '!', 'i', 'have', 'played', 'the', 'game', 'chrono', 'cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'i', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', '!', 'it', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras', '.', 'it', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', '!', '^_^']\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(type(X_train[0]))\n",
    "print(type(X_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288000/288000 [00:03<00:00, 83167.20it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = Counter() \n",
    "for i, tokens in enumerate(tqdm(X_train)):\n",
    "    #inplace update because of large dataset\n",
    "    # tokens = nltk.word_tokenize(feature)\n",
    "    corpus.update(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len corpus =  343114\n",
      "[('.', 1186413), ('the', 1174244), (',', 859565), ('i', 647376), ('and', 628593), ('a', 590073), ('to', 555912), ('it', 509192), ('of', 485206), ('this', 435750)]\n",
      "['.', 'the', ',', 'i', 'and', 'a', 'to', 'it', 'of', 'this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 343116/343116 [00:00<00:00, 1584499.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len =  343116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buiding vocab\n",
    "print('len corpus = ', len(corpus))\n",
    "print(corpus.most_common(10))\n",
    "\n",
    "tokens = list(map(lambda x: x[0], corpus.most_common()))\n",
    "print(tokens[:10])\n",
    "\n",
    "tokens = ['<pad>', '<unk>'] + tokens\n",
    "\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "\n",
    "for index, token in enumerate(tqdm(tokens)):\n",
    "    word2idx[token] = index\n",
    "    idx2word[index] = token\n",
    "\n",
    "print(\"vocab len = \", len(word2idx))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For debugging purposes\n",
    "with open('vocab.txt', 'w') as file:\n",
    "    json.dump(word2idx, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/288000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288000/288000 [00:14<00:00, 19432.27it/s]\n"
     ]
    }
   ],
   "source": [
    "#Converting tokens to idx\n",
    "# X_train_test = [None for _ in range(100)]\n",
    "\n",
    "for i, tokens in enumerate(tqdm(X_train)):\n",
    "    try:\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            #if token not found in vocab. put it in unk\n",
    "            # id = word2idx['<unk>']\n",
    "            id = 0\n",
    "            if token in word2idx:\n",
    "                id = word2idx[token]\n",
    "            ids.append(id)\n",
    "\n",
    "        #inplace because of large file\n",
    "        X_train[i] = torch.tensor(ids)\n",
    "    except Exception as e:\n",
    "        print(\"Error occured\", e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343116\n"
     ]
    }
   ],
   "source": [
    "print(len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape =  torch.Size([288000, 433])\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequence(X_train, batch_first = True, padding_value=word2idx['<pad>'])\n",
    "print(\"X train shape = \", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train shape =  torch.Size([288000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = torch.tensor(y_train)\n",
    "print(\"y train shape = \", y_train.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288000, 433])\n",
      "tensor([58642,    85,    17,     3, 84257,    13,    11,   196,   463,    18,\n",
      "          370,    15,     9,  5688,     3, 68210,    14,    76,   430,    36,\n",
      "           89,     5,    50,  1626,     9,    85,     8,   134,    79,   606,\n",
      "        17813,     2,   232,   128,    15,     5,    26,   562,     3,   232,\n",
      "        34462,  2135,    23,    57,    10,    35,    10,     3,   846,     5,\n",
      "           26,   124,   562,     9,    55,     3,    94,   128,    15,     9,\n",
      "         7176,   267,    48,  4120, 58643,     6,   433,     7, 16340,  1166,\n",
      "           24,  8639,  2957,     6,  4129, 17055,     2,     9,    50,  4814,\n",
      "          204,    79,  2329,     8,   323,    15, 17544,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_TO_IDX = os.path.join(root, 'output', 'word2idx.pkl')\n",
    "IDX_TO_WORD = os.path.join(root, 'output', 'idx2word.pkl')\n",
    "with open(WORD_TO_IDX, 'wb') as file:\n",
    "    pickle.dump(word2idx, file)\n",
    "with open(IDX_TO_WORD, 'wb') as file:\n",
    "    pickle.dump(idx2word, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save checkpoint after tokens np\n",
    "X_TOKENS_TRAIN_OUTPUT_PATH = os.path.join(root, 'output', 'X_tokens.npy')\n",
    "Y_TOKENS_TRAIN_OUTPUT_PATH = os.path.join(root, 'output', 'y_tokens.npy')\n",
    "with open(X_TOKENS_TRAIN_OUTPUT_PATH, 'wb') as file:\n",
    "    np.save(file, X_train)\n",
    "with open(Y_TOKENS_TRAIN_OUTPUT_PATH, 'wb') as file:\n",
    "    np.save(file, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6244.819011,
   "end_time": "2021-01-29T22:00:04.328777",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-29T20:15:59.509766",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
