{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "https://www.kaggle.com/code/frankmollard/a-story-about-unsupervised-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd()\n",
    "DATASET_PATH = os.path.join(ROOT, 'dataset')\n",
    "if os.path.exists(DATASET_PATH) == False: \n",
    "    os.makedirs(DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.Grayscale(),\n",
    "\n",
    "])\n",
    "mnist = torchvision.datasets.MNIST(root=DATASET_PATH, download = True, transform=compose)\n",
    "\n",
    "train_dataloader = DataLoader(mnist, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "sampleX, sampleY = next(iter(train_dataloader))\n",
    "print(sampleX.shape)\n",
    "print(sampleY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size = 32) -> None:\n",
    "        super().__init__()\n",
    "        assert len(input_size) == 3, 'input shape must be int format of (1,x,x)'\n",
    "        self.input_size = input_size\n",
    "        flatten_dim = input_size[0] * input_size[1] * input_size[2] \n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64,embedding_size),\n",
    "            nn.LeakyReLU(),\n",
    "        ) \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, flatten_dim),\n",
    "            nn.Sigmoid(),\n",
    "        ) \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim = 1)\n",
    "        X = self.encoder(X)\n",
    "        X = self.decoder(X)\n",
    "        w,h = self.input_size[1:]\n",
    "        X = X.reshape(-1, w,h)\n",
    "        return X\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, optimizer_fn, criterion_fn, lr = 0.001, epochs = 1000) -> None:\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        self.optimizer = optimizer_fn(self.model.parameters(), lr = lr)\n",
    "        self.criterion = criterion_fn()\n",
    "        self.history = defaultdict(list)\n",
    "\n",
    "    def optimize(self, input, target):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(input, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, train_dataloader):\n",
    "        self.model.train()\n",
    "        self.history.clear()\n",
    "\n",
    "        loop = tqdm(range(self.epochs))\n",
    "        val_loss = None\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for e in loop:\n",
    "\n",
    "            for X, y in train_dataloader:\n",
    "                X_pred = self.model(X)\n",
    "                \n",
    "                loss = self.criterion(X_pred, X)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.history['train loss'].append(loss.item())\n",
    "                running_loss += loss.item()\n",
    "        \n",
    "            loop.set_description(f'train loss = {running_loss:.3f}')\n",
    "        self.plot(self.history)\n",
    "        \n",
    "    # def evaluate(self, X_valid):\n",
    "    #     self.model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         X_valid_pred = self.model(X_valid)\n",
    "    #         val_loss = self.criterion(X_valid_pred, X_valid)\n",
    "    #     self.model.train()\n",
    "\n",
    "    #     return val_loss\n",
    "    \n",
    "    def plot(self, history):        \n",
    "        indices = list(range(len(history['train loss'])))\n",
    "        sns.lineplot(y = history['train loss'], x =indices, label = 'train')\n",
    "        if len(history['valid loss']) > 0:\n",
    "            sns.lineplot(y = history['valid loss'], x =indices, label = 'valid')\n",
    "        plt.title(\"Loss\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(sampleX[0].shape)\n",
    "print(len(sampleX[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784])\n",
      "torch.Size([32, 32])\n",
      "torch.Size([32, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "ae = AutoEncoder(sampleX[0].shape)\n",
    "sample_pred = ae(sampleX)\n",
    "print(sample_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32, 1, 28, 28])) that is different to the input size (torch.Size([32, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "train loss = 8299.500:   1%|          | 1/100 [00:27<45:29, 27.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m sampleX, sampleY \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataloader))\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoEncoder(sampleX[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m      5\u001b[0m     optimizer_fn\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[109], line 87\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_dataloader)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     86\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     90\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:392\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mfor\u001b[39;00m post_hook \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_post_hooks\u001b[39m.\u001b[39mvalues(), _global_optimizer_post_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    390\u001b[0m     post_hook(\u001b[39mself\u001b[39m, args, kwargs)\n\u001b[0;32m--> 392\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    621\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 622\u001b[0m         torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_exit\u001b[39m.\u001b[39m_RecordFunction(record)\n\u001b[1;32m    623\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_exit(record)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampleX, sampleY = next(iter(train_dataloader))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = AutoEncoder(sampleX[0].shape),\n",
    "    optimizer_fn= torch.optim.Adam,\n",
    "    criterion_fn= nn.MSELoss,\n",
    "    lr = 1e-4,\n",
    "    epochs = 100,\n",
    ")\n",
    "\n",
    "trainer.train(train_dataloader)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
