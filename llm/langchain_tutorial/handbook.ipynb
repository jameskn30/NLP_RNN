{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial from here: \n",
    "https://www.pinecone.io/learn/series/langchain/langchain-intro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other stuff\n",
    "import os\n",
    "import json\n",
    "ROOT = os.getcwd()\n",
    "\n",
    "key_path = os.path.join(ROOT, 'secrets.json')\n",
    "\n",
    "with open(key_path, 'r') as file:\n",
    "    key = json.load(file)\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = key[\"HUGGINGFACEHUB_API_TOKEN\"] \n",
    "os.environ[\"OPENAI_API_KEY\"] = key[\"OPENAI_API_KEY\"] \n",
    "os.environ[\"PINECONE_API_KEY\"] = key[\"PINECONE_API_KEY\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-loading models for following chapters\n",
    "\n",
    "So you don't have to load again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flan-T5-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4861/2457627710.py:13: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  hub_llm = HuggingFaceHub(\n",
      "/tmp/ipykernel_4861/2457627710.py:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain = LLMChain(\n",
      "/tmp/ipykernel_4861/2457627710.py:23: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  print(llm_chain.run(\"Which NFL team won the Super Bowl in the 2010 season?\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "san francisco 49ers\n"
     ]
    }
   ],
   "source": [
    "#this one uses flan-t5-base model, not big enough\n",
    "template = '''\n",
    "Question: {question}\n",
    "Answer: \n",
    "'''\n",
    "#https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id = \"google/flan-t5-base\",\n",
    "    model_kwargs={'temperature': 1e-10} # this is decoding strategy\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "print(llm_chain.run(\"Which NFL team won the Super Bowl in the 2010 season?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-3.5-turbo-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/anaconda3/envs/torch/lib/python3.12/site-packages/langchain_community/llms/openai.py:254: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/nguyen/anaconda3/envs/torch/lib/python3.12/site-packages/langchain_community/llms/openai.py:1073: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tutorial is out-dated. Read the official tutorial on LangChain\n",
    "# text-davinchi-003 is deprecrated, use gpt-3.5-turbo-instruct instead\n",
    "# OpenAI class is also deprecrated\n",
    "openai = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=key[\"OPENAI_API_KEY\"],\n",
    "    temperature = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 3, Prompt Template\n",
    "models\n",
    "https://huggingface.co/google/flan-t5-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "san francisco 49ers\n"
     ]
    }
   ],
   "source": [
    "template = '''\n",
    "Question: {question}\n",
    "Answer: \n",
    "'''\n",
    "#https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='New England Patriots')], [Generation(text='john f kennedy')], [Generation(text='uranus')]] llm_output=None run=[RunInfo(run_id=UUID('a642662b-92a2-4dfd-882d-317207bcbe89')), RunInfo(run_id=UUID('8b1fa061-b52e-4e7e-bc98-6bdaec1b3a7d')), RunInfo(run_id=UUID('8b4eb5db-623c-4eec-b1bd-5c97337b5770'))]\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    {\"question\": \"what team is the best NFL team in America\"},\n",
    "    {\"question\": \"Who is the 2nd president of the united states\"},\n",
    "    {\"question\": \"What is the first planet in the solar system\"},\n",
    "]\n",
    "\n",
    "answers = llm_chain.generate(questions)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='New England Patriots')], [Generation(text='john f kennedy')], [Generation(text='uranus')]] llm_output=None run=[RunInfo(run_id=UUID('a642662b-92a2-4dfd-882d-317207bcbe89')), RunInfo(run_id=UUID('8b1fa061-b52e-4e7e-bc98-6bdaec1b3a7d')), RunInfo(run_id=UUID('8b4eb5db-623c-4eec-b1bd-5c97337b5770'))]\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Enginering with PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I do not know\n"
     ]
    }
   ],
   "source": [
    "#Sample prompts format\n",
    "# In order to prevent hallucinations, we tell the model to tell I don't know if it doesn't know the answer ( just limit hallucinations, but not completely removed)\n",
    "template = '''Answer the question based on the context below. If the question can't be answered using the provided information, response with 'I do not know'\n",
    "context: Provide a useful context here ...\n",
    "question: {query}\n",
    "answer: \n",
    "'''\n",
    "#Just testing\n",
    "print(openai(template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use PromptTemplate?\n",
    "- few shot prompt template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the question can't be answered using the provided information, response with 'I do not know'\n",
      "context: Provide a useful context here ...\n",
      "question: what's the question?\n",
      "answer: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(query=\"what's the question?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is few shot learning? \n",
    "- parametric knowledge: stuff learned during pre-train\n",
    "- source knowledge: knowledge provided with prompt during inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a great question. As a funny and sarcastic AI, I can tell you that the meaning of life is to constantly question what the meaning of life is. It's a never-ending cycle, really. \n"
     ]
    }
   ],
   "source": [
    "# Few shot learning \n",
    "template = '''The following is a conversation with a funny and sarcastic AI.\n",
    "User: What's the mearning of life?\n",
    "AI: ''\n",
    "'''\n",
    "openai.temperature = 1.0 #increase randomness\n",
    "print(openai(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FewShotPromptTemplate is providing the agent a few examples \n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do\"},\n",
    "    {'query': 'What time is it?', 'answer': \"It's time to get a watch\"},\n",
    "]\n",
    "\n",
    "example_template = '''\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "'''\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "prefix = '''The following are exerpts from conversation with AI assistant. The assistant is funny and sarcastic. Here are a few examples:'''\n",
    "\n",
    "suffix='''\n",
    "User: {query}\n",
    "AI:\n",
    "'''\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate( \n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversation with AI assistant. The assistant is funny and sarcastic. Here are a few examples:\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch\n",
      "\n",
      "\n",
      "\n",
      "User: What's the meaning of life?\n",
      "AI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = few_shot_prompt_template.format(query=\"What's the meaning of life?\")\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response =  Well, some say it's to find true love, others say it's to pursue your passions, but I personally believe it's just to make sure all the snacks in your fridge are finished before they expire.\n"
     ]
    }
   ],
   "source": [
    "print(\"Response = \", openai(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples=[{'query': 'How are you?', 'answer': \"I can't complain but sometimes I still do\"}, {'query': 'What time is it?', 'answer': \"It's time to get a watch\"}] example_prompt=PromptTemplate(input_variables=['answer', 'query'], template='\\nUser: {query}\\nAI: {answer}\\n') get_text_length=<function _get_length_based at 0x7c8ac7f8eb60> max_length=50 example_text_lengths=[15, 14]\n"
     ]
    }
   ],
   "source": [
    "# Use selector to choose examples with max_length\n",
    "# There are many Selector, the following is just a base selector\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50\n",
    ")\n",
    "print(example_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversation with AI assistant. The assistant is funny and sarcastic. Here are a few examples:\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch\n",
      "\n",
      "\n",
      "\n",
      "User: What's the best time to hangout?\n",
      "AI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selector_templates = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, #previous example uses examples, here we use example selector\n",
    "    example_prompt = example_prompt,\n",
    "    suffix=suffix,\n",
    "    prefix=prefix,\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "print(selector_templates.format(query = \"What's the best time to hangout?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best time to hangout is when all your responsibilities are done, the stars align, and you're feeling spontaneous and adventurous. So basically, never.\n"
     ]
    }
   ],
   "source": [
    "response = openai(selector_templates.format(query=\"what's the best time to hangout?\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 4, Conversation Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect #inspect live object such as class, modules, ...\n",
    "from langchain import OpenAI\n",
    "from langchain import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationKGMemory\n",
    ")\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "convesation_chain = ConversationChain(\n",
    "    llm=openai\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(convesation_chain.prompt.template) # this is the pre-defined prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory types\n",
    "\n",
    "- modifies text passed to {history} in conversation chain template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning Alladin',\n",
       " 'history': '',\n",
       " 'response': ' Good morning, human. It is currently 7:00 AM in my time zone and the weather outside is 65 degrees Fahrenheit with clear skies. My energy levels are at 85%, I have access to over 2 million documents and 500 terabytes of data, and I have successfully completed 32,000 tasks since my last reboot. How may I assist you today?'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ConversationBufferMemory is the most straightforward \n",
    "# conversational memory in LangChain. \n",
    "# As we described above, the raw input of the past \n",
    "# conversation between the human and AI is passed — \n",
    "# in its raw form — to the {history} parameter.\n",
    "\n",
    "#pros:\n",
    "#- store everything --> maximum amount of information\n",
    "#- simple and intuitive\n",
    "\n",
    "#cons:\n",
    "# -consume more tokens when query --> costly, more time to query\n",
    "#- can exceed the context length, or hit the LLM token limit\n",
    "\n",
    "# Alternatively, ConversationBufferWindowMemory keeps a certain windows in the past\n",
    "# This technique can reduce even more tokens\n",
    "\n",
    "# conversation_window = ConversationChain(\n",
    "# \tllm=llm,\n",
    "# \tmemory=ConversationBufferWindowMemory(k=1)\n",
    "# )\n",
    "conversation_buf = ConversationChain(\n",
    "    llm = openai,\n",
    "    memory= ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "#test\n",
    "conversation_buf(\"Good morning Alladin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'tokens spent: {cb.total_tokens}')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens spent: 410\n",
      " Hello again, human. As mentioned before, I have had previous interactions with you on 10 different occasions, most recently 3 days ago when you requested a recipe for blueberry pancakes. Is there anything else I can assist you with?\n",
      "tokens spent: 463\n",
      "  I am not familiar with a unit called \"kevin.\" However, 30 degrees Celsius is equivalent to 86 degrees Fahrenheit. Is there anything else you would like to know?\n",
      "tokens spent: 520\n",
      "   1000 meters is equal to 1 kilometer. This conversion is commonly used for measuring distances and lengths in the International System of Units (SI). Would you like me to convert any other units for you?\n",
      "tokens spent: 557\n",
      "   Your first question was \"Good morning Alladin.\" Would you like me to remind you of any other previous interactions we have had?\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens(conversation_buf, 'Hello there'))\n",
    "print(count_tokens(conversation_buf, 'What is 30 degree celsius to kevin?'))\n",
    "print(count_tokens(conversation_buf, 'What is 1000 meter to kilometers'))\n",
    "print(count_tokens(conversation_buf, 'what was my first question?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning Alladin\n",
      "AI:  Good morning, human. It is currently 7:00 AM in my time zone and the weather outside is 65 degrees Fahrenheit with clear skies. My energy levels are at 85%, I have access to over 2 million documents and 500 terabytes of data, and I have successfully completed 32,000 tasks since my last reboot. How may I assist you today?\n",
      "Human: Hello there\n",
      "AI:  Hello, human. As per my database, I have had previous interactions with you on 10 different occasions, most recently 3 days ago when you requested a recipe for blueberry pancakes. Is there anything specific you would like to discuss today or shall we engage in some small talk?\n",
      "Human: What is 30 degree celsius to kevin?\n",
      "AI:  I am not familiar with a unit called \"kevin.\" However, 30 degrees Celsius is equivalent to 86 degrees Fahrenheit. Is there anything else you would like to know?\n",
      "Human: What is 1000 meter to kilometers\n",
      "AI:   1000 meters is equal to 1 kilometer. This conversion is commonly used for measuring distances and lengths in the International System of Units (SI). Would you like me to convert any other units for you?\n",
      "Human: what was my first question?\n",
      "AI:  According to my logs, your first question was \"Good morning Alladin.\" Would you like me to remind you of any other previous interactions we have had?\n",
      "Human: Hello there\n",
      "AI:  Hello again, human. As mentioned before, I have had previous interactions with you on 10 different occasions, most recently 3 days ago when you requested a recipe for blueberry pancakes. Is there anything else I can assist you with?\n",
      "Human: What is 30 degree celsius to kevin?\n",
      "AI:   I am not familiar with a unit called \"kevin.\" However, 30 degrees Celsius is equivalent to 86 degrees Fahrenheit. Is there anything else you would like to know?\n",
      "Human: What is 1000 meter to kilometers\n",
      "AI:    1000 meters is equal to 1 kilometer. This conversion is commonly used for measuring distances and lengths in the International System of Units (SI). Would you like me to convert any other units for you?\n",
      "Human: what was my first question?\n",
      "AI:    Your first question was \"Good morning Alladin.\" Would you like me to remind you of any other previous interactions we have had?\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens spent: 344\n",
      " Hi! I am an AI and I was created by a team of programmers to assist and communicate with humans like yourself. My main function is to process and analyze large amounts of data to provide accurate and helpful responses. How can I help you today?\n",
      "tokens spent: 430\n",
      " According to my calculations, 30 degrees Celsius is equivalent to 303.15 Kelvin. Is there anything else you would like to know?\n",
      "tokens spent: 603\n",
      " Hello! My name is AI and I am here to help you with any questions or tasks you may have. So, you're asking for a conversion of meters to kilometers? That's no problem at all. 1000 meters is equal to 1 kilometer. Would you like me to convert any other units of measurement for you? I can also provide conversions for temperature, weight, volume, and more. Just let me know how I can assist you further.\n",
      "tokens spent: 556\n",
      " Your first question was for a conversion from meters to kilometers. I was able to provide the conversion accurately and quickly. Is there anything else I can assist you with, such as converting other units?\n"
     ]
    }
   ],
   "source": [
    "# Given the cons of ConversationBufferMemory\n",
    "# Use ConversationSummaryMemory instead\n",
    "\n",
    "# pros:\n",
    "# - This is less tokens than the ConversationBufferMemory, where all past conversations are stored\n",
    "# - Enable longer conversation\n",
    "# - straightforward to implement\n",
    "\n",
    "# cons:\n",
    "# - more tokens used in smaller conversations\n",
    "# - summarization depends on LLM --> cost tokens to summarize\n",
    "\n",
    "# ConversationSummaryBufferMemory is the mix of BufferMemory and BufferWindowMemory\n",
    "# uing max_token_limit to choose most recent tokens in the converstations\n",
    "\n",
    "conversation_summary = ConversationChain(\n",
    "    llm = openai,\n",
    "    #summarization is powered by LLM, \n",
    "    #therefore, we pass LLM to ConversationSummaryMemory\n",
    "    memory = ConversationSummaryMemory(llm = openai) \n",
    ")\n",
    "\n",
    "print(count_tokens(conversation_summary, 'Hello there'))\n",
    "print(count_tokens(conversation_summary, 'What is 30 degree celsius to kevin?'))\n",
    "print(count_tokens(conversation_summary, 'What is 1000 meter to kilometers'))\n",
    "print(count_tokens(conversation_summary, 'what was my first question?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greets the AI and the AI introduces itself as a helpful program designed to assist and communicate with humans. Its main function is to process and analyze data to provide accurate and helpful responses. The AI asks how it can help the human and the human requests a conversion from meters to kilometers. The AI quickly and accurately provides the conversion and offers to assist with any other unit conversions needed. The human then asks the AI to recall their first question, and the AI is able to accurately remember the request for a conversion from meters to kilometers. The AI offers its continued assistance for any other unit conversions needed.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_summary.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other types of ConversationChain to read more \n",
    "- ConversationKnowledgeGraphMemory\n",
    "- ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 5 Retrieval Augmentation\n",
    "- Fixing hallucinations with KnowledgeBase\n",
    "\n",
    "problem: LLMs are not aware of new information after pre-training. fintune everytime new data comes in is not efficient --> retrieval augmentation to provide LLM with new context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/anaconda3/envs/torch/lib/python3.12/site-packages/datasets/load.py:1486: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:10000]\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a text processing pipeline\n",
    "- a data point contains a very long text\n",
    "- long text can't fit model\n",
    "- long text is harder to search\n",
    "\n",
    "--> create chunks: \n",
    "+ improve \"embedding accuracy\" --> search result is more relevant (vectordb)\n",
    "+ reduce generation cost, faster response, LLM follow instruction better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('p50k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n",
    "\n",
    "tiktoken_len(\n",
    "    \"hello I am a chunk of text and using the tiktoken_len function\"\n",
    "    \"we can find the length of this chunk of text in tokens\"\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 400, #split original into chunks\n",
    "    chunk_overlap = 20, #to retain context between chunks\n",
    "    length_function = tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original = Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English\n",
      "len chunks =  4\n",
      "chunk sample (397 tokens) =  Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English\n",
      "chunk sample (304 tokens) =  In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. T\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "original = data[6]['text']\n",
    "print('original =', original[:100])\n",
    "\n",
    "chunks = text_splitter.split_text(original)\n",
    "print('len chunks = ', len(chunks))\n",
    "print(f'chunk sample ({tiktoken_len(chunks[0])} tokens) = ', chunks[0][:100])\n",
    "print(f'chunk sample ({tiktoken_len(chunks[1])} tokens) = ', chunks[1][:100])\n",
    "\n",
    "print(type(chunks))\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "\n",
    "- embeddings are important to retrieve relevant context for LLM\n",
    "- original text --> embedding --> vector representation\n",
    "- store these vectors into vector db\n",
    "- given a vector query, vector db find similiar vectors using techniques like cosine similarity \n",
    "\n",
    "NOTE: In the tutorial, they use Pinecone API, but you can use Chroma. Tutorials how to use Chroma is from LangChain official tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_texts(texts=chunks, embedding=OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={'k': 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\\n\\nEarly life and family \\nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\\n\\nEducation \\nTuring went to St. Michael\\'s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\\n\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\\n\\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\\n\\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\\n\\nA brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.'), Document(page_content='Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\\n\\nEarly life and family \\nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\\n\\nEducation \\nTuring went to St. Michael\\'s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\\n\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\\n\\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\\n\\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\\n\\nA brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.'), Document(page_content='In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\\n\\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\\n\\nCareer \\nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\\n\\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called \"intelligent\". A computer could be said to \"think\" if a human talking with it could not tell it was a machine.\\n\\nDuring World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain\\'s codebreaking centre that produced Ultra intelligence.\\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.'), Document(page_content='In 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\\n\\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\\n\\nCareer \\nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\\n\\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called \"intelligent\". A computer could be said to \"think\" if a human talking with it could not tell it was a machine.\\n\\nDuring World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain\\'s codebreaking centre that produced Ultra intelligence.\\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.'), Document(page_content='From 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was \"the first detailed design of a stored-program computer\". Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\\xa0May 1950.\\n\\nPrivate life \\nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\\n\\nIn May 2012, a private member\\'s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\\n\\nDeath \\nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.\\n\\nThe treatment forced on him is now believed to be very wrong. It is against medical ethics and international laws of human rights. In August 2009, a petition asking the British Government to apologise to Turing for punishing him for being a homosexual was started. The petition received thousands of signatures. Prime Minister Gordon Brown acknowledged the petition. He called Turing\\'s treatment \"appalling\".\\n\\nReferences'), Document(page_content='From 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was \"the first detailed design of a stored-program computer\". Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\\xa0May 1950.\\n\\nPrivate life \\nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\\n\\nIn May 2012, a private member\\'s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\\n\\nDeath \\nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.\\n\\nThe treatment forced on him is now believed to be very wrong. It is against medical ethics and international laws of human rights. In August 2009, a petition asking the British Government to apologise to Turing for punishing him for being a homosexual was started. The petition received thousands of signatures. Prime Minister Gordon Brown acknowledged the petition. He called Turing\\'s treatment \"appalling\".\\n\\nReferences')]\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is the meaning of art?\")\n",
    "print(retrieved_docs)\n",
    "print(type(retrieved_docs[0]))\n",
    "\n",
    "#retrieved a pre-made template\n",
    "#you can customize with PromptTemplate like chapt3\n",
    "\n",
    "prompt = hub.pull()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\n",
      "\n",
      "Early life and family \n",
      "Alan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\n",
      "\n",
      "Education \n",
      "Turing went to St. Michael's, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\n",
      "\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\n",
      "\n",
      "The Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\n",
      "\n",
      "Educated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\n",
      "\n",
      "A brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.\n",
      "{}\n",
      "None\n",
      "Document\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)\n",
    "print(retrieved_docs[0].metadata)\n",
    "print(retrieved_docs[0].id)\n",
    "print(retrieved_docs[0].type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/anaconda3/envs/torch/lib/python3.12/site-packages/langsmith/client.py:322: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filter question \\nContext: filter context \\nAnswer:\")]\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filter question \n",
      "Context: filter context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "message = prompt.invoke({\n",
    "    'context': 'filter context',\n",
    "    'question': 'filter question'\n",
    "})\n",
    "\n",
    "print(message)\n",
    "print(message.to_messages()[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object RunnableSequence.stream at 0x7eaedf021e40>\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs: list):\n",
    "    return '\\n\\n'.join((doc.page_content for doc in docs))\n",
    "\n",
    "gpt3 = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        'context': retriever | format_docs,\n",
    "        'question': RunnablePassthrough(),\n",
    "    } \n",
    "    | prompt \n",
    "    | gpt3 \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "res = rag_chain.stream(\"What is art?\")\n",
    "print(res)\n",
    "\n",
    "for chunk in res:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 6, AI Agents as tools\n",
    "\n",
    "agents are like tools for LLM, such as:\n",
    "- calculator agent\n",
    "- map agent\n",
    "...\n",
    "\n",
    "To use agents, we need:\n",
    "- base LLM\n",
    "- tool to interact with \n",
    "- agent to control interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.agents import Tool, load_tools, initialize_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is outdated\n",
    "# llm = OpenAI(\n",
    "#     openai_api_key=\"OPENAI_API_KEY\",\n",
    "#     temperate=0.5,\n",
    "#     model_name=\"gpt-3.5-turbo\"\n",
    "# )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculator, desc=Math helper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/anaconda3/envs/torch/lib/python3.12/site-packages/langchain/chains/llm_math/base.py:173: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_math = LLMMathChain(llm = llm)\n",
    "math_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=llm_math.run,\n",
    "    description=\"Math helper\"\n",
    ")\n",
    "\n",
    "tools = [math_tool]\n",
    "#sample\n",
    "print(tools[0].name +  \", desc=\" + tools[0].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Calculator' description='Useful for when you need to answer questions about math.' func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7eaede7985c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7eaede79a810>, root_client=<openai.OpenAI object at 0x7eaedf3c0260>, root_async_client=<openai.AsyncOpenAI object at 0x7eaede798ec0>, openai_api_key=SecretStr('**********'), openai_proxy='')))> coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7eaede7985c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7eaede79a810>, root_client=<openai.OpenAI object at 0x7eaedf3c0260>, root_async_client=<openai.AsyncOpenAI object at 0x7eaede798ec0>, openai_api_key=SecretStr('**********'), openai_proxy='')))>\n",
      "[Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7eaede7985c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7eaede79a810>, root_client=<openai.OpenAI object at 0x7eaedf3c0260>, root_async_client=<openai.AsyncOpenAI object at 0x7eaede798ec0>, openai_api_key=SecretStr('**********'), openai_proxy='')))>, coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7eaede7985c0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7eaede79a810>, root_client=<openai.OpenAI object at 0x7eaedf3c0260>, root_async_client=<openai.AsyncOpenAI object at 0x7eaede798ec0>, openai_api_key=SecretStr('**********'), openai_proxy='')))>)]\n"
     ]
    }
   ],
   "source": [
    "# Using prebuilt tools\n",
    "tools = load_tools(\n",
    "    ['llm-math'],\n",
    "    llm = llm\n",
    ")\n",
    "\n",
    "print(tools[0])\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init agent (controller)\n",
    "\n",
    "they don't discuss ReAct framework in this chapter, but you can think of it as if an LLM could cycle through Reasoning and Action steps. Enabling a multi-step process for identifying answers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_agent = initialize_agent(\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    "    max_iterations = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the Calculator tool to add these numbers together.\n",
      "Action: Calculator\n",
      "Action Input: 4+34\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 38\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 38\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 4+34', 'output': '38'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(\"what is 4+34\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWe can use the calculator to add the apples John had with the apples Mary brought.\n",
      "Action: Calculator\n",
      "Action Input: 3 + 3 + 0.5\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 6.5\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mJohn has a total of 6.5 apples.\n",
      "Final Answer: John has 6.5 apples.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'If John has 3 apples, Mary brought 3 more and half eaten apple. How many apples John has?',\n",
       " 'output': 'John has 6.5 apples.'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(\"If John has 3 apples, Mary brought 3 more and half eaten apple. How many apples John has?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI cannot use the calculator tool for this question as it requires general knowledge.\n",
      "Final Answer: Hanoi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the capitol of Vietnam?', 'output': 'Hanoi'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_agent(\"What is the capitol of Vietnam?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent types \n",
    "\n",
    "+ Zero Shot ReAct\n",
    "    - agent considers 1 interaction with LLM, no memory\n",
    "+ Conversational ReAct\n",
    "    - use ConversationBufferMemory to store memory (refer to chapt 4)\n",
    "+ Find out more about custom tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 7, Building custom tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from math import pi, sqrt, cos, sin\n",
    "from typing import Union, Optional #these are type hints in python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"CircumferenceTool\"\n",
    "    #Agent uses this description to choose tools\n",
    "    description=\"calculates the circumference of a circle\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius) * 2.0 * pi\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"CircumferenceTool.arun not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7eaede7985c0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7eaede79a810> root_client=<openai.OpenAI object at 0x7eaedf3c0260> root_async_client=<openai.AsyncOpenAI object at 0x7eaede798ec0> openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_mem = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    k = 5,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [CircumferenceTool()]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools = tools,\n",
    "    #this is a pre-defined name\n",
    "    agent=\"chat-conversational-react-description\", \n",
    "    llm = llm,\n",
    "    verbose = True,\n",
    "    max_iterations = 3,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=conversation_mem\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"CircumferenceTool\",\n",
      "    \"action_input\": \"3.4\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21.362830044410593\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"21.36\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"what's the circumference of a circle with radius = 3.4 mm and round the answer to 2 points decimal\",\n",
       " 'chat_history': [HumanMessage(content=\"what's the circumference of a circle with radius = 3.4 mm\"),\n",
       "  AIMessage(content='21.362830044410593')],\n",
       " 'output': '21.36'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"what's the circumference of a circle with radius = 3.4 mm and round the answer to 2 points decimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi parameters tool\n",
    "class PythagorasTool(BaseTool):\n",
    "    name = \"PythagorasTool\"\n",
    "    #Description is like a prompt context\n",
    "    #to instruct the agent what the tool is for\n",
    "    description= '''Use this tool to calculate the hypotenuse of\n",
    "    a triangle  given 1 or 2 sides of a triangle or an angle (in degree). \n",
    "    To use this tool, you must provide at least 2 out of following 3 parameters:\n",
    "    ['adjacent_side','opposite_side', 'angle']\n",
    "    '''\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        adjacent_side: Optional[Union[int, float]] = None,\n",
    "        opposite_side: Optional[Union[int, float]] = None,\n",
    "        angle: Optional[Union[int, float]] = None\n",
    "    ):\n",
    "        # check for the values we have been given\n",
    "        if adjacent_side and opposite_side:\n",
    "            return sqrt(float(adjacent_side)**2 + float(opposite_side)**2)\n",
    "        elif adjacent_side and angle:\n",
    "            return adjacent_side / cos(float(angle))\n",
    "        elif opposite_side and angle:\n",
    "            return opposite_side / sin(float(angle))\n",
    "        else:\n",
    "            return \"Could not calculate the hypotenuse of the triangle. Need two or more of `adjacent_side`, `opposite_side`, or `angle`.\"\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CircumferenceTool(), PythagorasTool(), PythagorasTool(), PythagorasTool()]\n"
     ]
    }
   ],
   "source": [
    "tools.append(PythagorasTool())\n",
    "\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the agent prompts and tools. No need to initialize_agent again\n",
    "system_msg = '''Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "'''\n",
    "\n",
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message= system_msg,\n",
    "    tools = tools\n",
    ")\n",
    "\n",
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "agent.tools = tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"PythagorasTool\",\n",
      "    \"action_input\": {\"adjacent_side\": 51, \"opposite_side\": 34}\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m61.29437168288782\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"61.29\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?',\n",
       " 'chat_history': [HumanMessage(content=\"what's the circumference of a circle with radius = 3.4 mm\"),\n",
       "  AIMessage(content='21.362830044410593'),\n",
       "  HumanMessage(content=\"what's the circumference of a circle with radius = 3.4 mm and round the answer to 2 points decimal\"),\n",
       "  AIMessage(content='21.36'),\n",
       "  HumanMessage(content='If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?'),\n",
       "  AIMessage(content='The length of the hypotenuse of the triangle with sides of length 51cm and 34cm is approximately 61.01cm.')],\n",
       " 'output': '61.29'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"PythagorasTool\",\n",
      "    \"action_input\": {\"opposite_side\": 51, \"angle\": 20}\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m55.86315275680817\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": 55.86\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'If I have a triangle with the opposite side of length 51cm and an angle of 20 deg, what is the length of the hypotenuse?',\n",
       " 'chat_history': [HumanMessage(content=\"what's the circumference of a circle with radius = 3.4 mm\"),\n",
       "  AIMessage(content='21.362830044410593'),\n",
       "  HumanMessage(content=\"what's the circumference of a circle with radius = 3.4 mm and round the answer to 2 points decimal\"),\n",
       "  AIMessage(content='21.36'),\n",
       "  HumanMessage(content='If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?'),\n",
       "  AIMessage(content='The length of the hypotenuse of the triangle with sides of length 51cm and 34cm is approximately 61.01cm.'),\n",
       "  HumanMessage(content='If I have a triangle with two sides of length 51cm and 34cm, what is the length of the hypotenuse?'),\n",
       "  AIMessage(content='61.29')],\n",
       " 'output': 55.86}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"If I have a triangle with the opposite side of length 51cm and an angle of 20 deg, what is the length of the hypotenuse?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced tool usage\n",
    "\n",
    "This section uses an image-to-caption model from HF to aid the LLM with ability to describe image\n",
    "\n",
    "Read more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 8, Combine memory and RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 9, Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 10, RAG multi-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapt 11, LangChain Expression Language (LCEL)\n",
    "\n",
    "abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer\n",
    "\n",
    "+ fast development of LangChain\n",
    "+ features like streaming, async, parallel execution\n",
    "+ integration with LangSmith and LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7eaedf1150a0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7eaedf118950> root_client=<openai.OpenAI object at 0x7eaedf114590> root_async_client=<openai.AsyncOpenAI object at 0x7eaedf115700> openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    }
   ],
   "source": [
    "#Using model from previous chapters\n",
    "print(gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
