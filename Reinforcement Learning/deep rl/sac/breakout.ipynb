{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://spinningup.openai.com/en/latest/algorithms/sac.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    ClipRewardEnv, \n",
    "    EpisodicLifeEnv, #make end of life = end of episode\n",
    "    FireResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    NoopResetEnv\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': 'BreakoutNoFrameskip-v4'\n",
    "}\n",
    "NUM_ENVS = 3\n",
    "SEED = 1\n",
    "\n",
    "LR = 1e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output', ENV_ARGS['id'])\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(**env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = NoopResetEnv(env)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = ClipRewardEnv(env)\n",
    "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "    env = gym.wrappers.GrayScaleObservation(env)\n",
    "    env = gym.wrappers.FrameStack(env, 4)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num envs =  3\n"
     ]
    }
   ],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(**ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "print('num envs = ', NUM_ENVS)\n",
    "\n",
    "envs.close()\n",
    "\n",
    "#check to make sure this is continous action\n",
    "# assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer_init(layer: nn.Conv2d, bias = 0.0):\n",
    "    nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "def linear_layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, envs: gym.Env, fc_hidden = 512):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_shape = np.prod(envs.single_observation_space.shape)\n",
    "        self.action_shape = np.prod(envs.single_action_space.n)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.LazyConv2d(32, kernel_size=8, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv2d(64, kernel_size=4, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyConv2d(64, kernel_size=3, stride = 1),\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(fc_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(self.action_shape),\n",
    "        )\n",
    "\n",
    "        #initlaize lazy layers\n",
    "        obs, _ = envs.reset()\n",
    "        obs = torch.tensor(obs).float()\n",
    "\n",
    "        self.network(obs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x/255.0)\n",
    "\n",
    "\n",
    "class Actor(QNetwork):\n",
    "    def __init__(self, envs, fc_hidden = 512):\n",
    "        super().__init__(envs, fc_hidden)\n",
    "\n",
    "    def get_action(self, X):\n",
    "        logits = self(X/255.0)\n",
    "\n",
    "        prob_dist = Categorical(logits = logits)\n",
    "\n",
    "        action = prob_dist.sample()\n",
    "\n",
    "        action_probs = prob_dist.probs\n",
    "\n",
    "        log_prob = F.log_softmax(logits, dim = 1)\n",
    "\n",
    "        return action, log_prob, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "action shape =  torch.Size([3])\n",
      "log prob shape =  torch.Size([3, 4])\n",
      "action probs shape =  torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(**ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "qnet = QNetwork(envs)\n",
    "\n",
    "actor = Actor(envs)\n",
    "\n",
    "obs, _ = envs.reset()\n",
    "obs = torch.tensor(obs).float()\n",
    "\n",
    "q_value = qnet(obs)\n",
    "print(q_value.shape)\n",
    "actor = Actor(envs)\n",
    "action, log_prob, action_probs = actor.get_action(obs)\n",
    "print('action shape = ', action.shape)\n",
    "print('log prob shape = ', log_prob.shape)\n",
    "print('action probs shape = ', action_probs.shape)\n",
    "\n",
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "def replay_buffer_usage(rb: ReplayBuffer):\n",
    "    return (rb.pos / rb.buffer_size) * 100 if rb.full == False else 100\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "    \n",
    "def pickle_load(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        obj = pickle.dump(file)\n",
    "    return obj\n",
    "\n",
    "def evaluate(agent, episodes = 10):\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "\n",
    "    return total_rewards\n",
    "\n",
    "def soft_update(net:nn.Module, other: nn.Module, tau = 0.005):\n",
    "    for this_param, other_param in zip(net.parameters(), other.parameters()):\n",
    "        this_param.data.copy_(tau * other_param.data + (1 - tau) * this_param.data)\n",
    "    \n",
    "def weight_update(net:nn.Module, other: nn.Module, tau = 0.005):\n",
    "    net.load_state_dict(other.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(\n",
    "    envs: gym.Env, actor:Actor, qf1:QNetwork, qf2:QNetwork, \n",
    "    qf1_target:QNetwork, qf2_target:QNetwork,\n",
    "    q_optimizer, actor_optimizer, device,\n",
    "    buffer_size = int(1e6), batch_size = 32,\n",
    "    total_timesteps = 1000, warmup_steps = 50,\n",
    "    policy_update_freq = 2, target_net_update_freq = 1, gamma = 0.99, \n",
    "    plot_update_freq = 10, label = 'test', history = None):\n",
    "\n",
    "    SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "    FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "    HISTORY_PATH = os.path.join(SAVE_PATH, 'history.pickle')\n",
    "\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    print('save path = ', SAVE_PATH)\n",
    "\n",
    "    state_size = np.prod(envs.single_observation_space.shape)\n",
    "    action_size = np.prod(envs.single_action_space.shape)\n",
    "    n_envs = envs.observation_space.shape[0]\n",
    "\n",
    "    actor.to(device)\n",
    "    qf1.to(device)\n",
    "    qf2.to(device)\n",
    "    qf1_target.to(device)\n",
    "    qf2_target.to(device)\n",
    "\n",
    "    #default to float\n",
    "    envs.single_observation_space.dtype = np.float32\n",
    "\n",
    "    alpha = 0.2 # expected return and entropy trade-off coefficient \n",
    "\n",
    "    # target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()\n",
    "    # log_alpha = torch.zeros(1, requires_grad = True, device= device)\n",
    "    # alpha = log_alpha.exp().item()\n",
    "    # a_optimizer = torch.optim.Adam([log_alpha], lr = 1e-3)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(\n",
    "            buffer_size,\n",
    "            envs.single_observation_space, \n",
    "            envs.single_action_space, \n",
    "            device = device,\n",
    "            handle_timeout_termination=False,\n",
    "            optimize_memory_usage=True,\n",
    "            n_envs=n_envs\n",
    "    )\n",
    "\n",
    "    obs, _ = envs.reset()\n",
    "    avg_reward = 0\n",
    "    avg_reward = 0\n",
    "    best_score = -float('inf')\n",
    "    score_window = deque(maxlen = 100)\n",
    "    if history == None:\n",
    "        history = defaultdict(list)\n",
    "\n",
    "    loop = tqdm(range(total_timesteps))\n",
    "    episode_count = 0\n",
    "    updated_t = 0\n",
    "\n",
    "    #start training loop\n",
    "    for global_step in loop:\n",
    "        t = int(loop.format_dict['elapsed'])\n",
    "\n",
    "        #if still warming up, get random action\n",
    "        if global_step < warmup_steps:\n",
    "            actions = envs.action_space.sample()\n",
    "        \n",
    "        #else done warmup, get actions from actor\n",
    "        else:\n",
    "            actions, _,_ = actor.get_action(torch.tensor(obs).to(device).float())\n",
    "            actions = actions.detach().cpu().numpy()\n",
    "        \n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "        if 'final_info' in infos:\n",
    "            for info in infos['final_info']:\n",
    "                if info and 'episode' in info:\n",
    "                    ep_return = info['episode']['r']\n",
    "                    score_window.append(ep_return)\n",
    "                    episode_count += 1\n",
    "\n",
    "                    avg_reward = np.mean(score_window)\n",
    "                    history['reward'].append(avg_reward)\n",
    "                    history['buffer_usage'].append(replay_buffer_usage(replay_buffer))\n",
    "\n",
    "                    #save model with new best score \n",
    "                    if avg_reward > best_score:\n",
    "                        best_score = avg_reward\n",
    "                        torch.save(actor, os.path.join(SAVE_PATH, 'actor.checkpoint.torch'))\n",
    "                        torch.save(qf1, os.path.join(SAVE_PATH, 'qf1.checkpoint.torch'))\n",
    "                        torch.save(qf2, os.path.join(SAVE_PATH, 'qf2.checkpoint.torch'))\n",
    "                        torch.save(qf1_target, os.path.join(SAVE_PATH, 'qf1_target.checkpoint.torch'))\n",
    "                        torch.save(qf2_target, os.path.join(SAVE_PATH, 'qf2_target.checkpoint.torch'))\n",
    "\n",
    "        real_next_obs = next_obs.copy()\n",
    "        for i, truncated in enumerate(truncations):\n",
    "            if truncated:\n",
    "                real_next_obs[i] = infos['final_observation'][i]\n",
    "        \n",
    "        replay_buffer.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        #optimize when done warming up \n",
    "        if global_step > warmup_steps:\n",
    "\n",
    "            data = replay_buffer.sample(batch_size)\n",
    "            b_next_obs = data.next_observations\n",
    "            b_obs = data.observations\n",
    "            b_actions = data.actions\n",
    "            b_rewards = data.rewards\n",
    "            b_dones = data.dones\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, next_state_log_probs, next_state_action_probs = actor.get_action(b_next_obs)\n",
    "                qf1_next_target = qf1_target(b_next_obs)\n",
    "                qf2_next_target = qf2_target(b_next_obs)\n",
    "                #convert to Q-target discrete\n",
    "                min_qf_next_target = next_state_action_probs * (torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_probs)\n",
    "                min_qf_next_target = min_qf_next_target.sum(dim = 1)\n",
    "\n",
    "                next_q_value = b_rewards.flatten() + (1 - b_dones.flatten()) * gamma * min_qf_next_target\n",
    "            \n",
    "            qf1_values = qf1(b_obs)\n",
    "            qf2_values = qf2(b_obs)\n",
    "            \n",
    "            qf1_a_values = qf1_values.gather(1, b_actions.long()).view(-1)\n",
    "            qf2_a_values = qf2_values.gather(1, b_actions.long()).view(-1)\n",
    "\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "\n",
    "            qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "            #step q_optimizer\n",
    "            q_optimizer.zero_grad()\n",
    "            qf_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            #optimize actor\n",
    "            _, log_pi, action_probs = actor.get_action(b_obs)\n",
    "            with torch.no_grad():\n",
    "                qf1_values = qf1(b_obs)\n",
    "                qf2_values = qf2(b_obs)\n",
    "\n",
    "                min_qf_pi = torch.min(qf1_values, qf2_values)\n",
    "\n",
    "            actor_loss = (action_probs * ((alpha * log_pi) - min_qf_pi)).mean()\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            \n",
    "            if global_step % target_net_update_freq == 0:\n",
    "                soft_update(qf1_target, qf1)\n",
    "                soft_update(qf2_target, qf2)\n",
    "        \n",
    "        if t != updated_t and t % plot_update_freq == 0: \n",
    "            updated_t = t\n",
    "            loop.set_description(f\"avg_reward = {avg_reward:.2f}, best_score = {best_score:.2f}, episode_count = {episode_count}, buffer usage = {replay_buffer_usage(replay_buffer):.2f}\")\n",
    "            plot(history, save_path = FIG_SAVE_PATH)\n",
    "            pickle_dump(history, HISTORY_PATH)\n",
    "\n",
    "    plot(history, show = True, save_path = FIG_SAVE_PATH)\n",
    "    envs.reset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path =  e:\\ML\\NLP\\Reinforcement Learning\\deep rl\\sac\\output\\BreakoutNoFrameskip-v4\\baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg_reward = 0.97, best_score = 1.40, episode_count = 31, buffer usage = 0.96:   0%|          | 1720/1000000 [00:47<15:40:29, 17.69it/s]"
     ]
    }
   ],
   "source": [
    "device= DEVICE\n",
    "q_lr = 1e-3\n",
    "actor_lr = 1e-3\n",
    "total_timesteps = int(1e6)\n",
    "warmup_steps = int(1e3)\n",
    "buffer_size = int(5e5)\n",
    "batch_size = 256\n",
    "num_envs = 6\n",
    "\n",
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(**ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "actor = Actor(envs).to(device)\n",
    "qf1 = QNetwork(envs).to(device)\n",
    "qf2 = QNetwork(envs).to(device)\n",
    "qf1_target = QNetwork(envs).to(device)\n",
    "qf2_target = QNetwork(envs).to(device)\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "\n",
    "q_optimizer = torch.optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr = q_lr) \n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_lr) \n",
    "#NOTE: use fixec entropy method mentioned in Spin-up, improve this later with enforce method\n",
    "# https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "\n",
    "\n",
    "tune(envs, actor, qf1, qf2, qf1_target, qf2_target, \n",
    "q_optimizer, actor_optimizer, device = device,\n",
    "total_timesteps= total_timesteps, warmup_steps=warmup_steps,\n",
    "buffer_size=buffer_size, batch_size=batch_size, label = 'baseline'\n",
    ")\n",
    "\n",
    "envs.close()\n",
    "\n",
    "\n",
    "# #check to make sure this is continous action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
