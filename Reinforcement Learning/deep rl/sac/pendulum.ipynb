{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://spinningup.openai.com/en/latest/algorithms/sac.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': \"InvertedPendulum-v4\"\n",
    "}\n",
    "NUM_ENVS = 3\n",
    "SEED = 1\n",
    "\n",
    "LR = 1e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output', ENV_ARGS['id'])\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gamma, **env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    # env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    # env = gym.wrappers.ClipAction(env)\n",
    "    # env = gym.wrappers.NormalizeObservation(env)\n",
    "    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    # env = gym.wrappers.NormalizeReward(env, gamma = gamma)\n",
    "    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n",
      "tensor([-3.])\n",
      "tensor([6.])\n",
      "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999, 0.3971, 0.7544],\n",
      "        [0.5695, 0.4388, 0.6387, 0.5247, 0.6826, 0.3051, 0.4635, 0.4550],\n",
      "        [0.5725, 0.4980, 0.9371, 0.6556, 0.3138, 0.1980, 0.4162, 0.2843]])\n",
      "tensor([[-2.2729, -0.8379, -1.2092, -2.2041, -0.0878, -2.3996, -1.1914, -2.2631],\n",
      "        [-1.7085, -1.3163, -1.9160, -1.5740, -2.0478, -0.9154, -1.3906, -1.3650],\n",
      "        [-1.7174, -1.4940, -2.8113, -1.9668, -0.9414, -0.5941, -1.2486, -0.8530]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(envs.single_action_space.high)\n",
    "b = torch.tensor(envs.single_action_space.low)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "c = a - b\n",
    "print(c)\n",
    "\n",
    "d = torch.rand(3,8)\n",
    "print(d)\n",
    "\n",
    "print(d*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,envs: gym.Env, hidden_size = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_shape = np.prod(envs.single_observation_space.shape)\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape + self.action_shape, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(hidden_size, 1)),\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim = 1)\n",
    "        return self.network(x)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.Env, hidden_size = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_shape = np.prod(envs.single_observation_space.shape)\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc_mean = nn.Linear(hidden_size, self.action_shape)\n",
    "        self.fc_logstd = nn.Linear(hidden_size, self.action_shape)\n",
    "\n",
    "        #NOTE: register buffer so that optimizer will not update its values\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((envs.single_action_space.high - envs.single_action_space.low)/2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((envs.single_action_space.high + envs.single_action_space.low)/2.0, dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.network(X)\n",
    "        mean = self.fc_mean(X)\n",
    "        logstd = torch.tanh(self.fc_logstd(X))\n",
    "        # https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "        logstd = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (logstd + 1)\n",
    "\n",
    "        return mean, logstd\n",
    "    \n",
    "    def get_action(self, X):\n",
    "\n",
    "        mean, logstd = self(X)\n",
    "        #exponential to convert from log(std) to std\n",
    "        std = logstd.exp()\n",
    "\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        #reparameterize trick \n",
    "        # https://stackoverflow.com/questions/60533150/what-is-the-difference-between-sample-and-rsample\n",
    "        x_t = normal.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "\n",
    "        #action scale and bias is registered as buffer in init()\n",
    "        # print('y t shape = ', y_t.shape)\n",
    "        # print('scale shape = ', self.action_scale.shape)\n",
    "        # print('bias shape = ', self.action_bias.shape)\n",
    "\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "\n",
    "        logprob = normal.log_prob(x_t)\n",
    "\n",
    "        logprob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        logprob = logprob.sum(1, keepdim = True)\n",
    "\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "\n",
    "        return action, logprob, mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape =  torch.Size([3, 4])\n",
      "action shape =  torch.Size([3, 1])\n",
      "q value =  torch.Size([3, 1])\n",
      "actor action =  torch.Size([3, 1])\n",
      "logprob  =  torch.Size([3, 1])\n",
      "mean  =  torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#Test agent\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "obs, info = envs.reset()\n",
    "\n",
    "actor = Actor(envs)\n",
    "qnet = QNetwork(envs)\n",
    "\n",
    "obs = torch.tensor(obs).float()\n",
    "action = torch.tensor(envs.action_space.sample()).float()\n",
    "print('state shape = ', obs.shape)\n",
    "print('action shape = ', action.shape)\n",
    "\n",
    "qvalue = qnet(obs, action)\n",
    "print('q value = ', qvalue.shape)\n",
    "\n",
    "actor_action, logprob, mean = actor.get_action(obs)\n",
    "print('actor action = ', actor_action.shape)\n",
    "print('logprob  = ', logprob.shape)\n",
    "print('mean  = ', mean.shape)\n",
    "\n",
    "\n",
    "# action, log_prob, entropy, value = test_agent.get_action_and_value(obs)\n",
    "\n",
    "# print('log prob shape = ', log_prob.shape)\n",
    "# print('entropy shape = ', entropy.shape)\n",
    "# print('value shape = ', value.shape)\n",
    "\n",
    "# del test_agent\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "def replay_buffer_usage(rb: ReplayBuffer):\n",
    "    return (rb.pos / rb.buffer_size) * 100 if rb.full == False else 100\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "    \n",
    "def pickle_load(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        obj = pickle.dump(file)\n",
    "    return obj\n",
    "\n",
    "def evaluate(agent, episodes = 10):\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "\n",
    "    return total_rewards\n",
    "\n",
    "def soft_update(net:nn.Module, other: nn.Module, tau = 0.005):\n",
    "    for this_param, other_param in zip(net.parameters(), other.parameters()):\n",
    "        this_param.data.copy_(tau * other_param.data + (1 - tau) * this_param.data)\n",
    "    \n",
    "def weight_update(net:nn.Module, other: nn.Module, tau = 0.005):\n",
    "    net.load_state_dict(other.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(\n",
    "    envs: gym.Env, actor:Actor, qf1:QNetwork, qf2:QNetwork, \n",
    "    qf1_target:QNetwork, qf2_target:QNetwork,\n",
    "    q_optimizer, actor_optimizer, device,\n",
    "    buffer_size = int(1e6), batch_size = 32,\n",
    "    total_timesteps = 1000, warmup_steps = 50,\n",
    "    policy_update_freq = 2, target_net_update_freq = 1, gamma = 0.99, \n",
    "    plot_update_freq = 10, label = 'test', plot_udpate_freq = 10, history = None):\n",
    "\n",
    "    SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "    FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "    HISTORY_PATH = os.path.join(SAVE_PATH, 'history.pickle')\n",
    "\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    print('save path = ', SAVE_PATH)\n",
    "\n",
    "    state_size = np.prod(envs.single_observation_space.shape)\n",
    "    action_size = np.prod(envs.single_action_space.shape)\n",
    "    n_envs = envs.observation_space.shape[0]\n",
    "\n",
    "    #default to float\n",
    "    envs.single_observation_space.dtype = np.float32\n",
    "\n",
    "    # alpha = 0.2 # expected return and entropy trade-off coefficient \n",
    "\n",
    "    target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()\n",
    "    log_alpha = torch.zeros(1, requires_grad = True, device= device)\n",
    "    alpha = log_alpha.exp().item()\n",
    "    a_optimizer = torch.optim.Adam([log_alpha], lr = 1e-3)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(\n",
    "            buffer_size,\n",
    "            envs.single_observation_space, \n",
    "            envs.single_action_space, \n",
    "            device = device,\n",
    "            handle_timeout_termination=False,\n",
    "            optimize_memory_usage=True,\n",
    "            n_envs=n_envs\n",
    "    )\n",
    "\n",
    "    obs, _ = envs.reset()\n",
    "    avg_reward = 0\n",
    "    avg_reward = 0\n",
    "    best_score = -float('inf')\n",
    "    score_window = deque(maxlen = 100)\n",
    "    if history == None:\n",
    "        history = defaultdict(list)\n",
    "\n",
    "    loop = tqdm(range(total_timesteps))\n",
    "    episode_count = 0\n",
    "    updated_t = 0\n",
    "\n",
    "    #start training loop\n",
    "    for global_step in loop:\n",
    "        t = int(loop.format_dict['elapsed'])\n",
    "\n",
    "        #if still warming up, get random action\n",
    "        if global_step < warmup_steps:\n",
    "            actions = envs.action_space.sample()\n",
    "        \n",
    "        #else done warmup, get actions from actor\n",
    "        else:\n",
    "            actions, _,_ = actor.get_action(torch.tensor(obs).to(device).float())\n",
    "            actions = actions.detach().cpu().numpy()\n",
    "        \n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "        if 'final_info' in infos:\n",
    "            for info in infos['final_info']:\n",
    "                if info and 'episode' in info:\n",
    "                    test = True\n",
    "                    ep_return = info['episode']['r']\n",
    "                    score_window.append(ep_return)\n",
    "                    episode_count += 1\n",
    "\n",
    "                    avg_reward = np.mean(score_window)\n",
    "                    history['reward'].append(avg_reward)\n",
    "                    history['buffer_usage'].append(replay_buffer_usage(replay_buffer))\n",
    "\n",
    "                    #save model with new best score \n",
    "                    if avg_reward > best_score:\n",
    "                        best_score = avg_reward\n",
    "                        torch.save(actor, os.path.join(SAVE_PATH, 'actor.checkpoint.torch'))\n",
    "                        torch.save(qf1, os.path.join(SAVE_PATH, 'qf1.checkpoint.torch'))\n",
    "                        torch.save(qf2, os.path.join(SAVE_PATH, 'qf2.checkpoint.torch'))\n",
    "                        torch.save(qf1_target, os.path.join(SAVE_PATH, 'qf1_target.checkpoint.torch'))\n",
    "                        torch.save(qf2_target, os.path.join(SAVE_PATH, 'qf2_target.checkpoint.torch'))\n",
    "\n",
    "        real_next_obs = next_obs.copy()\n",
    "        for i, truncated in enumerate(truncations):\n",
    "            if truncated:\n",
    "                real_next_obs[i] = infos['final_observation'][i]\n",
    "        \n",
    "        replay_buffer.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        #optimize when done warming up \n",
    "        if global_step > warmup_steps:\n",
    "\n",
    "            data = replay_buffer.sample(batch_size)\n",
    "            b_next_obs = data.next_observations.to()\n",
    "            b_obs = data.observations\n",
    "            b_actions = data.actions\n",
    "            b_rewards = data.rewards\n",
    "            b_dones = data.dones\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_state_actions, next_state_log_probs, _ = actor.get_action(b_next_obs)\n",
    "                qf1_next_target = qf1_target(b_next_obs, next_state_actions)\n",
    "                qf2_next_target = qf2_target(b_next_obs, next_state_actions)\n",
    "                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_probs\n",
    "                next_q_value = b_rewards.flatten() + (1 - b_dones.flatten()) * gamma * min_qf_next_target.view(-1)\n",
    "            \n",
    "            qf1_a_values = qf1(b_obs, b_actions).view(-1)\n",
    "            qf2_a_values = qf2(b_obs, b_actions).view(-1)\n",
    "\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "\n",
    "            qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "            #step q_optimizer\n",
    "            q_optimizer.zero_grad()\n",
    "            qf_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            #TD3 update delayed\n",
    "            if global_step % policy_update_freq == 0:\n",
    "                for _ in range(policy_update_freq):\n",
    "\n",
    "                    pi, log_pi, _ = actor.get_action(b_obs)\n",
    "\n",
    "                    qf1_pi = qf1(b_obs, pi)\n",
    "                    qf2_pi = qf2(b_obs, pi)\n",
    "\n",
    "                    min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        _, log_pi,_ = actor.get_action(b_obs)\n",
    "\n",
    "                    alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                    a_optimizer.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    a_optimizer.step()\n",
    "\n",
    "                    alpha = log_alpha.exp().item()\n",
    "            \n",
    "            if global_step % target_net_update_freq == 0:\n",
    "                soft_update(qf1_target, qf1)\n",
    "                soft_update(qf2_target, qf2)\n",
    "        \n",
    "        if t != updated_t and t % plot_update_freq == 0: \n",
    "            updated_t = t\n",
    "            loop.set_description(f\"avg_reward = {avg_reward:.2f}, best_score = {best_score}, episode_count = {episode_count}, buffer usage = {replay_buffer_usage(replay_buffer):.2f}\")\n",
    "            plot(history, save_path = FIG_SAVE_PATH)\n",
    "            pickle_dump(history, HISTORY_PATH)\n",
    "\n",
    "    plot(history, show = True, save_path = FIG_SAVE_PATH)\n",
    "    envs.reset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path =  /Volumes/SanDisk/NLP_RNN/Reinforcement Learning/deep rl/sac/output/InvertedPendulum-v4/baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg_reward = 974.40, best_score = 979.5700073242188, episode_count = 999, buffer usage = 29.28: 100%|██████████| 50000/50000 [04:57<00:00, 168.12it/s] \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1wklEQVR4nO3de3zU1b3v//fcMrmQDLmQDIEE4t5pQYOKYKlICy2ItiL1eE6xotQ+6unWjVJTtSjb7l3qqYmlu8ivZavV4xYrtXT3Uem2PW4LtBbLBgWjVC5eaiUhQEK4JJOEXCaZWb8/kvmSIdxCZjKXvJ6Pxzya+c6amTXL0Hnn811rfW3GGCMAAIAEY491BwAAAC4EIQYAACQkQgwAAEhIhBgAAJCQCDEAACAhEWIAAEBCIsQAAICERIgBAAAJyRnrDkRLMBjUoUOHlJmZKZvNFuvuAACA82CMUUtLiwoLC2W3n73WkrQh5tChQyoqKop1NwAAwAWora3V2LFjz9omaUNMZmampJ5ByMrKinFvAADA+WhublZRUZH1PX42SRtiQqeQsrKyCDEAACSY85kKwsReAACQkAgxAAAgIQ04xLz++uu64YYbVFhYKJvNpt/85jdhjxtjtHz5chUWFiotLU2zZs3Snj17wtp0dnZqyZIlysvLU0ZGhubPn68DBw6EtWlsbNSiRYvk8Xjk8Xi0aNEiNTU1DfgDAgCA5DTgEHPixAlddtllWr169WkfX7FihVauXKnVq1drx44d8nq9uuaaa9TS0mK1KS8v1/r167Vu3Tpt2bJFra2tmjdvngKBgNVm4cKF2rlzp1599VW9+uqr2rlzpxYtWnQBHxEAACQlMwiSzPr16637wWDQeL1e89hjj1nHOjo6jMfjMU899ZQxxpimpibjcrnMunXrrDYHDx40drvdvPrqq8YYY/bu3WskmTfeeMNqs23bNiPJvP/+++fVN5/PZyQZn883mI8IAACG0EC+vyM6J2bfvn2qr6/X3LlzrWNut1szZ87U1q1bJUlVVVXq6uoKa1NYWKiysjKrzbZt2+TxeDRt2jSrzac//Wl5PB6rzak6OzvV3NwcdgMAAMkroiGmvr5eklRQUBB2vKCgwHqsvr5eKSkpys7OPmub/Pz8fq+fn59vtTlVZWWlNX/G4/Gw0R0AAEkuKquTTl3bbYw553rvU9ucrv3ZXmfZsmXy+XzWrba29gJ6DgAAEkVEQ4zX65WkftWShoYGqzrj9Xrl9/vV2Nh41jaHDx/u9/pHjhzpV+UJcbvd1sZ2bHAHAEDyi2iIKSkpkdfr1caNG61jfr9fmzdv1vTp0yVJU6ZMkcvlCmtTV1en3bt3W22uuuoq+Xw+bd++3Wrz5ptvyufzWW0AAMDwNuDLDrS2tuqjjz6y7u/bt087d+5UTk6OiouLVV5eroqKCpWWlqq0tFQVFRVKT0/XwoULJUkej0d33HGH7r//fuXm5ionJ0cPPPCAJk2apDlz5kiSJk6cqOuuu07f+MY39NOf/lSS9A//8A+aN2+ePvnJT0bicwMAgAQ34BDz1ltv6XOf+5x1/7777pMk3X777VqzZo2WLl2q9vZ2LV68WI2NjZo2bZo2bNgQdiGnxx9/XE6nUwsWLFB7e7tmz56tNWvWyOFwWG1+/vOf65vf/Ka1imn+/Pln3JsGAAAMPzZjjIl1J6KhublZHo9HPp+P+TEAAJxBIGjUFQjq93vqtbO26YztUhx23fKpYo3Py4hqfwby/Z20V7EGAADndtfaKm3c238xzen87Uir/u/tV0a5R+ePEAMAwDDVHQiGBZj8TLe+PHVsv3bN7d164Y0avf7hUX3p3/7bOj65aKSWz79kSPp6OoQYAACGqf3H28LuT/+7XH372gn92gWCRpveO6w6X4f+0ueUkyfNFe0unhUhBgCAYeqjhlbr55K8DP2vKaff7d5ht2n94qu1+6Av7HjOiJSo9u9cCDEAAAxTfztyQpL0pcsL9f99ZfJZ23o9qfJ6UoeiW+ctKpcdAAAA8e9vR3oqMX83akSMe3JhCDEAAAxDwaDR2zU9lwD6+3xCDAAASBDvHvTp46MnlJHi0IzSvFh354IQYgAAGIZCk3ovKxqprNTYrjK6UIQYAACGoT990CBJUd+BN5oIMQAADDNHWjr1u3frJEkluYQYAACQILb+7aj18w2XFcawJ4NDiAEAYJip6l2V9L9nlMTd3i8DQYgBAGCY2Xe0Z5O7T3gzY9yTwSHEAAAwzNQc67lm0vgEng8jEWIAABhW/N1BHWgMhZj0GPdmcAgxAAAMIweb2hU0UprLoVGZ7lh3Z1AIMQAADCPVx3rmw4zLTZfNZotxbwaHEAMAwDCyv3c+zLgEP5UkEWIAABhWQpWYRJ/UKxFiAAAYVkIrk4qpxAAAgERCJQYAACScQNDowPF2ScyJAQAACaTO1y5/ICiXw6bRnrRYd2fQCDEAAAwTofkwRTnpctgTe3m1RIgBAGDYSJbLDYQQYgAAGCZq+mx0lwwIMQAADBPWbr05hBgAAJBAQqeTxuVxOgkAACQIYwxzYgAAQOI50tKp9q6AHHabxoxM/OXVEiEGAIBhobq3ClM4MlUpzuT4+k+OTwEAAM4qmS43EEKIAQBgGNgfmtSbJMurJUIMAADDwsnl1VRiAABAAqmhEgMAABKNMebknJgk2SNGIsQAAJD0mtq61NLRLUkqTpLdeiVCDAAASS9UhfFmpSrV5YhxbyKHEAMAQJJLxvkwEiEGAICkl2yXGwghxAAAkORqQsur86jEAACABJKMe8RIhBgAAJIec2IAAEDCaeno0rETfkmEGAAAkEBCVZjcjBRlprpi3JvIIsQAAJDEkvVUkkSIAQAgqdUc773cQJItr5YIMQAAJLWao6FKDCEGAAAkEGt5NaeTAABAImFODAAASDgdXQHVN3dIYk4MAABIIPuP91RhslKdGpmeXMurJUIMAABJq/poaD5Mhmw2W4x7E3mEGAAAklQyz4eRCDEAACStZN4jRiLEAACQtEKVmGIqMQAAIJGE9oihEnOeuru79Z3vfEclJSVKS0vTRRddpEceeUTBYNBqY4zR8uXLVVhYqLS0NM2aNUt79uwJe53Ozk4tWbJEeXl5ysjI0Pz583XgwIFIdxcAgKTk7w7qYGO7JGk8lZjz84Mf/EBPPfWUVq9erffee08rVqzQD3/4Q/3kJz+x2qxYsUIrV67U6tWrtWPHDnm9Xl1zzTVqaWmx2pSXl2v9+vVat26dtmzZotbWVs2bN0+BQCDSXQYAIOkcbGpX0EhpLodGZbpj3Z2ocEb6Bbdt26YvfelLuv766yVJ48eP1y9+8Qu99dZbknqqMKtWrdLDDz+sm266SZL0/PPPq6CgQC+++KLuvPNO+Xw+Pfvss3rhhRc0Z84cSdLatWtVVFSkTZs26dprr410twEASCp9LzeQjMurpShUYmbMmKE//OEP+vDDDyVJf/nLX7RlyxZ98YtflCTt27dP9fX1mjt3rvUct9utmTNnauvWrZKkqqoqdXV1hbUpLCxUWVmZ1eZUnZ2dam5uDrsBADBc1RxN3msmhUS8EvPggw/K5/NpwoQJcjgcCgQCevTRR3XLLbdIkurr6yVJBQUFYc8rKChQTU2N1SYlJUXZ2dn92oSef6rKykp973vfi/THAQAgIdX07tabrJN6pShUYn75y19q7dq1evHFF/X222/r+eef17/+67/q+eefD2t3amnLGHPOctfZ2ixbtkw+n8+61dbWDu6DAACQwE5udJe8ISbilZhvf/vbeuihh/SVr3xFkjRp0iTV1NSosrJSt99+u7xer6Seasvo0aOt5zU0NFjVGa/XK7/fr8bGxrBqTENDg6ZPn37a93W73XK7k3PiEgAAA9V3Tkyyinglpq2tTXZ7+Ms6HA5riXVJSYm8Xq82btxoPe73+7V582YroEyZMkUulyusTV1dnXbv3n3GEAMAAHoEgka1x5P7kgNSFCoxN9xwgx599FEVFxfrkksu0TvvvKOVK1fq61//uqSe00jl5eWqqKhQaWmpSktLVVFRofT0dC1cuFCS5PF4dMcdd+j+++9Xbm6ucnJy9MADD2jSpEnWaiUAAHB6db52dQWMUhx2jfakxbo7URPxEPOTn/xE//zP/6zFixeroaFBhYWFuvPOO/Uv//IvVpulS5eqvb1dixcvVmNjo6ZNm6YNGzYoMzPTavP444/L6XRqwYIFam9v1+zZs7VmzRo5HI5IdxkAgKQSmg8zNidNDntyLq+WJJsxxsS6E9HQ3Nwsj8cjn8+nrKysWHcHAIAh8/M3a/Tw+t36/IR8/fvXrox1dwZkIN/fXDsJAIAks/9Y8s+HkQgxAAAknWS/8GMIIQYAgCQTmhNTTCUGAAAkCmOMFWKoxAAAgIRxpKVT7V0BOew2jRmZvMurJUIMAABJpbq3CjNmZJpSnMn9NZ/cnw4AgGFmOFxuIIQQAwBAEqkhxAAAgEQ0XCb1SoQYAACSirW8OodKDAAASBDGmJMb3eVRiQEAAAmiqa1LLR3dkqjEAACABBKqwoz2pCrV5Yhxb6KPEAMAQJIYTvNhJEIMAABJY7hc+DGEEAMAQJLY31uJGZdHJQYAACQQKjEAACAhMScGAAAknJaOLh074Zc0PC45IBFiAABICqEqTN6IFGWmumLcm6FBiAEAIAkMt1NJEiEGAICkMNwm9UqEGAAAkoK1vJoQAwAAEsnJCz9yOgkAACQQ5sQAAICE09EVUH1zhyTmxAAAgASy/3hPFSYr1amR6cNjebVEiAEAIOFVHw3Nh8mQzWaLcW+GDiEGAIAENxznw0iEGAAAEt5w3CNGIsQAAJDwQnNihss1k0IIMQAAJLhQJWY4bXQnEWIAAEho/u6gDja2S5LGU4kBAACJ4mBTu4JGSnM5NCrTHevuDClCDAAACezkqaT0YbW8WiLEAACQ0GqOngwxww0hBgCABFbdu0fMcFteLRFiAABIaCeXVxNiAABAAjm50R2nkwAAQIIIBI1qeysxxYQYAACQKA41tasrYJTisGu0Jy3W3RlyhBgAABJUaD5MUU6aHPbhtbxaIsQAAJCwhuvlBkIIMQAAJKiaY8Pzwo8hhBgAABJUjbUyiUoMAABIIFRiAABAwjHGMCcm1h0AAAAD19DSqY6uoBx2m8aMHH7LqyVCDAAACSl0KmnMyDSlOIfn1/nw/NQAACS4k6eShud8GIkQAwBAQqohxBBiAABIRNW9p5OG6/JqiRADAEBC2m8trybEAACABBG+vJrTSQAAIEE0tnWppaNbklScQ4gBAAAJIlSFGe1JVarLEePexA4hBgCABLN/mF9uIIQQAwBAgrHmw+QM30m9UpRCzMGDB3XbbbcpNzdX6enpuvzyy1VVVWU9bozR8uXLVVhYqLS0NM2aNUt79uwJe43Ozk4tWbJEeXl5ysjI0Pz583XgwIFodBcAgIRiXfgxj0pMRDU2Nurqq6+Wy+XSf/3Xf2nv3r360Y9+pJEjR1ptVqxYoZUrV2r16tXasWOHvF6vrrnmGrW0tFhtysvLtX79eq1bt05btmxRa2ur5s2bp0AgEOkuAwCQUEIb3Q3nPWIkyRnpF/zBD36goqIiPffcc9ax8ePHWz8bY7Rq1So9/PDDuummmyRJzz//vAoKCvTiiy/qzjvvlM/n07PPPqsXXnhBc+bMkSStXbtWRUVF2rRpk6699tpIdxsAgIRRw5wYSVGoxLz88suaOnWqvvzlLys/P1+TJ0/WM888Yz2+b98+1dfXa+7cudYxt9utmTNnauvWrZKkqqoqdXV1hbUpLCxUWVmZ1eZUnZ2dam5uDrsBAJBsmju6dOyEX9Lw3uhOikKI+fjjj/Xkk0+qtLRUv//973XXXXfpm9/8pn72s59Jkurr6yVJBQUFYc8rKCiwHquvr1dKSoqys7PP2OZUlZWV8ng81q2oqCjSHw0AgJgLrUzKG5GiEe6In1BJKBEPMcFgUFdccYUqKio0efJk3XnnnfrGN76hJ598MqydzWYLu2+M6XfsVGdrs2zZMvl8PutWW1s7uA8CAEAcquFyA5aIh5jRo0fr4osvDjs2ceJE7d+/X5Lk9XolqV9FpaGhwarOeL1e+f1+NTY2nrHNqdxut7KyssJuAAAkm5PLq4f3fBgpCiHm6quv1gcffBB27MMPP9S4ceMkSSUlJfJ6vdq4caP1uN/v1+bNmzV9+nRJ0pQpU+RyucLa1NXVaffu3VYbAACGoxrrmklUYiJ+Mu1b3/qWpk+froqKCi1YsEDbt2/X008/raefflpSz2mk8vJyVVRUqLS0VKWlpaqoqFB6eroWLlwoSfJ4PLrjjjt0//33Kzc3Vzk5OXrggQc0adIka7USAADDUXXv6aTxw3yPGCkKIebKK6/U+vXrtWzZMj3yyCMqKSnRqlWrdOutt1ptli5dqvb2di1evFiNjY2aNm2aNmzYoMzMTKvN448/LqfTqQULFqi9vV2zZ8/WmjVr5HAM32tEAACwnzkxFpsxxsS6E9HQ3Nwsj8cjn8/H/BgAQFJo9wc08V9elSS988/XKDsjJcY9iryBfH9z7SQAABLE/uM9VZisVKdGprti3JvYI8QAAJAgrMsN5GWcc1uS4YAQAwBAgmCPmHCEGAAAEgR7xIQjxAAAkCC48GM4QgwAAAmi5vjJOTEgxAAAkBD83UEdbGyXxOmkEEIMAAAJ4EBjm4JGSnM5NCrTHevuxAVCDAAACaDvfBiWV/cgxAAAkACsPWJYXm0hxAAAkACqWZnUDyEGAIAEEKrEsNHdSYQYAAASQE3vdZPGU4mxEGIAAIhzgaBRbW+IGcceMRZCDAAAce5QU7u6AkYpDru8Wamx7k7cIMQAABDnQsuri3LS5LCzvDqEEAMAQJyzLjfApN4whBgAAOJcqBJTzKTeMIQYAADiXPVRKjGnQ4gBACDO1bDR3WkRYgAAiGPGGObEnAEhBgCAONbQ0qmOrqAcdpvGZKfFujtxhRADAEAcC82HGTMyTS4HX9t9MRoAAMSx0OUGmA/THyEGAIA4FrrwI/Nh+iPEAAAQx6pZmXRGhBgAAOJYqBIzjkpMP4QYAADilDHG2iNmPJWYfggxAADEqca2LrV0dEuSinIIMacixAAAEKeqe08ljfakKtXliHFv4g8hBgCAOHVyPgxVmNMhxAAAEKdOzodhUu/pEGIAAIhToRBTTCXmtAgxAADEqWo2ujsrQgwAAHGqho3uzooQAwBAHGru6NLxE35JbHR3JoQYAADi0P7eKkzeCLdGuJ0x7k18IsQAABCHTs6H4VTSmRBiAACIQyfnw3Aq6UwIMQAAxKHqo2x0dy6EGAAA4hArk86NEAMAQBxij5hzI8QAABBn2vzdamjplESIORtCDAAAcSZ0KmlkukuedFeMexO/CDEAAMQZViadH0IMAABxpoY9Ys4LIQYAgDhTTSXmvBBiAACIM6FKzLgcKjFnQ4gBACDOhObEjM8jxJwNIQYAgDjS0RXQIV+7JE4nnQshBgCAOHKgsU3GSCPcTuVmpMS6O3GNEAMAQBzpe7kBm80W497EN0IMAABxJLQyiZ16z40QAwBAHLFWJrFHzDkRYgAAiCPVXL36vBFiAACII9VHQ5UYTiedCyEGAIA44e8O6kBjTyWmJI8Qcy6EGAAA4sSBxjYFjZTmcig/0x3r7sS9qIeYyspK2Ww2lZeXW8eMMVq+fLkKCwuVlpamWbNmac+ePWHP6+zs1JIlS5SXl6eMjAzNnz9fBw4ciHZ3AQCImeo+k3pZXn1uUQ0xO3bs0NNPP61LL7007PiKFSu0cuVKrV69Wjt27JDX69U111yjlpYWq015ebnWr1+vdevWacuWLWptbdW8efMUCASi2WUAAGKm+iinkgYiaiGmtbVVt956q5555hllZ2dbx40xWrVqlR5++GHddNNNKisr0/PPP6+2tja9+OKLkiSfz6dnn31WP/rRjzRnzhxNnjxZa9eu1a5du7Rp06ZodRkAgJgKVWLGE2LOS9RCzN13363rr79ec+bMCTu+b98+1dfXa+7cudYxt9utmTNnauvWrZKkqqoqdXV1hbUpLCxUWVmZ1eZUnZ2dam5uDrsBAJBI9vWuTBrP8urz4ozGi65bt05vv/22duzY0e+x+vp6SVJBQUHY8YKCAtXU1FhtUlJSwio4oTah55+qsrJS3/ve9yLRfQAAYsKqxLC8+rxEvBJTW1ure++9V2vXrlVqauoZ2506YckYc85JTGdrs2zZMvl8PutWW1s78M4DABAj/u6gDjb2XL2aOTHnJ+IhpqqqSg0NDZoyZYqcTqecTqc2b96sH//4x3I6nVYF5tSKSkNDg/WY1+uV3+9XY2PjGducyu12KysrK+wGAECiqO1dXp2e4tAollefl4iHmNmzZ2vXrl3auXOndZs6dapuvfVW7dy5UxdddJG8Xq82btxoPcfv92vz5s2aPn26JGnKlClyuVxhberq6rR7926rDQAAyaTvTr0srz4/EZ8Tk5mZqbKysrBjGRkZys3NtY6Xl5eroqJCpaWlKi0tVUVFhdLT07Vw4UJJksfj0R133KH7779fubm5ysnJ0QMPPKBJkyb1mygMAEAyCF0zqSSPSb3nKyoTe89l6dKlam9v1+LFi9XY2Khp06Zpw4YNyszMtNo8/vjjcjqdWrBggdrb2zV79mytWbNGDocjFl0GACCqqo8yqXegbMYYE+tORENzc7M8Ho98Ph/zYwAAcW/Rs2/qz389qhX/81ItuLIo1t2JmYF8f3PtJAAA4oC1Rwwrk84bIQYAgBjr7A7oUFPP8urxzIk5b4QYAABirPZ4u4JGykhxaNQIllefL0IMAAAxxvLqC0OIAQAgxkKXG2Cn3oEhxAAAEGMnr17NfJiBIMQAABBj1Ud7Nrobxx4xA0KIAQAgxjiddGEIMQAAxFDf5dXjcjmdNBCEGAAAYuhAI8urLxQhBgCAGKo5xvLqC0WIAQAghkKTelmZNHCEGAAAYihUiSnOYVLvQBFiAACIoepjvZUYJvUOGCEGAIAY6jsnBgNDiAEAIEa6AkEdaOTq1ReKEAMAQIwcampXd9DI7bSrIDM11t1JOIQYAABiJDQfZlxuuux2llcPFCEGAIAY2c98mEEhxAAAECOsTBocQgwAADHCyqTBIcQAABAjfefEYOAIMQAAxEAgaLTfOp1EJeZCEGIAAIiB+uYO+QNBuRw2jfawvPpCEGIAAIiBmqM982GKstPldPB1fCEYNQAAYoD5MINHiAEAIAZYmTR4hBgAAGKghj1iBo0QAwBADFSHKjF5VGIuFCEGAIAhZozpU4khxFwoQgwAAEPsSEun2rsCstukMSPTYt2dhEWIAQBgiIVWJo3JTlOKk6/iC8XIAQAwxELzYTiVNDiEGAAAhtjJ5dWsTBoMQgwAAEOsmkm9EUGIAQBgiLHRXWQQYgAAGELhy6s5nTQYhBgAAIZQY1uXWjq6ZbNJRTmEmMEgxAAAMIRCK5NGZ6Uq1eWIcW8SGyEGAIAhFJoPU8yppEEjxAAAMISqj7IyKVIIMQAADCFWJkUOIQYAgCFUzcqkiCHEAAAwhKjERA4hBgCAIeJr71JjW5ckLjkQCYQYAACGyP7eU0mjMt3KcDtj3JvER4gBAGCInLx6NVWYSCDEAAAwRJgPE1mEGAAAhkhoZdI4LjcQEYQYAACGiFWJyaMSEwmEGAAAhgh7xEQWIQYAgCFworNbR1o6JUnjcqjERAIhBgCAIVDTW4XJTnfJk+6KcW+SAyEGAIAhwMqkyCPEAAAwBGqOMx8m0ggxAAAMASoxkUeIAQBgCFQf7a3E5FGJiRRCDAAAQyBUiSlmZVLERDzEVFZW6sorr1RmZqby8/N144036oMPPghrY4zR8uXLVVhYqLS0NM2aNUt79uwJa9PZ2aklS5YoLy9PGRkZmj9/vg4cOBDp7gIAEHUdXQEd8nVIYk5MJEU8xGzevFl333233njjDW3cuFHd3d2aO3euTpw4YbVZsWKFVq5cqdWrV2vHjh3yer265ppr1NLSYrUpLy/X+vXrtW7dOm3ZskWtra2aN2+eAoFApLsMAEBU1fZO6s10O5WTkRLj3iQPmzHGRPMNjhw5ovz8fG3evFmf/exnZYxRYWGhysvL9eCDD0rqqboUFBToBz/4ge688075fD6NGjVKL7zwgm6++WZJ0qFDh1RUVKRXXnlF11577Tnft7m5WR6PRz6fT1lZWdH8iAAAnNXGvYf1jZ+9pbIxWfrdks/EujtxbSDf31GfE+Pz+SRJOTk5kqR9+/apvr5ec+fOtdq43W7NnDlTW7dulSRVVVWpq6srrE1hYaHKysqsNqfq7OxUc3Nz2A0AgHjAyqToiGqIMcbovvvu04wZM1RWViZJqq+vlyQVFBSEtS0oKLAeq6+vV0pKirKzs8/Y5lSVlZXyeDzWraioKNIfBwCAC1LdG2KYDxNZUQ0x99xzj95991394he/6PeYzWYLu2+M6XfsVGdrs2zZMvl8PutWW1t74R0HACCCQpccoBITWVELMUuWLNHLL7+s1157TWPHjrWOe71eSepXUWloaLCqM16vV36/X42NjWdscyq3262srKywGwAA8aDGuno1ISaSIh5ijDG655579NJLL+mPf/yjSkpKwh4vKSmR1+vVxo0brWN+v1+bN2/W9OnTJUlTpkyRy+UKa1NXV6fdu3dbbQAASAT+7qAONHLJgWhwRvoF7777br344ov6z//8T2VmZloVF4/Ho7S0NNlsNpWXl6uiokKlpaUqLS1VRUWF0tPTtXDhQqvtHXfcofvvv1+5ubnKycnRAw88oEmTJmnOnDmR7jIAAFFzsKldQSOluRwalemOdXeSSsRDzJNPPilJmjVrVtjx5557Tl/72tckSUuXLlV7e7sWL16sxsZGTZs2TRs2bFBmZqbV/vHHH5fT6dSCBQvU3t6u2bNna82aNXI4HJHuMgAAUVNtrUxKP+fcTwxM1PeJiRX2iQEAxIM1/71Py3+7V9deUqCfLpoa6+7EvbjaJwYAgOGsmkm9UUOIAQAgitjoLnoIMQAARNHJ5dWsTIo0QgwAAFESCBrV9i6vHpdHJSbSCDEAAETJoaZ2dQWMUpx2jc5KjXV3kg4hBgCAKAmdSirOSZfdzvLqSCPEAAAQJdYeMTnMh4kGQgwAAFHCyqToIsQAABAl1h4xeVRiooEQAwBAlFCJiS5CDAAAURAMGvaIiTJCDAAAUXC4pUOd3UE57TaNGZkW6+4kJUIMAABRUH20pwozNjtNTgdft9HAqAIAEAX7jzMfJtoIMQAAREE182GijhADAEAUhFYmFVOJiRpCDAAAURCaE0MlJnoIMQAARJgxhj1ihgAhBgCACDva6tcJf0A2m1SUw/LqaCHEAAAQYaEqTKEnTW6nI8a9SV6EGAAAIoxrJg0NQgwAABHGfJihQYgBACDCuGbS0CDEAAAQYVRihgYhBgCACAvNiRlHJSaqCDEAAETQ0dZO+dq7JEnFOYSYaCLEAAAQQb9664AkaeLoLKWnOGPcm+RGiAEAIEI6ugJas3WfJOnrV4+PbWeGAUIMAAAR8quqAzrc3KlCT6rmX14Y6+4kPUIMAAAR8m5tkyTpf00Zy069Q4AQAwBAhHxwuEWS9ElvVox7MjwQYgAAiID/eKtW7x7wSZIuLiTEDAVCDAAAg2SM0b/+/gNJ0v+8YqxK8tjkbigQYgAAGKQ3Pj6uhpZOSdL3byyLcW+GDxawAwCiJhg0+vhoqz5qaFXNsTbVNrbJ5bCrKDtdV47P0Se8I5JiAuwTf/pIkvSJghFKS0n8z5MoCDEAgIir93Xouf/ep9/vqbe24D8dT5pLZWOyVOhJ0+iRaRozMlXFORma4M1UdkbKEPZ4cD4+0nOtpDtmlMS4J8MLIQYAEBHv1zfrP3Yc0Fs1x60JrpLkdto1wZup8XkZKspOV1cwqA/rW/RWTaN87V3674+Onfb1RntSlZXq0ie9mSrNH6H65g75u4Mam52u7AyXbJLSU5wan5euTxRkKjPVZT23szugprYu2WxSVqpLqa7oVUc6uwM65GuXJH1+QkHU3gf9EWIAABesOxDUjupGbdhbr3Xba9XeFbAeu2hUhm6/arxunDxGnjTXaZ/77kGfPj5yQnVN7Trk69ChpnZ9fLRVtcfbVefrUJ2vw1q2fDZ2W8+y5qxUp1wOu3bWNqm1s1uSlOK0qzR/hNJTHPJ60mSTFAgaBYJG3UGjORPzNbk4W8U56Rd0KujjIydkjJSR4lDeiMSpHiUDQgwAYEDa/N16/cMj2rDnsP74QYOa2rqsx6aOy9aCK4v02dJR8npSz/o6ToddVxRn64ri7H6PtXR06b26Fh0/0an36lr0fn2zRnvS5Gvvkt1mU5u/J6AcO+FX7fE21fk69F5d82nfx98d1J5Docca+z2+6b3D1s9FOWma8fejNGdivi4dO1LZ6S45Hf3XwBhj1NEVVEtnl27/9+2SpCnjc2Sz2c76mRFZNmOMiXUnoqG5uVkej0c+n09ZWazXB4DBau3s1o82fKD/2FGrE/6TFZfsdJc+P6FA115SoDkTC2S3D/0Xeb2vQ+/sb1RLR7c6ugO6eHSWrijOls0m7axtUlN7l1o7unW4uUM2m00Om+Sw23S01a8New9r/7ETYZ+pL0+aSzkZKcpwO+TvDqq9K6DDzZ3ydwfD2q26+XLdOHnMUHzcpDaQ729CDADgrNr9Ab2yq04rfv++Djf3LCMuyknTtRd7dc3FBZoyLvu01YpEEgga1fna9Yf3GrR933Ft+/iYGtv8Op9vyIvyMvR/bizT9L/LpRITAYQYEWIAYLA6ugJ6YVuNfvr6xzra2hNeMt1O/fDLl+naSwqS/gs7EDRqavOrsc2vY61+tfkDcjvtcrscys90a2S6S0Y9Y5LsYzGUBvL9zZwYAECYIy2demrz3/S7dw9ZlZcxI9N0y6eKtHDaOOUk0NLnwXDYbcod4VbuCLf+Pj/WvcHpEGIAAJJ6Vgut21GrFa++r+aOnomzY0am6d7ZpfofV4yRK8FPGSH5EGIAYJg70dmtTe8d1uo/fqS/NrRKki4pzNI9n/t7fW5CflT3WAEGgxADAMPUXw+36OnXP9Zv3z2kjq6elTYj010qn12q2z49LuEn6yL5EWIAYJjp7A5o3fZaff//7VVXoGdtx7jcdH3pskLd8ZmLTrsxHRCPCDEAMEw0tfn1xJ/+pue3Vquzd4+Tz31ylO75fKmuKB7JChskHEIMACS55o4uPfvnffr3LfvU0rsVf0aKQ+VzPqH//ZkSwgsSFiEGAJLYlr8e1YO/flcHm3ouUDjBm6ml131Ssz6RH5OddYFIIsQAQBL6qKFFFa+8rz++3yBJKs5J14PXTdAXyryEFyQNQgwAJJFjrZ1atemvenH7fgWCRk67Tbd9epy+dc0nmLCLpEOIAYAk4Gvv0k83/01rtlarrfdChnMvLtBDX5igi0aNiHHvgOggxABAAjPG6GfbavTEnz6yLhEwaYxH//TFibrq73Jj3DsguggxAJCAjDF6c99xrXj1fb29v0lSz7yXf553seZMzGfFEYYFQgwAJIiOroB2VB/Xa+8f0e/31FsrjlJddj0w95O67dPjuEQAhhVCDADEqXZ/QHsO+fT2/ka9Vd2oN/cdl6+9y3o8zeXQ/7hijO6dXaqCrNQY9hSIDUIMAMRIVyCow80dqvd16JCvQ/W+dtX5OrT3ULNqjrXpSGunAkET9pyCLLc+UzpKcy8u0Gc/MYrKC4a1uA8xTzzxhH74wx+qrq5Ol1xyiVatWqXPfOYzse6WAkGjD+pb9NeGFtUeb9NbNY16Z3+TcjNSNCrTrfysVGWmOuW022TvPTdtt9lkt0l2u002SbbQfZtNNlv4/UDQKBA0cthtPa9h72kTeh2n3WY95rDb5XScPObofU9772vaQu9tP+W+9b7h9099XtD09MXlsMtxyv4SNpvU01LWa0mnHOvTVqc93ntM4c8/9ZR+3zFyOexy2Gz93s96obD37HfYek9jjIJBKdD7GQNBo6Ax/T6bkVHvYZne5/U8/+Rrnnz8DG37tjFGnd3B3ltAgeCZnxP6IfS6xpzpdU2fx8L7cmqfjfr83Odx0+dNQ88N9rYPhl449Ptn6/m9dNh7fk9Cv3f+7qD8gaBkpGDva5pTXsM6ZnqOhcY8aMLfU5KcdptcDruCpueaP/7uoIJBo0Dvc0/+G+rtU+jfWO9xh83W53dccthtvb9HJ38jRqQ65XbaT/4b7fMcSeoOBtUVMOoKBK1Q0bdNd9CoqzuorkBQXUEjf3dQJzq7T36uoNGBxnad8HerzR/QwaZ2Nbd369iJzrDfodMZlenW5UUjNXVctqaOz9ZlY0dyYUagV1yHmF/+8pcqLy/XE088oauvvlo//elP9YUvfEF79+5VcXFxTPrU5u9WxSvv6T/fOWRt392Xr71LHx89EYOeAUhELodNXk+qRnvSNNqTKq8nVUXZ6Zo0xiOvJ1X5mW4m6QJnYDPmXH8HxM60adN0xRVX6Mknn7SOTZw4UTfeeKMqKyvP+tzm5mZ5PB75fD5lZWVFrE//988f6/v/7z3rfnFOuq4cn6PRnlRdWZKjFIddDS0dOtLSqdbO7p6/KE3PX/dB0/evz5N/gfb9izTQe99u6/kLtG+FICRoeipB3UGj7kBQ3b2P9/xvUN0BE/aXb+i9ZMLvG+uvXhP2F3PofjB48nO7HDZ1BXreJ+yv+N7HQ5WAkz+fvQrRt8Jw1ranea9g72eNhr6Vs1AlIqzaY/18rqpTn9rQqcd777udDrlddqU47GF/WYdXrvpXp2y9B0N9CFX1+vYr1Ne+90+tlJ18/XO8Vqiq1/scY/pXrkK/f8YYuZ0OuRwnK3vq0+++1b9QH/tWSkLvH6oIht7L3x2Uw26T22m3KoKh6mHov1OoL33/zQWC4f/eev599fxuB/v8HrZ0dKkr0P/5wZ6Ckly9lc4UZ897hypYoddw2m1yOuxKcfRUjZwOu0a4nepbMBntSVNWmksj3A7ljXArp7dqm5fhZgddoI+BfH/HbSXG7/erqqpKDz30UNjxuXPnauvWrf3ad3Z2qrOz07rf3NwclX59bfp47axt0qdKcnTj5DHKSmUHzFgIhb7Qz1KfUy/qH4L6HjuVo8+pEQBA4ojbEHP06FEFAgEVFBSEHS8oKFB9fX2/9pWVlfre974X9X45HXatXnhF1N8HZ9cz/8G6F8uuAABiJO5nh516LtgYc9rzw8uWLZPP57NutbW1Q9VFAAAQA3FbicnLy5PD4ehXdWloaOhXnZEkt9stt9s9VN0DAAAxFreVmJSUFE2ZMkUbN24MO75x40ZNnz49Rr0CAADxIm4rMZJ03333adGiRZo6daquuuoqPf3009q/f7/uuuuuWHcNAADEWFyHmJtvvlnHjh3TI488orq6OpWVlemVV17RuHHjYt01AAAQY3G9T8xgRGufGAAAED0D+f6O2zkxAAAAZ0OIAQAACYkQAwAAEhIhBgAAJCRCDAAASEiEGAAAkJAIMQAAICHF9WZ3gxHa/qa5uTnGPQEAAOcr9L19PtvYJW2IaWlpkSQVFRXFuCcAAGCgWlpa5PF4ztomaXfsDQaDOnTokDIzM2Wz2SL62s3NzSoqKlJtbS27AUcR4zw0GOehwTgPDcZ5aERznI0xamlpUWFhoez2s896SdpKjN1u19ixY6P6HllZWfwjGQKM89BgnIcG4zw0GOehEa1xPlcFJoSJvQAAICERYgAAQEIixFwAt9ut7373u3K73bHuSlJjnIcG4zw0GOehwTgPjXgZ56Sd2AsAAJIblRgAAJCQCDEAACAhEWIAAEBCIsQAAICERIgZoCeeeEIlJSVKTU3VlClT9Oc//znWXUoolZWVuvLKK5WZman8/HzdeOON+uCDD8LaGGO0fPlyFRYWKi0tTbNmzdKePXvC2nR2dmrJkiXKy8tTRkaG5s+frwMHDgzlR0kYlZWVstlsKi8vt44xxpFz8OBB3XbbbcrNzVV6erouv/xyVVVVWY8z1oPX3d2t73znOyopKVFaWpouuugiPfLIIwoGg1YbxnngXn/9dd1www0qLCyUzWbTb37zm7DHIzWmjY2NWrRokTwejzwejxYtWqSmpqbIfAiD87Zu3TrjcrnMM888Y/bu3Wvuvfdek5GRYWpqamLdtYRx7bXXmueee87s3r3b7Ny501x//fWmuLjYtLa2Wm0ee+wxk5mZaX7961+bXbt2mZtvvtmMHj3aNDc3W23uuusuM2bMGLNx40bz9ttvm8997nPmsssuM93d3bH4WHFr+/btZvz48ebSSy819957r3WcMY6M48ePm3Hjxpmvfe1r5s033zT79u0zmzZtMh999JHVhrEevO9///smNzfX/O53vzP79u0zv/rVr8yIESPMqlWrrDaM88C98sor5uGHHza//vWvjSSzfv36sMcjNabXXXedKSsrM1u3bjVbt241ZWVlZt68eRH5DISYAfjUpz5l7rrrrrBjEyZMMA899FCMepT4GhoajCSzefNmY4wxwWDQeL1e89hjj1ltOjo6jMfjMU899ZQxxpimpibjcrnMunXrrDYHDx40drvdvPrqq0P7AeJYS0uLKS0tNRs3bjQzZ860QgxjHDkPPvigmTFjxhkfZ6wj4/rrrzdf//rXw47ddNNN5rbbbjPGMM6RcGqIidSY7t2710gyb7zxhtVm27ZtRpJ5//33B91vTiedJ7/fr6qqKs2dOzfs+Ny5c7V169YY9Srx+Xw+SVJOTo4kad++faqvrw8bZ7fbrZkzZ1rjXFVVpa6urrA2hYWFKisr479FH3fffbeuv/56zZkzJ+w4Yxw5L7/8sqZOnaovf/nLys/P1+TJk/XMM89YjzPWkTFjxgz94Q9/0IcffihJ+stf/qItW7boi1/8oiTGORoiNabbtm2Tx+PRtGnTrDaf/vSn5fF4IjLuSXsByEg7evSoAoGACgoKwo4XFBSovr4+Rr1KbMYY3XfffZoxY4bKysokyRrL041zTU2N1SYlJUXZ2dn92vDfose6dev09ttva8eOHf0eY4wj5+OPP9aTTz6p++67T//0T/+k7du365vf/Kbcbre++tWvMtYR8uCDD8rn82nChAlyOBwKBAJ69NFHdcstt0jidzoaIjWm9fX1ys/P7/f6+fn5ERl3QswA2Wy2sPvGmH7HcH7uuecevfvuu9qyZUu/xy5knPlv0aO2tlb33nuvNmzYoNTU1DO2Y4wHLxgMaurUqaqoqJAkTZ48WXv27NGTTz6pr371q1Y7xnpwfvnLX2rt2rV68cUXdckll2jnzp0qLy9XYWGhbr/9dqsd4xx5kRjT07WP1LhzOuk85eXlyeFw9EuODQ0N/ZIqzm3JkiV6+eWX9dprr2ns2LHWca/XK0lnHWev1yu/36/GxsYzthnOqqqq1NDQoClTpsjpdMrpdGrz5s368Y9/LKfTaY0RYzx4o0eP1sUXXxx2bOLEidq/f78kfp8j5dvf/rYeeughfeUrX9GkSZO0aNEifetb31JlZaUkxjkaIjWmXq9Xhw8f7vf6R44cici4E2LOU0pKiqZMmaKNGzeGHd+4caOmT58eo14lHmOM7rnnHr300kv64x//qJKSkrDHS0pK5PV6w8bZ7/dr8+bN1jhPmTJFLpcrrE1dXZ12797NfwtJs2fP1q5du7Rz507rNnXqVN16663auXOnLrroIsY4Qq6++up+WwR8+OGHGjdunCR+nyOlra1Ndnv415XD4bCWWDPOkRepMb3qqqvk8/m0fft2q82bb74pn88XmXEf9NTgYSS0xPrZZ581e/fuNeXl5SYjI8NUV1fHumsJ4x//8R+Nx+Mxf/rTn0xdXZ11a2trs9o89thjxuPxmJdeesns2rXL3HLLLadd1jd27FizadMm8/bbb5vPf/7zw3qp5Ln0XZ1kDGMcKdu3bzdOp9M8+uij5q9//av5+c9/btLT083atWutNoz14N1+++1mzJgx1hLrl156yeTl5ZmlS5dabRjngWtpaTHvvPOOeeedd4wks3LlSvPOO+9Y24ZEakyvu+46c+mll5pt27aZbdu2mUmTJrHEOlb+7d/+zYwbN86kpKSYK664wloajPMj6bS35557zmoTDAbNd7/7XeP1eo3b7Taf/exnza5du8Jep7293dxzzz0mJyfHpKWlmXnz5pn9+/cP8adJHKeGGMY4cn7729+asrIy43a7zYQJE8zTTz8d9jhjPXjNzc3m3nvvNcXFxSY1NdVcdNFF5uGHHzadnZ1WG8Z54F577bXT/v/x7bffboyJ3JgeO3bM3HrrrSYzM9NkZmaaW2+91TQ2NkbkM9iMMWbw9RwAAIChxZwYAACQkAgxAAAgIRFiAABAQiLEAACAhESIAQAACYkQAwAAEhIhBgAAJCRCDAAASEiEGAAAkJAIMQAAICERYgAAQEIixAAAgIT0/wM568kVw1esQAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device= DEVICE\n",
    "q_lr = 1e-3\n",
    "actor_lr = 1e-3\n",
    "total_timesteps = int(5e4)\n",
    "warmup_steps = int(1e3)\n",
    "buffer_size = int(5e5)\n",
    "batch_size = 256\n",
    "fc_hidden_size = 256\n",
    "\n",
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'\n",
    "\n",
    "actor = Actor(envs, hidden_size=fc_hidden_size).to(device)\n",
    "qf1 = QNetwork(envs).to(device)\n",
    "qf2 = QNetwork(envs).to(device)\n",
    "qf1_target = QNetwork(envs).to(device)\n",
    "qf2_target = QNetwork(envs).to(device)\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "\n",
    "q_optimizer = torch.optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr = q_lr) \n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_lr) \n",
    "#NOTE: use fixec entropy method mentioned in Spin-up, improve this later with enforce method\n",
    "# https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "\n",
    "\n",
    "tune(envs, actor, qf1, qf2, qf1_target, qf2_target, \n",
    "q_optimizer, actor_optimizer, device = device,\n",
    "total_timesteps= total_timesteps, warmup_steps=warmup_steps,\n",
    "buffer_size=buffer_size, batch_size=batch_size, label = 'baseline'\n",
    ")\n",
    "\n",
    "envs.close()\n",
    "\n",
    "\n",
    "# #check to make sure this is continous action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
