{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "https://spinningup.openai.com/en/latest/algorithms/sac.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': \"InvertedDoublePendulum-v4\"\n",
    "}\n",
    "NUM_ENVS = 3\n",
    "SEED = 1\n",
    "\n",
    "LR = 1e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "LOG_STD_MAX = 2\n",
    "LOG_STD_MIN = -5\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output', ENV_ARGS['id'])\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gamma, **env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    # env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    # env = gym.wrappers.ClipAction(env)\n",
    "    # env = gym.wrappers.NormalizeObservation(env)\n",
    "    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    # env = gym.wrappers.NormalizeReward(env, gamma = gamma)\n",
    "    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "tensor([-1.])\n",
      "tensor([2.])\n",
      "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999, 0.3971, 0.7544],\n",
      "        [0.5695, 0.4388, 0.6387, 0.5247, 0.6826, 0.3051, 0.4635, 0.4550],\n",
      "        [0.5725, 0.4980, 0.9371, 0.6556, 0.3138, 0.1980, 0.4162, 0.2843]])\n",
      "tensor([[-0.7576, -0.2793, -0.4031, -0.7347, -0.0293, -0.7999, -0.3971, -0.7544],\n",
      "        [-0.5695, -0.4388, -0.6387, -0.5247, -0.6826, -0.3051, -0.4635, -0.4550],\n",
      "        [-0.5725, -0.4980, -0.9371, -0.6556, -0.3138, -0.1980, -0.4162, -0.2843]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(envs.single_action_space.high)\n",
    "b = torch.tensor(envs.single_action_space.low)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "c = a - b\n",
    "print(c)\n",
    "\n",
    "d = torch.rand(3,8)\n",
    "print(d)\n",
    "\n",
    "print(d*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self,envs: gym.Env, hidden_size = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_shape = np.prod(envs.single_observation_space.shape)\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape + self.action_shape, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(hidden_size, 1)),\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim = 1)\n",
    "        return self.network(x)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.Env, hidden_size = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_shape = np.prod(envs.single_observation_space.shape)\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc_mean = nn.Linear(hidden_size, self.action_shape)\n",
    "        self.fc_logstd = nn.Linear(hidden_size, self.action_shape)\n",
    "\n",
    "        #NOTE: register buffer so that optimizer will not update its values\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((envs.single_action_space.high - envs.single_action_space.low)/2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((envs.single_action_space.high + envs.single_action_space.low)/2.0, dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.network(X)\n",
    "        mean = self.fc_mean(X)\n",
    "        logstd = torch.tanh(self.fc_logstd(X))\n",
    "        # https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "        logstd = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (logstd + 1)\n",
    "\n",
    "        return mean, logstd\n",
    "    \n",
    "    def get_action(self, X):\n",
    "\n",
    "        mean, logstd = self(X)\n",
    "        #exponential to convert from log(std) to std\n",
    "        std = logstd.exp()\n",
    "\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        #reparameterize trick \n",
    "        # https://stackoverflow.com/questions/60533150/what-is-the-difference-between-sample-and-rsample\n",
    "        x_t = normal.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "\n",
    "        #action scale and bias is registered as buffer in init()\n",
    "        # print('y t shape = ', y_t.shape)\n",
    "        # print('scale shape = ', self.action_scale.shape)\n",
    "        # print('bias shape = ', self.action_bias.shape)\n",
    "\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "\n",
    "        logprob = normal.log_prob(x_t)\n",
    "\n",
    "        logprob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
    "        logprob = logprob.sum(1, keepdim = True)\n",
    "\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "\n",
    "        return action, logprob, mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape =  torch.Size([3, 11])\n",
      "action shape =  torch.Size([3, 1])\n",
      "q value =  torch.Size([3, 1])\n",
      "actor action =  torch.Size([3, 1])\n",
      "logprob  =  torch.Size([3, 1])\n",
      "mean  =  torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#Test agent\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "obs, info = envs.reset()\n",
    "\n",
    "actor = Actor(envs)\n",
    "qnet = QNetwork(envs)\n",
    "\n",
    "obs = torch.tensor(obs).float()\n",
    "action = torch.tensor(envs.action_space.sample()).float()\n",
    "print('state shape = ', obs.shape)\n",
    "print('action shape = ', action.shape)\n",
    "\n",
    "qvalue = qnet(obs, action)\n",
    "print('q value = ', qvalue.shape)\n",
    "\n",
    "actor_action, logprob, mean = actor.get_action(obs)\n",
    "print('actor action = ', actor_action.shape)\n",
    "print('logprob  = ', logprob.shape)\n",
    "print('mean  = ', mean.shape)\n",
    "\n",
    "\n",
    "# action, log_prob, entropy, value = test_agent.get_action_and_value(obs)\n",
    "\n",
    "# print('log prob shape = ', log_prob.shape)\n",
    "# print('entropy shape = ', entropy.shape)\n",
    "# print('value shape = ', value.shape)\n",
    "\n",
    "# del test_agent\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "def replay_buffer_usage(rb: ReplayBuffer):\n",
    "    return (rb.pos / rb.buffer_size) * 100 if rb.full == False else 100\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "    \n",
    "def pickle_load(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        obj = pickle.dump(file)\n",
    "    return obj\n",
    "\n",
    "def evaluate(agent, episodes = 10):\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "\n",
    "    return total_rewards\n",
    "\n",
    "def soft_update(net:nn.Module, other: nn.Module, tau = 0.005):\n",
    "    for this_param, other_param in zip(net.parameters(), other.parameters()):\n",
    "        this_param.data.copy_(tau * other_param.data + (1 - tau) * this_param.data)\n",
    "    \n",
    "def weight_update(net:nn.Module, other: nn.Module, tau = 0.005):\n",
    "    net.load_state_dict(other.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(\n",
    "    envs: gym.Env, actor:Actor, qf1:QNetwork, qf2:QNetwork, \n",
    "    qf1_target:QNetwork, qf2_target:QNetwork,\n",
    "    q_optimizer, actor_optimizer, device,\n",
    "    buffer_size = int(1e6), batch_size = 32,\n",
    "    total_timesteps = 1000, warmup_steps = 50,\n",
    "    policy_update_freq = 2, target_net_update_freq = 1, gamma = 0.99, \n",
    "    plot_update_freq = 10, label = 'test', plot_udpate_freq = 10, history = None):\n",
    "\n",
    "    SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "    FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "    HISTORY_PATH = os.path.join(SAVE_PATH, 'history.pickle')\n",
    "\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    print('save path = ', SAVE_PATH)\n",
    "\n",
    "    state_size = np.prod(envs.single_observation_space.shape)\n",
    "    action_size = np.prod(envs.single_action_space.shape)\n",
    "    n_envs = envs.observation_space.shape[0]\n",
    "\n",
    "    #default to float\n",
    "    envs.single_observation_space.dtype = np.float32\n",
    "\n",
    "    alpha = 0.2 # expected return and entropy trade-off coefficient \n",
    "\n",
    "    # target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()\n",
    "    # log_alpha = torch.zeros(1, requires_grad = True, device= device)\n",
    "    # alpha = log_alpha.exp().item()\n",
    "    # a_optimizer = torch.optim.Adam([log_alpha], lr = 1e-3)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(\n",
    "            buffer_size,\n",
    "            envs.single_observation_space, \n",
    "            envs.single_action_space, \n",
    "            device = device,\n",
    "            handle_timeout_termination=False,\n",
    "            optimize_memory_usage=True,\n",
    "            n_envs=n_envs\n",
    "    )\n",
    "\n",
    "    obs, _ = envs.reset()\n",
    "    avg_reward = 0\n",
    "    avg_reward = 0\n",
    "    best_score = -float('inf')\n",
    "    score_window = deque(maxlen = 100)\n",
    "    if history == None:\n",
    "        history = defaultdict(list)\n",
    "\n",
    "    loop = tqdm(range(total_timesteps))\n",
    "    episode_count = 0\n",
    "    updated_t = 0\n",
    "\n",
    "    #start training loop\n",
    "    for global_step in loop:\n",
    "        t = int(loop.format_dict['elapsed'])\n",
    "\n",
    "        #if still warming up, get random action\n",
    "        if global_step < warmup_steps:\n",
    "            actions = envs.action_space.sample()\n",
    "        \n",
    "        #else done warmup, get actions from actor\n",
    "        else:\n",
    "            actions, _,_ = actor.get_action(torch.tensor(obs).to(device).float())\n",
    "            actions = actions.detach().cpu().numpy()\n",
    "        \n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "        if 'final_info' in infos:\n",
    "            for info in infos['final_info']:\n",
    "                if info and 'episode' in info:\n",
    "                    test = True\n",
    "                    ep_return = info['episode']['r']\n",
    "                    score_window.append(ep_return)\n",
    "                    episode_count += 1\n",
    "\n",
    "                    avg_reward = np.mean(score_window)\n",
    "                    history['reward'].append(avg_reward)\n",
    "                    history['buffer_usage'].append(replay_buffer_usage(replay_buffer))\n",
    "\n",
    "                    #save model with new best score \n",
    "                    if avg_reward > best_score:\n",
    "                        best_score = avg_reward\n",
    "                        torch.save(actor, os.path.join(SAVE_PATH, 'actor.checkpoint.torch'))\n",
    "                        torch.save(qf1, os.path.join(SAVE_PATH, 'qf1.checkpoint.torch'))\n",
    "                        torch.save(qf2, os.path.join(SAVE_PATH, 'qf2.checkpoint.torch'))\n",
    "                        torch.save(qf1_target, os.path.join(SAVE_PATH, 'qf1_target.checkpoint.torch'))\n",
    "                        torch.save(qf2_target, os.path.join(SAVE_PATH, 'qf2_target.checkpoint.torch'))\n",
    "\n",
    "        real_next_obs = next_obs.copy()\n",
    "        for i, truncated in enumerate(truncations):\n",
    "            if truncated:\n",
    "                real_next_obs[i] = infos['final_observation'][i]\n",
    "        \n",
    "        replay_buffer.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        #optimize when done warming up \n",
    "        if global_step > warmup_steps:\n",
    "\n",
    "            data = replay_buffer.sample(batch_size)\n",
    "            b_next_obs = data.next_observations.to()\n",
    "            b_obs = data.observations\n",
    "            b_actions = data.actions\n",
    "            b_rewards = data.rewards\n",
    "            b_dones = data.dones\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_state_actions, next_state_log_probs, _ = actor.get_action(b_next_obs)\n",
    "                qf1_next_target = qf1_target(b_next_obs, next_state_actions)\n",
    "                qf2_next_target = qf2_target(b_next_obs, next_state_actions)\n",
    "                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_probs\n",
    "                next_q_value = b_rewards.flatten() + (1 - b_dones.flatten()) * gamma * min_qf_next_target.view(-1)\n",
    "            \n",
    "            qf1_a_values = qf1(b_obs, b_actions).view(-1)\n",
    "            qf2_a_values = qf2(b_obs, b_actions).view(-1)\n",
    "\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "\n",
    "            qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "            #step q_optimizer\n",
    "            q_optimizer.zero_grad()\n",
    "            qf_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            #TD3 update delayed\n",
    "            if global_step % policy_update_freq == 0:\n",
    "                for _ in range(policy_update_freq):\n",
    "\n",
    "                    pi, log_pi, _ = actor.get_action(b_obs)\n",
    "\n",
    "                    qf1_pi = qf1(b_obs, pi)\n",
    "                    qf2_pi = qf2(b_obs, pi)\n",
    "\n",
    "                    min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "                    actor_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "\n",
    "                    # with torch.no_grad():\n",
    "                    #     _, log_pi,_ = actor.get_action(b_obs)\n",
    "\n",
    "                    # alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()\n",
    "\n",
    "                    # a_optimizer.zero_grad()\n",
    "                    # alpha_loss.backward()\n",
    "                    # a_optimizer.step()\n",
    "\n",
    "                    # alpha = log_alpha.exp().item()\n",
    "            \n",
    "            if global_step % target_net_update_freq == 0:\n",
    "                soft_update(qf1_target, qf1)\n",
    "                soft_update(qf2_target, qf2)\n",
    "        \n",
    "        if t != updated_t and t % plot_update_freq == 0: \n",
    "            updated_t = t\n",
    "            loop.set_description(f\"avg_reward = {avg_reward:.2f}, best_score = {best_score}, episode_count = {episode_count}, buffer usage = {replay_buffer_usage(replay_buffer):.2f}\")\n",
    "            plot(history, save_path = FIG_SAVE_PATH)\n",
    "            pickle_dump(history, HISTORY_PATH)\n",
    "\n",
    "    plot(history, show = True, save_path = FIG_SAVE_PATH)\n",
    "    envs.reset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path =  /Volumes/SanDisk/NLP_RNN/Reinforcement Learning/deep rl/sac/output/InvertedDoublePendulum-v4/baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg_reward = 9305.85, best_score = 9348.7333984375, episode_count = 1481, buffer usage = 59.64: 100%|██████████| 100000/100000 [19:16<00:00, 86.45it/s]  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device= DEVICE\n",
    "q_lr = 1e-3\n",
    "actor_lr = 1e-3\n",
    "total_timesteps = int(1e5)\n",
    "warmup_steps = int(1e3)\n",
    "buffer_size = int(5e5)\n",
    "batch_size = 256\n",
    "fc_hidden_size = 256\n",
    "\n",
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'\n",
    "\n",
    "actor = Actor(envs, hidden_size=fc_hidden_size).to(device)\n",
    "qf1 = QNetwork(envs).to(device)\n",
    "qf2 = QNetwork(envs).to(device)\n",
    "qf1_target = QNetwork(envs).to(device)\n",
    "qf2_target = QNetwork(envs).to(device)\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "\n",
    "q_optimizer = torch.optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr = q_lr) \n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr = actor_lr) \n",
    "#NOTE: use fixec entropy method mentioned in Spin-up, improve this later with enforce method\n",
    "# https://spinningup.openai.com/en/latest/algorithms/sac.html\n",
    "\n",
    "\n",
    "tune(envs, actor, qf1, qf2, qf1_target, qf2_target, \n",
    "q_optimizer, actor_optimizer, device = device,\n",
    "total_timesteps= total_timesteps, warmup_steps=warmup_steps,\n",
    "buffer_size=buffer_size, batch_size=batch_size, label = 'baseline'\n",
    ")\n",
    "\n",
    "envs.close()\n",
    "\n",
    "\n",
    "# #check to make sure this is continous action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
