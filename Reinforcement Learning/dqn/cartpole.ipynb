{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Setting up matplotlib for live update the traiing progress\n",
    "import matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size = 16) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "        #init weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m,nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)  # Example: Xavier initialization\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "dqn = DQN(4,2,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "obs, info = env.reset()\n",
    "\n",
    "rewards = []\n",
    "for ep in tqdm(range(10)):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated: \n",
    "            break\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "sns.lineplot(y = rewards, x = list(range(len(rewards))))\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen = capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.memory.append(Transition(state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.memory): return None\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test memory\n",
    "memory = ReplayMemory(capacity=10)\n",
    "\n",
    "for _ in range(10):\n",
    "    state = random.randint(0,8)\n",
    "    action = random.randint(0,3)\n",
    "    reward = random.randint(-10, 10)\n",
    "    next_state = random.randint(0,8)\n",
    "    memory.push(state, action, reward, next_state)\n",
    "\n",
    "batch = memory.sample(5)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Epsilon Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, policy_net, env):\n",
    "    p = random.random()\n",
    "    #exploit\n",
    "    # state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "    assert torch.is_tensor(state), 'state in select action must be a tensor'\n",
    "    if p > epsilon: \n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1)\n",
    "    else:\n",
    "        return torch.tensor([env.action_space.sample()], dtype = torch.long)\n",
    "\n",
    "def epsilon_function(min_ep, max_ep, decay_ep, current_ep, total_eps, mode = 'linear'):\n",
    "    return  max(min_ep, max_ep * decay_ep ** current_ep)\n",
    "\n",
    "def plot_epsilon_decay(min_ep, max_ep, decay_ep, training_eps):\n",
    "    eps = []\n",
    "    for t in range(training_eps):\n",
    "        eps.append(epsilon_function(min_ep, max_ep, decay_ep, t, training_eps))\n",
    "    sns.lineplot(y = eps, x = list(range(len(eps))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "# memory\n",
    "memory = ReplayMemory(capacity=10)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "state, info = env.reset()\n",
    "\n",
    "observation_n = len(state)\n",
    "print('observation n = ', observation_n)\n",
    "action_n = env.action_space.n\n",
    "print('action n = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n)\n",
    "target_net = DQN(observation_n, action_n)\n",
    "\n",
    "for _ in range(100):\n",
    "    state = torch.rand((1,observation_n))\n",
    "    next_state = torch.rand((1,observation_n))\n",
    "    reward = torch.randint(-10,10, (1,1))\n",
    "    action = torch.randint(0,action_n, (1,1))\n",
    "\n",
    "    memory.push(state, action, reward, next_state)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "print(len(memory.memory))\n",
    "transitions = memory.sample(5)\n",
    "print(transitions)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(policy_net.parameters(), lr = 0.001)\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# GAMMA = 0.9\n",
    "\n",
    "# optimize(memory, 5, policy_net=policy_net, target_net=target_net, optimizer=optimizer, criterion=criterion, gamma=GAMMA, device = 'cpu')\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epsilon_decay(0.2, 1.0, 0.9991, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "\n",
    "observation_n = len(obs)\n",
    "action_n = env.action_space.n\n",
    "print('observation space = ', observation_n)\n",
    "print('action space = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "target_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "\n",
    "optimzer = torch.optim.Adam(policy_net.parameters(), lr = 5e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "memory = ReplayMemory(50000)\n",
    "\n",
    "batch_size = 5\n",
    "gamma = 0.99\n",
    "#sync target net with policy net\n",
    "c = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(policy_net, target_net, batch_size, gamma, memory):\n",
    "    transitions = memory.sample(batch_size=batch_size)\n",
    "\n",
    "    if transitions != None:\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype = torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(batch_size)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "        \n",
    "        expected_action_values =(gamma * next_state_values) + reward \n",
    "        expected_action_values = expected_action_values.unsqueeze(1)\n",
    "\n",
    "        loss = criterion(state_action_values, expected_action_values)\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output', 'cartpole')\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "def optimize(policy_net, target_net, batch_size, gamma, memory):\n",
    "\n",
    "    transitions = memory.sample(batch_size=batch_size)\n",
    "\n",
    "    if transitions != None:\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype = torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(batch_size)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "        \n",
    "        expected_action_values =(gamma * next_state_values) + reward \n",
    "        expected_action_values = expected_action_values.unsqueeze(1)\n",
    "\n",
    "        loss = criterion(state_action_values, expected_action_values)\n",
    "        optimzer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    return 0.0\n",
    "\n",
    "def tunev1(\n",
    "    policy_net, target_net, optimzer, criterion, env, memory,\n",
    "    gamma, batch_size,\n",
    "    min_ep, max_ep, decay_ep, training_eps, \n",
    "    c = 10\n",
    "):\n",
    "\n",
    "    loop = tqdm(range(training_eps))\n",
    "    history = defaultdict(list)\n",
    "    best_score = 0 \n",
    "    step = 0\n",
    "\n",
    "    for epoch in loop:\n",
    "\n",
    "        state, info = env.reset() \n",
    "        state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "        done = False\n",
    "        running_loss = 0\n",
    "        total_reward = 0\n",
    "        epsilon = epsilon_function(min_ep, max_ep, decay_ep, epoch, training_eps)\n",
    "        history['epsilon'].append(epsilon)\n",
    "\n",
    "        for t in count():\n",
    "            with torch.no_grad():\n",
    "                action =  select_action(state, 0.1, policy_net, env)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            reward = torch.tensor([reward])\n",
    "\n",
    "            if terminated or truncated: \n",
    "                next_state = None\n",
    "                done = True\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype = torch.float32).unsqueeze(0)\n",
    "            \n",
    "            action = action.unsqueeze(0)\n",
    "            #push transition to memory\n",
    "            memory.push(state, action, reward, next_state)\n",
    "            \n",
    "            #state is next state\n",
    "            state = next_state \n",
    "\n",
    "            transitions = memory.sample(batch_size=batch_size)\n",
    "\n",
    "            if transitions != None:\n",
    "            \n",
    "                batch = Transition(*zip(*transitions))\n",
    "\n",
    "                non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype = torch.bool)\n",
    "                non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "                state_batch = torch.cat(batch.state)\n",
    "                action_batch = torch.cat(batch.action)\n",
    "                reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "                state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "                next_state_values = torch.zeros(batch_size)\n",
    "                next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "                expected_action_values =(gamma * next_state_values) + reward \n",
    "                expected_action_values = expected_action_values.unsqueeze(1)\n",
    "\n",
    "                loss = criterion(state_action_values, expected_action_values)\n",
    "                optimzer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimzer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            step += 1\n",
    "            if step % c == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "            if best_score <= total_reward:\n",
    "                torch.save(policy_net, 'best_policy_net.torch')\n",
    "                torch.save(target_net, 'best_target_net.torch')\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "\n",
    "        loop.set_description(f'ep = {epoch}\\t,epsilon={epsilon:.2f}\\t,loss = {running_loss:.2f}\\t,rewards = {total_reward}')\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['loss'].append(running_loss)\n",
    "\n",
    "    torch.save(policy_net, 'final_policy_net.torch')\n",
    "    torch.save(target_net, 'final_target_net.torch')\n",
    "\n",
    " \n",
    "\n",
    "def plot(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "    sns.lineplot(y = history['rewards'], x = list(range(len(history['rewards']))), ax = ax1, color = 'green')\n",
    "    ax1.set_title('rewards')\n",
    "    sns.lineplot(y = history['loss'], x = list(range(len(history['loss']))), ax = ax2)\n",
    "    ax2.set_title('loss')\n",
    "    plt.show()\n",
    "    sns.lineplot(y = history['epsilon'], x = list(range(len(history['epsilon']))))\n",
    "    plt.title('epsilon')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space =  4\n",
      "action space =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "ep = 151\t,epsilon=0.87\t,loss = 67178.40\t,rewards = 500.0:   8%|▊         | 152/2000 [00:21<04:24,  6.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m c \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mtarget_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_score \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m total_reward:\n\u001b[1;32m     69\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(policy_net, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_policy_net.torch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:2139\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2132\u001b[0m         out \u001b[39m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2133\u001b[0m         \u001b[39massert\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, (\n\u001b[1;32m   2134\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2135\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2136\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mit should be done inplace.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2137\u001b[0m         )\n\u001b[0;32m-> 2139\u001b[0m load(\u001b[39mself\u001b[39;49m, state_dict)\n\u001b[1;32m   2140\u001b[0m \u001b[39mdel\u001b[39;00m load\n\u001b[1;32m   2142\u001b[0m \u001b[39mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:2126\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2124\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2125\u001b[0m         child_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 2126\u001b[0m         child_state_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m local_state_dict\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m   2127\u001b[0m         load(child, child_state_dict, child_prefix)\n\u001b[1;32m   2129\u001b[0m \u001b[39m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:2126\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2124\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2125\u001b[0m         child_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 2126\u001b[0m         child_state_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m local_state_dict\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m   2127\u001b[0m         load(child, child_state_dict, child_prefix)\n\u001b[1;32m   2129\u001b[0m \u001b[39m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "\n",
    "observation_n = len(obs)\n",
    "action_n = env.action_space.n\n",
    "print('observation space = ', observation_n)\n",
    "print('action space = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "target_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "\n",
    "optimzer = torch.optim.Adam(policy_net.parameters(), lr = 5e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "memory = ReplayMemory(50000)\n",
    "\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "#sync target net with policy net\n",
    "c = 10 \n",
    "step = 0\n",
    "training_epochs = 2000\n",
    "min_ep = 0.2\n",
    "max_ep = 1.0\n",
    "decay_ep = 0.9991\n",
    "\n",
    "loop = tqdm(range(training_epochs))\n",
    "history = defaultdict(list)\n",
    "best_score = 0 \n",
    "\n",
    "for epoch in loop:\n",
    "\n",
    "    state, info = env.reset() \n",
    "    state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "    done = False\n",
    "    running_loss = 0\n",
    "    total_reward = 0\n",
    "    epsilon = epsilon_function(min_ep, max_ep, decay_ep, epoch, training_epochs)\n",
    "    history['epsilon'].append(epsilon)\n",
    "\n",
    "    for t in count():\n",
    "        with torch.no_grad():\n",
    "            action =  select_action(state, 0.1, policy_net, env)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward])\n",
    "\n",
    "        if terminated or truncated: \n",
    "            next_state = None\n",
    "            done = True\n",
    "        else:\n",
    "            next_state = torch.tensor(next_state, dtype = torch.float32).unsqueeze(0)\n",
    "        \n",
    "        action = action.unsqueeze(0)\n",
    "        #push transition to memory\n",
    "        memory.push(state, action, reward, next_state)\n",
    "        \n",
    "        #state is next state\n",
    "        state = next_state \n",
    "\n",
    "        running_loss += optimize(policy_net, target_net, batch_size, gamma, memory)\n",
    "        \n",
    "        step += 1\n",
    "        if step % c == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        if best_score <= total_reward:\n",
    "            torch.save(policy_net, 'best_policy_net.torch')\n",
    "            torch.save(target_net, 'best_target_net.torch')\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    loop.set_description(f'ep = {epoch}\\t,epsilon={epsilon:.2f}\\t,loss = {running_loss:.2f}\\t,rewards = {total_reward}')\n",
    "    history['rewards'].append(total_reward)\n",
    "    history['loss'].append(running_loss)\n",
    "\n",
    "torch.save(policy_net, 'final_policy_net.torch')\n",
    "torch.save(target_net, 'final_target_net.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.save(policy_net, 'final_policy_net.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.lineplot(y = history['rewards'], x = list(range(len(history['rewards']))), ax = ax1, color = 'green')\n",
    "ax1.set_title('rewards')\n",
    "sns.lineplot(y = history['loss'], x = list(range(len(history['loss']))), ax = ax2)\n",
    "ax2.set_title('loss')\n",
    "plt.show()\n",
    "sns.lineplot(y = history['epsilon'], x = list(range(len(history['epsilon']))))\n",
    "plt.title('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space =  4\n",
      "action space =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "ep = 899\t,epsilon=0.45\t,loss = 9810.09\t,rewards = 500.0:  45%|████▌     | 900/2000 [06:17<07:40,  2.39it/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     24\u001b[0m memory \u001b[38;5;241m=\u001b[39m ReplayMemory(\u001b[38;5;241m50000\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m policy_net, target_net, history \u001b[38;5;241m=\u001b[39m \u001b[43mtunev1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_ep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMIN_EP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_ep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_EP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecay_ep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDECAY_EP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_eps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTRAINING_EPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m plot(history)\n",
      "Cell \u001b[0;32mIn[43], line 102\u001b[0m, in \u001b[0;36mtunev1\u001b[0;34m(policy_net, target_net, optimzer, criterion, env, memory, gamma, batch_size, min_ep, max_ep, decay_ep, training_eps, c)\u001b[0m\n\u001b[1;32m    100\u001b[0m     optimzer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    101\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 102\u001b[0m     \u001b[43moptimzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    106\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "LR = 5e-4\n",
    "C = 10 \n",
    "MIN_EP = 0.2\n",
    "MAX_EP = 1.0\n",
    "DECAY_EP = 0.9991\n",
    "TRAINING_EPS = 2000\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "\n",
    "observation_n = len(obs)\n",
    "action_n = env.action_space.n\n",
    "print('observation space = ', observation_n)\n",
    "print('action space = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "target_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr = 5e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "memory = ReplayMemory(50000)\n",
    "\n",
    "policy_net, target_net, history = tunev1(\n",
    "    policy_net, target_net, optimizer, criterion, env, memory,\n",
    "    gamma = GAMMA, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    min_ep=MIN_EP,\n",
    "    max_ep = MAX_EP,\n",
    "    decay_ep=DECAY_EP,\n",
    "    training_eps = TRAINING_EPS,\n",
    "    c = C\n",
    ")\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the agent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, 'solved_policy_net.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "ep = 99, reward = 500.0: 100%|██████████| 100/100 [17:38<00:00, 10.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nv = gym.make('CartPole-v1')\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "rewards = []\n",
    "\n",
    "# net = torch.load('best_target_net.torch')\n",
    "net= policy_net\n",
    "\n",
    "loop = tqdm(range(100))\n",
    "for epoch in loop:\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0\n",
    "    for step in count():\n",
    "        action = select_action(state, 0.0, net, env)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        next_state = torch.tensor(next_state, dtype = torch.float32).unsqueeze(0)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    loop.set_description(f'ep = {epoch}, reward = {total_reward}')\n",
    "    rewards.append(total_reward)\n",
    "    \n",
    "sns.lineplot(y = total_reward, x= list(range(len(rewards))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-4\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_CAP = 50000\n",
    "TRAINING_EPOCHS = 2000 \n",
    "MIN_EP = 0.2\n",
    "MAX_EP = 1.0\n",
    "DECAY_EP = 0.9991\n",
    "C = 10\n",
    "HIDDEN_SIZE = 16\n",
    "\n",
    "#===EP_DECAY===\n",
    "plot_epsilon_decay(MIN_EP, MAX_EP, DECAY_EP, TRAINING_EPOCHS)\n",
    "\n",
    "#===CONFIG====\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "\n",
    "observation_n = len(obs)\n",
    "action_n = env.action_space.n\n",
    "print('observation space = ', observation_n)\n",
    "print('action space = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n, hidden_size = HIDDEN_SIZE)\n",
    "target_net = DQN(observation_n, action_n, hidden_size = HIDDEN_SIZE)\n",
    "memory = ReplayMemory(MEMORY_CAP)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr = LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "policy_net, target_net, history = tune(\n",
    "    policy_net, target_net, optimizer, criterion, env, memory,\n",
    "    gamma = GAMMA, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    training_epochs= TRAINING_EPOCHS, \n",
    "    min_ep = MIN_EP, \n",
    "    max_ep = MAX_EP, \n",
    "    decay_ep = DECAY_EP, \n",
    "    c= C)\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.lineplot(y = history['rewards'], x = list(range(len(history['rewards']))), ax = ax1, color = 'green')\n",
    "ax1.set_title('rewards')\n",
    "sns.lineplot(y = history['loss'], x = list(range(len(history['loss']))), ax = ax2)\n",
    "ax2.set_title('loss')\n",
    "plt.show()\n",
    "sns.lineplot(y = history['epsilon'], x = list(range(len(history['epsilon']))))\n",
    "plt.title('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "rewards = []\n",
    "\n",
    "net = torch.load('best_target_net.torch')\n",
    "\n",
    "loop = tqdm(range(100))\n",
    "for epoch in loop:\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0\n",
    "    for step in count():\n",
    "        action = select_action(state, 0.0, net, env)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        next_state = torch.tensor(next_state, dtype = torch.float32).unsqueeze(0)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    loop.set_description(f'ep = {epoch}, reward = {total_reward}')\n",
    "    rewards.append(total_reward)\n",
    "    \n",
    "sns.lineplot(y = total_reward, x= list(range(len(rewards))))\n",
    "\n",
    "\n",
    "# for epoch in loop:\n",
    "\n",
    "#     state, info = env.reset() \n",
    "#     state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "#     done = False\n",
    "#     running_loss = 0\n",
    "#     total_reward = 0\n",
    "#     epsilon = epsilon_function(min_ep, max_ep, decay_ep, epoch, 1000)\n",
    "#     history['epsilon'].append(epsilon)\n",
    "\n",
    "#     for t in count():\n",
    "#         with torch.no_grad():\n",
    "#             action =  select_action(state, 0.1, policy_net, env)\n",
    "        \n",
    "#         next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "#         total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
