{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Setting up matplotlib for live update the traiing progress\n",
    "import matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size = 16) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "        #init weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m,nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)  # Example: Xavier initialization\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "dqn = DQN(4,2,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "obs, info = env.reset()\n",
    "\n",
    "rewards = []\n",
    "for ep in tqdm(range(10)):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated: \n",
    "            break\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "sns.lineplot(y = rewards, x = list(range(len(rewards))))\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state'))\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen = capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.memory.append(Transition(state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > len(self.memory): return None\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test memory\n",
    "memory = ReplayMemory(capacity=10)\n",
    "\n",
    "for _ in range(10):\n",
    "    state = random.randint(0,8)\n",
    "    action = random.randint(0,3)\n",
    "    reward = random.randint(-10, 10)\n",
    "    next_state = random.randint(0,8)\n",
    "    memory.push(state, action, reward, next_state)\n",
    "\n",
    "batch = memory.sample(5)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Epsilon Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, policy_net, env):\n",
    "    p = random.random()\n",
    "    #exploit\n",
    "    # state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "    assert torch.is_tensor(state), 'state in select action must be a tensor'\n",
    "    if p > epsilon: \n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1)\n",
    "    else:\n",
    "        return torch.tensor([env.action_space.sample()], dtype = torch.long)\n",
    "\n",
    "def epsilon_function(min_ep, max_ep, decay_ep, current_ep, total_eps, mode = 'linear'):\n",
    "    return  max(min_ep, max_ep * decay_ep ** current_ep)\n",
    "\n",
    "def plot_epsilon_decay(min_ep, max_ep, decay_ep, training_eps):\n",
    "    eps = []\n",
    "    for t in range(training_eps):\n",
    "        eps.append(epsilon_function(min_ep, max_ep, decay_ep, t, training_eps))\n",
    "    sns.lineplot(y = eps, x = list(range(len(eps))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "# memory\n",
    "memory = ReplayMemory(capacity=10)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "state, info = env.reset()\n",
    "\n",
    "observation_n = len(state)\n",
    "print('observation n = ', observation_n)\n",
    "action_n = env.action_space.n\n",
    "print('action n = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n)\n",
    "target_net = DQN(observation_n, action_n)\n",
    "\n",
    "for _ in range(100):\n",
    "    state = torch.rand((1,observation_n))\n",
    "    next_state = torch.rand((1,observation_n))\n",
    "    reward = torch.randint(-10,10, (1,1))\n",
    "    action = torch.randint(0,action_n, (1,1))\n",
    "\n",
    "    memory.push(state, action, reward, next_state)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "print(len(memory.memory))\n",
    "transitions = memory.sample(5)\n",
    "print(transitions)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(policy_net.parameters(), lr = 0.001)\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# GAMMA = 0.9\n",
    "\n",
    "# optimize(memory, 5, policy_net=policy_net, target_net=target_net, optimizer=optimizer, criterion=criterion, gamma=GAMMA, device = 'cpu')\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epsilon_decay(0.2, 1.0, 0.9991, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "\n",
    "observation_n = len(obs)\n",
    "action_n = env.action_space.n\n",
    "print('observation space = ', observation_n)\n",
    "print('action space = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "target_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "\n",
    "optimzer = torch.optim.Adam(policy_net.parameters(), lr = 5e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "memory = ReplayMemory(50000)\n",
    "\n",
    "batch_size = 5\n",
    "gamma = 0.99\n",
    "#sync target net with policy net\n",
    "c = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "\n",
    "observation_n = len(obs)\n",
    "action_n = env.action_space.n\n",
    "print('observation space = ', observation_n)\n",
    "print('action space = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "target_net = DQN(observation_n, action_n, hidden_size = 16)\n",
    "\n",
    "optimzer = torch.optim.Adam(policy_net.parameters(), lr = 5e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "memory = ReplayMemory(50000)\n",
    "\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "#sync target net with policy net\n",
    "c = 10 \n",
    "step = 0\n",
    "training_epochs = 2000\n",
    "min_ep = 0.2\n",
    "max_ep = 1.0\n",
    "decay_ep = 0.9991\n",
    "\n",
    "loop = tqdm(range(training_epochs))\n",
    "history = defaultdict(list)\n",
    "best_score = 0 \n",
    "\n",
    "for epoch in loop:\n",
    "\n",
    "    state, info = env.reset() \n",
    "    state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "    done = False\n",
    "    running_loss = 0\n",
    "    total_reward = 0\n",
    "    epsilon = epsilon_function(min_ep, max_ep, decay_ep, epoch, 1000)\n",
    "    history['epsilon'].append(epsilon)\n",
    "\n",
    "    for t in count():\n",
    "        with torch.no_grad():\n",
    "            action =  select_action(state, 0.1, policy_net, env)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward])\n",
    "\n",
    "        if terminated or truncated: \n",
    "            next_state = None\n",
    "            done = True\n",
    "        else:\n",
    "            next_state = torch.tensor(next_state, dtype = torch.float32).unsqueeze(0)\n",
    "        \n",
    "        action = action.unsqueeze(0)\n",
    "        #push transition to memory\n",
    "        memory.push(state, action, reward, next_state)\n",
    "        \n",
    "        #state is next state\n",
    "        state = next_state \n",
    "\n",
    "        transitions = memory.sample(batch_size=batch_size)\n",
    "\n",
    "        if transitions != None:\n",
    "\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype = torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\n",
    "            state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "            next_state_values = torch.zeros(batch_size)\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "            \n",
    "            expected_action_values =(gamma * next_state_values) + reward \n",
    "            expected_action_values = expected_action_values.unsqueeze(1)\n",
    "\n",
    "            loss = criterion(state_action_values, expected_action_values)\n",
    "            optimzer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimzer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        step += 1\n",
    "        if step % c == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        if best_score <= total_reward:\n",
    "            torch.save(policy_net, 'best_policy_net.torch')\n",
    "            torch.save(target_net, 'best_target_net.torch')\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "    loop.set_description(f'ep = {epoch}\\t,epsilon={epsilon:.2f}\\t,loss = {running_loss:.2f}\\t,rewards = {total_reward}')\n",
    "    history['rewards'].append(total_reward)\n",
    "    history['loss'].append(running_loss)\n",
    "\n",
    "torch.save(policy_net, 'final_policy_net.torch')\n",
    "torch.save(target_net, 'final_target_net.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.save(policy_net, 'final_policy_net.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.lineplot(y = history['rewards'], x = list(range(len(history['rewards']))), ax = ax1, color = 'green')\n",
    "ax1.set_title('rewards')\n",
    "sns.lineplot(y = history['loss'], x = list(range(len(history['loss']))), ax = ax2)\n",
    "ax2.set_title('loss')\n",
    "plt.show()\n",
    "sns.lineplot(y = history['epsilon'], x = list(range(len(history['epsilon']))))\n",
    "plt.title('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "rewards = []\n",
    "\n",
    "net = torch.load('best_target_net.torch')\n",
    "\n",
    "loop = tqdm(range(100))\n",
    "for epoch in loop:\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0\n",
    "    for step in count():\n",
    "        action = select_action(state, 0.0, net, env)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        next_state = torch.tensor(next_state, dtype = torch.float32).unsqueeze(0)\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    loop.set_description(f'ep = {epoch}, reward = {total_reward}')\n",
    "    rewards.append(total_reward)\n",
    "    \n",
    "sns.lineplot(y = total_reward, x= list(range(len(rewards))))\n",
    "\n",
    "\n",
    "# for epoch in loop:\n",
    "\n",
    "#     state, info = env.reset() \n",
    "#     state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "#     done = False\n",
    "#     running_loss = 0\n",
    "#     total_reward = 0\n",
    "#     epsilon = epsilon_function(min_ep, max_ep, decay_ep, epoch, 1000)\n",
    "#     history['epsilon'].append(epsilon)\n",
    "\n",
    "#     for t in count():\n",
    "#         with torch.no_grad():\n",
    "#             action =  select_action(state, 0.1, policy_net, env)\n",
    "        \n",
    "#         next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "#         total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-4\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_CAP = 50000\n",
    "TRAINING_EPOCHS = 2000 \n",
    "MIN_EP = 0.2\n",
    "MAX_EP = 1.0\n",
    "DECAY_EP = 0.9991\n",
    "C = 10\n",
    "HIDDEN_SIZE = 16\n",
    "\n",
    "#===EP_DECAY===\n",
    "plot_epsilon_decay(MIN_EP, MAX_EP, DECAY_EP, TRAINING_EPOCHS)\n",
    "\n",
    "#===CONFIG====\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "obs, info = env.reset()\n",
    "\n",
    "observation_n = len(obs)\n",
    "action_n = env.action_space.n\n",
    "print('observation space = ', observation_n)\n",
    "print('action space = ', action_n)\n",
    "\n",
    "policy_net = DQN(observation_n, action_n, hidden_size = HIDDEN_SIZE)\n",
    "target_net = DQN(observation_n, action_n, hidden_size = HIDDEN_SIZE)\n",
    "memory = ReplayMemory(MEMORY_CAP)\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr = LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "policy_net, target_net, history = tune(\n",
    "    policy_net, target_net, optimizer, criterion, env, memory,\n",
    "    gamma = GAMMA, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    training_epochs= TRAINING_EPOCHS, \n",
    "    min_ep = MIN_EP, \n",
    "    max_ep = MAX_EP, \n",
    "    decay_ep = DECAY_EP, \n",
    "    c= C)\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.lineplot(y = history['rewards'], x = list(range(len(history['rewards']))), ax = ax1, color = 'green')\n",
    "ax1.set_title('rewards')\n",
    "sns.lineplot(y = history['loss'], x = list(range(len(history['loss']))), ax = ax2)\n",
    "ax2.set_title('loss')\n",
    "plt.show()\n",
    "sns.lineplot(y = history['epsilon'], x = list(range(len(history['epsilon']))))\n",
    "plt.title('epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "rewards = []\n",
    "\n",
    "net = torch.load('best_target_net.torch')\n",
    "\n",
    "loop = tqdm(range(100))\n",
    "for epoch in loop:\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "\n",
    "    total_reward = 0\n",
    "    for step in count():\n",
    "        action = select_action(state, 0.0, net, env)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        next_state = torch.tensor(next_state, dtype = torch.float32).unsqueeze(0)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    loop.set_description(f'ep = {epoch}, reward = {total_reward}')\n",
    "    rewards.append(total_reward)\n",
    "    \n",
    "sns.lineplot(y = total_reward, x= list(range(len(rewards))))\n",
    "\n",
    "\n",
    "# for epoch in loop:\n",
    "\n",
    "#     state, info = env.reset() \n",
    "#     state = torch.tensor(state, dtype = torch.float32).unsqueeze(0)\n",
    "#     done = False\n",
    "#     running_loss = 0\n",
    "#     total_reward = 0\n",
    "#     epsilon = epsilon_function(min_ep, max_ep, decay_ep, epoch, 1000)\n",
    "#     history['epsilon'].append(epsilon)\n",
    "\n",
    "#     for t in count():\n",
    "#         with torch.no_grad():\n",
    "#             action =  select_action(state, 0.1, policy_net, env)\n",
    "        \n",
    "#         next_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "#         total_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
