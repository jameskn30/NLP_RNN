{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "Pytorch tutorial: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import math\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "\n",
    "#Action\n",
    "#0: move left\n",
    "#1: move right\n",
    "\n",
    "#Observation       \n",
    "#Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity \n",
    "\n",
    "#Goal: is to keep the pole upright as long as possible. There's a no terminal state for CartPole\n",
    "\n",
    "#Rewards\n",
    "#+1 for every step the pole is upright position\n",
    "obs = env.reset()\n",
    "MAX_TIMESTEP = 1000\n",
    "timestep = 0\n",
    "terminated = False\n",
    "history = []\n",
    "\n",
    "while timestep < MAX_TIMESTEP and not terminated:\n",
    "    random_action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "    env.render()\n",
    "    timestep += 1\n",
    "    history.append(reward)\n",
    "\n",
    "env.close()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "\n",
    "stores the (s,a,r,s') values into a deque with a fixed length (drop furthest element when capacity is reached)\n",
    "Why?\n",
    "\n",
    "- breaking correlated sequential data. E.g: actions take earlier will greatly influence the current actions, result in biased learning\n",
    "- By randomly sampling transitions from this memory during training, the agent learns from a diverse and uncorrelated dataset. \n",
    "- allows the agent to revisit and learn from past experiences multiple times. This is particularly beneficial when dealing with sparse rewards, where rewards might be infrequent or delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(x=1, y=2, z=3)\n"
     ]
    }
   ],
   "source": [
    "Tuple = namedtuple(\"Tuple\", ('x', 'y', 'z'))\n",
    "a = Tuple(1,2,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory len =  1\n"
     ]
    }
   ],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity) -> None:\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        return random.sample(self.memory, batchsize)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "#test it\n",
    "rep_mem = ReplayMemory(capacity=10)\n",
    "\n",
    "rep_mem.push('s1', 'action 1', 's2', 35)\n",
    "\n",
    "print('memory len = ', len(rep_mem))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "### Objective\n",
    "train a policy that tries to maximize the discounted, cumulative reward\n",
    "\n",
    "### Gamma, discounted factor\n",
    "lower Î³ makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about.\n",
    "\n",
    "### minimize the TD equation using Huber Loss. \n",
    "The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large - this makes it more robust to outliers when the estimates of Q are very noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        #what's the purpose of this?\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.LazyLinear(128)\n",
    "        self.layer3 = nn.LazyLinear(n_actions)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.layer1(X))\n",
    "        X = F.relu(self.layer2(X))\n",
    "        return self.layer3(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape =  torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN(4, 4)\n",
    "X = torch.rand(4,4)\n",
    "\n",
    "y = dqn(X)\n",
    "print(\"y shape = \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[99,  6, 92, 88],\n",
      "        [70, 65, 20, 26],\n",
      "        [53, 45, 16, 98],\n",
      "        [28, 29, 59, 16]])\n",
      "tensor([0, 0, 3, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1]' is invalid for input of size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m maxaction \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(maxaction)\n\u001b[0;32m----> 6\u001b[0m maxaction \u001b[38;5;241m=\u001b[39m maxaction\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(maxaction)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1]' is invalid for input of size 4"
     ]
    }
   ],
   "source": [
    "a = torch.randint(1,100, (4,4))\n",
    "print(a)\n",
    "maxaction = a.max(1).indices\n",
    "print(maxaction)\n",
    "\n",
    "maxaction = maxaction.view(1,1)\n",
    "\n",
    "print(maxaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0381, 0.9885, 0.9034, 0.9378],\n",
      "        [0.9217, 0.7292, 0.4444, 0.5068],\n",
      "        [0.1608, 0.1414, 0.3183, 0.4310],\n",
      "        [0.6017, 0.6649, 0.8968, 0.8986],\n",
      "        [0.9200, 0.8781, 0.7053, 0.6742],\n",
      "        [0.5678, 0.3205, 0.0467, 0.3574],\n",
      "        [0.2037, 0.2012, 0.8730, 0.9202],\n",
      "        [0.2821, 0.9785, 0.4867, 0.5124],\n",
      "        [0.5163, 0.3850, 0.4200, 0.7734],\n",
      "        [0.0230, 0.2200, 0.0613, 0.9619]])\n",
      "torch.Size([10, 4])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "gather() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (int dim, Tensor index, *, bool sparse_grad)\n * (name dim, Tensor index, *, bool sparse_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m actions_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m4\u001b[39m,(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mgather(actions_idx)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n",
      "\u001b[0;31mTypeError\u001b[0m: gather() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (int dim, Tensor index, *, bool sparse_grad)\n * (name dim, Tensor index, *, bool sparse_grad)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(10,4)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "actions_idx = torch.randint(0,4,(10,4))\n",
    "a = a.gather(actions_idx)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non final mask shape =  torch.Size([128])\n",
      "non final next states shape =  torch.Size([128, 4])\n",
      "state batch shape =  torch.Size([128, 4])\n",
      "action batch shape =  torch.Size([128, 1])\n",
      "reward state batch shape =  torch.Size([128, 1])\n",
      "\n",
      "state action values shape =  torch.Size([128, 1])\n",
      "next state values shape =  torch.Size([128])\n",
      "expected state actions values shape =  torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "#Optimization\n",
    "def optimize_model(memory, policy_net, target_net, \\\n",
    "    batch_size, gamma, device, debug = False):\n",
    "    #can't optimize yet\n",
    "    if len(memory) < batch_size: return\n",
    "\n",
    "    transitions = memory.sample(batch_size)\n",
    "    #this converts an array of transitions into transitions of action array, state arrays, ... \n",
    "    #Transition(state = (1,2,3...), action=(4,5,6,...), next_state=(7,8,9,...))\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    #NOTE: Make a mask of non-final states. If non-final: True, if final state: True\n",
    "    #return a tensor of ([True, True, ..., False, True,...])\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device = device)\n",
    "    test = [s for s in batch.next_state if s is not None]\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    #get state_batch, action_batch, reward_batch, \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    if debug:\n",
    "        print('non final mask shape = ', non_final_mask.shape)\n",
    "        print('non final next states shape = ', non_final_next_states.shape)\n",
    "        print('state batch shape = ', state_batch.shape)\n",
    "        print('action batch shape = ', action_batch.shape)\n",
    "        print('reward state batch shape = ', reward_batch.shape)\n",
    "        print()\n",
    "\n",
    "    #Select actions that would've been taken by the model\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    print('state action values shape = ', state_action_values.shape)\n",
    "\n",
    "    #we want to mask the terminal states to 0s\n",
    "    next_state_values = torch.zeros(batch_size, device = device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    print('next state values shape = ', next_state_values.shape)\n",
    "    print('expected state actions values shape = ', expected_state_action_values.shape)\n",
    "\n",
    "    #Compute Huber Loss\n",
    "    # criterion = nn.SmoothL1Loss()\n",
    "    # loss =criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "#test optimze model function\n",
    "\n",
    "policy_net = DQN(4,4).to(DEVICE)\n",
    "target_net = DQN(4,4).to(DEVICE)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "#test\n",
    "memory = ReplayMemory(capacity = 10000)\n",
    "memory.clear()\n",
    "\n",
    "for _ in range(1000):\n",
    "    state = torch.tensor([random.randint(0,3) for _ in range(4)], dtype = torch.float32).unsqueeze(0)\n",
    "    action = torch.tensor([random.randint(0,3)]).unsqueeze(0)\n",
    "    next_state = torch.tensor([random.randint(0,3) for _ in range(4)], dtype = torch.float32).unsqueeze(0)\n",
    "    reward = torch.tensor([random.randint(0,3)], dtype = torch.float32).unsqueeze(0)\n",
    "    memory.push(state, action, next_state, reward)\n",
    "\n",
    "optimize_model(memory, policy_net, target_net, BATCH_SIZE, GAMMA, device = DEVICE, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "#why use policy net?\n",
    "policy_net = DQN(n_observations, n_actions).to(DEVICE)\n",
    "#why use target net?\n",
    "target_net = DQN(n_observations, n_actions).to(DEVICE)\n",
    "\n",
    "#copy policy net weights into target net\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimzer = torch.optim.Adam(policy_net.parameters(), lr = LR, amsgrad = True)\n",
    "memory = ReplayMemory(capacity = 10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init state =  tensor([[-0.0024, -0.0013, -0.0259,  0.0245]])\n",
      "reward  tensor([1.])\n",
      "reward  tensor([1.])\n",
      "reward  tensor([1.])\n",
      "reward  tensor([1.])\n",
      "reward  tensor([1.])\n",
      "reward  tensor([1.])\n",
      "deque([Transition(state=tensor([[-0.0024, -0.0013, -0.0259,  0.0245]]), action=tensor([[1]]), next_state=tensor([[-0.0024,  0.1942, -0.0255, -0.2762]]), reward=tensor([1.])), Transition(state=tensor([[-0.0024,  0.1942, -0.0255, -0.2762]]), action=tensor([[0]]), next_state=tensor([[ 0.0015, -0.0006, -0.0310,  0.0083]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0015, -0.0006, -0.0310,  0.0083]]), action=tensor([[0]]), next_state=tensor([[ 0.0014, -0.1952, -0.0308,  0.2911]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0014, -0.1952, -0.0308,  0.2911]]), action=tensor([[1]]), next_state=tensor([[-0.0025,  0.0003, -0.0250, -0.0112]]), reward=tensor([1.])), Transition(state=tensor([[-0.0025,  0.0003, -0.0250, -0.0112]]), action=tensor([[0]]), next_state=tensor([[-0.0025, -0.1945, -0.0252,  0.2735]]), reward=tensor([1.])), Transition(state=tensor([[-0.0025, -0.1945, -0.0252,  0.2735]]), action=tensor([[1]]), next_state=tensor([[-0.0063,  0.0010, -0.0197, -0.0270]]), reward=tensor([1.]))], maxlen=10000)\n"
     ]
    }
   ],
   "source": [
    "#explore and exploit epsilon\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    #epsilon decay rate\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        #best action  \n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indicies.view(1,1)\n",
    "    else:\n",
    "        #random action\n",
    "        return torch.tensor([[env.action_space.sample()]], device = DEVICE, dtype = torch.long) \n",
    "\n",
    "memory = ReplayMemory(capacity = 10000)\n",
    "memory.clear()\n",
    " \n",
    "#Training loop\n",
    "if DEVICE == 'cuda':\n",
    "    num_episodes = 10000\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype = torch.float32, device = DEVICE).unsqueeze(0)\n",
    "    print('init state = ' , state)\n",
    "\n",
    "    for t in count():\n",
    "\n",
    "        action = select_action(state)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        # print(obs)\n",
    "        reward = torch.tensor([reward])\n",
    "        print('reward ', reward)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype = torch.float32, device = DEVICE).unsqueeze(0)\n",
    "        \n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if t == 5:\n",
    "            #debug\n",
    "            print(memory.memory)\n",
    "            break\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply DQN on 3 other different environments:\n",
    "\n",
    "1. Mountain Car\n",
    "2. Snake Game\n",
    "3. Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
