{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "https://github.com/vwxyzjn/cleanrl?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': \"InvertedPendulum-v4\"\n",
    "}\n",
    "NUM_ENVS = 3\n",
    "SEED = 1\n",
    "LR = 1e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output', ENV_ARGS['id'])\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gamma, **env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.ClipAction(env)\n",
    "    # env = gym.wrappers.NormalizeObservation(env)\n",
    "    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    env = gym.wrappers.NormalizeReward(env, gamma = gamma)\n",
    "    env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.Env, hidden_size: int = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_shape = np.array(envs.single_observation_space.shape).prod()\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            #NOTE: why use tanh here? \n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(layer = nn.Linear(hidden_size, self.action_shape), std = 0.01),\n",
    "        )\n",
    "\n",
    "        #shape = (1, state_shape)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, self.action_shape, dtype=torch.float))\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(nn.Linear(hidden_size, 1), std = 1.0),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action = None):\n",
    "        '''\n",
    "        @params:\n",
    "            x: torch.tensor observation, shape = (N, observation size)\n",
    "            action: torch.tensor action\n",
    "        @returns:\n",
    "            action: torch.tensor, shape = (N, action size)\n",
    "            log_prob: torch.tensor, shape = (N,)\n",
    "            entropy: torch.tensor, shape = (N,)\n",
    "            value: torch.tensor, shape = (N,)\n",
    "        '''\n",
    "        action_mean = self.actor_mean(x)\n",
    "        #make action logstd the shape[0] with mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        #exponential trick to remove log\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        probs = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample() \n",
    "        \n",
    "        #get value from critic\n",
    "        value = self.get_value(x)\n",
    "        log_prob = probs.log_prob(action).sum(1)\n",
    "        #entropy for regularization\n",
    "        entropy = probs.entropy().sum(1)\n",
    "        \n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape =  torch.Size([3, 4])\n",
      "action shape =  (1,)\n",
      "log prob shape =  torch.Size([3])\n",
      "entropy shape =  torch.Size([3])\n",
      "value shape =  torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#Test agent\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "obs, info = envs.reset()\n",
    "\n",
    "test_agent = Agent(envs)\n",
    "\n",
    "obs = torch.tensor(obs).float()\n",
    "\n",
    "action, log_prob, entropy, value = test_agent.get_action_and_value(obs)\n",
    "\n",
    "print('state shape = ', obs.shape)\n",
    "print('action shape = ', envs.single_action_space.shape)\n",
    "print('log prob shape = ', log_prob.shape)\n",
    "print('entropy shape = ', entropy.shape)\n",
    "print('value shape = ', value.shape)\n",
    "\n",
    "del test_agent\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, episodes = 10):\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(envs, agent, optimizer, num_steps, num_envs, \n",
    "         num_iterations = 1000, update_epochs = 10, \n",
    "         label = 'test', plot_udpate_freq = 10, history = None):\n",
    "\n",
    "    # label = str(uuid.uuid4()).split('-')[0]\n",
    "    print('run id = ', label)\n",
    "    SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "    FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        print(f'output folder: {SAVE_PATH}')\n",
    "        os.makedirs(SAVE_PATH)\n",
    "    print('save path = ', SAVE_PATH)\n",
    "\n",
    "    M,N = num_steps, num_envs\n",
    "\n",
    "    obs = torch.zeros((M, N) + envs.single_observation_space.shape)\n",
    "    actions = torch.zeros((M,N) + envs.single_action_space.shape)\n",
    "    log_probs = torch.zeros((M,N))\n",
    "    rewards = torch.zeros((M,N))\n",
    "    dones = torch.zeros((M,N)) # for masking\n",
    "    values = torch.zeros((M,N))\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    #Reset env\n",
    "    next_obs, _ = envs.reset()\n",
    "    next_obs = torch.tensor(next_obs).float()\n",
    "    next_done = torch.zeros(N) #N is num envs\n",
    "\n",
    "    print('next obs = ', next_obs.shape)\n",
    "    print('next done = ', next_done.shape)\n",
    "\n",
    "    reward_window = deque(maxlen = 100)\n",
    "\n",
    "    if history == None:\n",
    "        history = defaultdict(list)\n",
    "\n",
    "    loop = tqdm(range(num_iterations))\n",
    "    agent.train()\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    loss = float('inf')\n",
    "\n",
    "    for iter in loop:\n",
    "\n",
    "        #ROLLOUT phase\n",
    "        #M is max steps\n",
    "        if iter % plot_udpate_freq == 0:\n",
    "            plot(history, save_path=FIG_SAVE_PATH)\n",
    "\n",
    "        for step in range(M):\n",
    "            global_step += N\n",
    "\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            #get action\n",
    "            #NOTE: no_grad disables gradient calculation --> reduce memory consumption\n",
    "            #the result of every computation will have requires_grad=False\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "\n",
    "            actions[step] = action\n",
    "            log_probs[step] = log_prob\n",
    "\n",
    "            #make next step with actions\n",
    "            next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "            next_done = np.logical_or(terminated, truncated)\n",
    "\n",
    "            #NOTE: difference between view and reshape\n",
    "            # https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n",
    "            rewards[step] = torch.tensor(reward).view(-1)\n",
    "            next_obs = torch.tensor(next_obs).float()\n",
    "            next_done = torch.tensor(next_done).float()\n",
    "\n",
    "            #NOTE: vector envs will automatically reset, so no need to break \n",
    "            if 'final_info' in info:\n",
    "                for data in info['final_info']:\n",
    "                    if data:\n",
    "                        reward = data['episode']['r']\n",
    "                        reward_window.append(reward)\n",
    "                        avg_reward = np.mean(reward_window)\n",
    "                        history['reward'].append(avg_reward)\n",
    "                        loop.set_description(f\"reward = {avg_reward:.2f}, global_step = {global_step}, best_score = {best_score:.2f}, loss={loss:.2f}\")\n",
    "\n",
    "                        if best_score < avg_reward:\n",
    "                            best_score = avg_reward\n",
    "                            #save model\n",
    "                            torch.save(agent, os.path.join(SAVE_PATH, 'ppo.checkpoint.torch'))\n",
    "            \n",
    "        #update the history for plotting, and printing progress\n",
    "\n",
    "        #OPTIMIZE phase:\n",
    "        with torch.no_grad():\n",
    "            #bootstrap values, compute returns\n",
    "            next_value = agent.get_value(next_obs).reshape(1,-1)\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            last_gae_lambda = 0\n",
    "\n",
    "            for t in reversed(range(NUM_STEPS)):\n",
    "                if t == NUM_STEPS - 1:\n",
    "                    next_none_terminal = np.logical_not(next_done)\n",
    "                    next_values = next_value\n",
    "                else:\n",
    "                    next_none_terminal = np.logical_not(dones[t + 1])\n",
    "                    next_values = values[t + 1]\n",
    "                \n",
    "                #A(s,a) = Q(s,a) - V(s,a) = r(t) + gamma * V(s', a) * mask - V(s)\n",
    "                delta = rewards[t] + GAMMA * next_values * next_none_terminal - values[t]\n",
    "                #NOTE: learn about this formula\n",
    "                advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_none_terminal * last_gae_lambda\n",
    "            returns = advantages + values\n",
    "        \n",
    "        #flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_log_probs = log_probs.reshape(-1)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        #NOTE: randomize the batch to break correlation\n",
    "        batch_size = M * N\n",
    "        mini_batch_size = batch_size // MINI_BATCH_COUNT\n",
    "        b_indicies = np.arange(batch_size)\n",
    "        clip_fracs = []\n",
    "        \n",
    "        for _ in range(update_epochs):\n",
    "            np.random.shuffle(b_indicies)\n",
    "\n",
    "            #NOTE: mini-batch update: \n",
    "            # pros: reduce memory usage, faster updates\n",
    "            # pros: a whole batch may stuck in local minima, mini batches introduce randomness\n",
    "            # cons: estimate a true gradient, larger mini batch size --> more accurate but more memory\n",
    "            for start in range(0, batch_size, mini_batch_size):\n",
    "                end = start + mini_batch_size\n",
    "                mini_indicies = b_indicies[start:end]\n",
    "\n",
    "                _, new_log_prob, entropy, new_value = agent.get_action_and_value(b_obs[mini_indicies], b_actions[mini_indicies])\n",
    "\n",
    "                #NOTE: what formula is this? \n",
    "                log_ratio = new_log_prob - b_log_probs[mini_indicies]\n",
    "\n",
    "                ratio = log_ratio.exp() # trick to remove log\n",
    "\n",
    "                #compute approximate KL: http://joschu.net/blog/kl-approx.html\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kd = (-log_ratio).mean()\n",
    "                    approximate_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                    clip_fracs += [((ratio - 1.0).abs() > CLIP_COEF).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mini_indicies]\n",
    "\n",
    "                #normalize advantage\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                #policy loss (actor)\n",
    "\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2= -mb_advantages * torch.clamp(ratio, 1 - CLIP_COEF, 1 + CLIP_COEF)\n",
    "\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                new_value = new_value.view(-1)\n",
    "\n",
    "                #value loss (MSE)\n",
    "                v_loss = 0.5 * ((new_value - b_returns[mini_indicies]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = pg_loss - ENTROPY_COEF * entropy_loss + v_loss * VF_COEF\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #clip grad\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "        \n",
    "    torch.save(agent, os.path.join(SAVE_PATH, 'ppo.final.torch'))\n",
    "    plot(history, show=True, save_path=FIG_SAVE_PATH)\n",
    "\n",
    "    with open(os.path.join(SAVE_PATH, 'history.pickle'), 'wb') as file:\n",
    "        pickle.dump(history, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id =  baseline\n",
      "output folder: e:\\ML\\NLP\\Reinforcement Learning\\policy_gradient\\final_project\\ppo\\output\\InvertedPendulum-v4\\baseline\n",
      "save path =  e:\\ML\\NLP\\Reinforcement Learning\\policy_gradient\\final_project\\ppo\\output\\InvertedPendulum-v4\\baseline\n",
      "next obs =  torch.Size([3, 4])\n",
      "next done =  torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward = 1000.00, global_step = 613932, best_score = 1000.00, loss=0.02: 100%|██████████| 100/100 [06:57<00:00,  4.17s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAAGdCAYAAAAVEKdkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4fElEQVR4nO3de3yU1aHv/+/ccx8SIBkiAaLiNXgDRdBWLIi2IvV4TrXVsmnraXWrtNnitqW2W/S0YGlFu8uul+oWq9vS3zlK6z61brBVLAesFKVyUaoFkVsMYphJMslc1++PZB4cciGXyWWe+bxfr3mZPLNmsmbN8+L5utZ61nIYY4wAAAByjHOoKwAAADAUCEEAACAnEYIAAEBOIgQBAICcRAgCAAA5iRAEAAByEiEIAADkJEIQAADISe6hrsBASSaTOnDggIqLi+VwOIa6OgAAoAeMMWpsbFRlZaWczoHtq7FtCDpw4ICqqqqGuhoAAKAP9u7dq7Fjxw7o37BtCCouLpbU1oglJSVDXBsAANAToVBIVVVV1nV8INk2BKWGwEpKSghBAABkmcGYysLEaAAAkJMIQQAAICcRggAAQE4iBAEAgJxECAIAADmJEAQAAHISIQgAAOQkQhAAAMhJhCAAAJCTeh2CXn31VV111VWqrKyUw+HQb37zm7TnjTFavHixKisrlZ+frxkzZmj79u1pZSKRiBYsWKBRo0apsLBQc+fO1b59+9LKNDQ0aN68efL7/fL7/Zo3b56OHDnS6w8IAADQmV6HoObmZp199tlasWJFp88vW7ZMy5cv14oVK7Rp0yYFAgFddtllamxstMrU1tZq9erVWrVqldavX6+mpibNmTNHiUTCKnP99ddry5YtevHFF/Xiiy9qy5YtmjdvXh8+IgAAQCdMP0gyq1evtn5PJpMmEAiY++67zzrW2tpq/H6/efjhh40xxhw5csR4PB6zatUqq8z+/fuN0+k0L774ojHGmB07dhhJ5rXXXrPKbNy40Ugy77zzTo/qFgwGjSQTDAb78xEBAMAgGszrd0Y3UN29e7fq6uo0e/Zs65jP59Mll1yiDRs26KabbtLmzZsVi8XSylRWVqqmpkYbNmzQ5Zdfro0bN8rv92vq1KlWmQsvvFB+v18bNmzQqaee2uFvRyIRRSIR6/dQKJTJjwYAQI9s2XtE//nXA0oak/H39rqduuGC8Ro3sqDP77HjQEjPvbFPiQzU76TRRfryheP7/T5DJaMhqK6uTpJUUVGRdryiokJ79uyxyni9XpWWlnYok3p9XV2dysvLO7x/eXm5VeZYS5cu1T333NPvzwAAQH/ctXqrth8YuP8Rf2TdLn3vytM195xKlRfn9fh1rbGENv79sL66clPG6vLpU0YTgo7lcDjSfjfGdDh2rGPLdFa+u/dZtGiRbr/9duv3UCikqqqq3lQbAIB+ORKOWgHo+qnjVFrgydh7/+Hter1T1za/9ge/e1s76xr14y+c3ePX//i/durx9but3y8+eZTOrvL3q04TRhb26/VDLaMhKBAISGrryRkzZox1vL6+3uodCgQCikajamhoSOsNqq+v1/Tp060yH374YYf3P3ToUIdephSfzyefz5exzwIAQG898/oH1s/f/dzpKvJl7jJbO+sU/fSld/X6+x/r9d0f62CwtVev33WoSZI0YWSBplaP1JJrJsnl7L6Dwu4yuk5QdXW1AoGA1q5dax2LRqNat26dFXAmT54sj8eTVubgwYPatm2bVWbatGkKBoN6/fXXrTJ//vOfFQwGrTIAAAw3hxrb5qbWnFCS0QAkSR6XU3dcfqq+8akTJUmNrbFevb6xNS5JuvOK0/Sj/3FWzgcgqQ89QU1NTXrvvfes33fv3q0tW7aorKxM48aNU21trZYsWaKJEydq4sSJWrJkiQoKCnT99ddLkvx+v2688UYtXLhQI0eOVFlZme644w5NmjRJs2bNkiSdfvrpuuKKK/T1r39djzzyiCTpG9/4hubMmdPppGgAAIaDVND43KQxxynZd8V5bZfuUPvf6qlU3UryMjdEl+16HYL+8pe/6NJLL7V+T83DmT9/vlauXKk777xTLS0tuuWWW9TQ0KCpU6dqzZo1Ki4utl7zwAMPyO1269prr1VLS4tmzpyplStXyuVyWWX+4z/+Q9/85jetu8jmzp3b5dpEAAAMB03tQaN4AINGSX7be+/+qFmnf/9F3XXl6d1OTl71+gdaueF97fqoqb1uAzIdOCs5jBmAe/iGgVAoJL/fr2AwqJKSkqGuDgAgB9zw2Gv6f+8d1k+/eI4+f84JA/I3WqIJXfqTV1QXOjon6OKTR3VZfv17H1k/+9xOvbZopkoLvQNSt0wYzOs3cRAAgAxJDTllej7QJ+V7XVp35wxt/PthfeWJttvdPxl0uvKzL52ryeNLh3UAGmyEIAAAMqRxEIbDJMnndmnGqeX6PzdP076GluOWP7m8SDUn9O92eDsiBAEAkCGpO7YGa97NlAllmjJhUP6ULWX0FnkAAHJZyOoJoo8hGxCCAADIgEg8oWg8KWngh8OQGYQgAAAyoOkT6/YM5MRoZA4hCACADEhNii70uliNOUsQggAAyIDBujMMmUMIAgAgAwb7zjD0HyEIAIAM4M6w7EMIAgAgA472BDEcli0IQQAAZEBThJ6gbEMIAgAgAxoZDss6hCAAADKA4bDsQwgCACADrJ4gFkrMGoQgAAAygOGw7EMIAgAgA0IMh2UdQhAAABlAT1D2IQQBAJABqVvkiwhBWYMQBABABqTuDithOCxrEIIAAMgAhsOyDyEIAIB+iieSCkcTkpgYnU0IQQAA9FNqPpBET1A2IQQBANBPqaGwPI9THheX1mzBNwUAQD+l1ggq8jEUlk0IQQAA9FNTe09QCUNhWYUQBABAP3FnWHYiBAEA0E+NEbbMyEaEIAAA+omeoOxECAIAoJ8IQdmJEAQAQD9xd1h2IgQBANBPTfQEZSVCEAAA/cRwWHYiBAEA0E/sIJ+dCEEAAPQTPUHZiRAEAEA/HQ1B9ARlE0IQAAD9lBoOK6InKKsQggAA6CeGw7ITIQgAgH5IJo2aooSgbEQIAgCgH5qjcRnT9jN3h2UXQhAAAP2QGgrzuBzyubmsZhO+LQAA+uGTd4Y5HI4hrg16gxAEAEA/NEXaQlChzzXENUFvEYIAAOiH5lQI8jIpOtsQggAA6IdwNNUTRAjKNoQgAAD6oSmSkEQIykaEIAAA+sHqCfIyJyjbEIIAAOiHoxOj6QnKNoQgAAD6IZwaDqMnKOsQggAA6Ad6grIXIQgAgH7g7rDsRQgCAKAfmhkOy1qEIAAA+qG5vSeogJ6grEMIAgCgH1IrRhcRgrIOIQgAgH5IDYcVMByWdQhBAAD0Q2o4jJ6g7EMIAgCgH472BBGCsk3GQ1A8Htf3vvc9VVdXKz8/XyeeeKLuvfdeJZNJq4wxRosXL1ZlZaXy8/M1Y8YMbd++Pe19IpGIFixYoFGjRqmwsFBz587Vvn37Ml1dAAD6hTlB2SvjIehHP/qRHn74Ya1YsUJvv/22li1bph//+Mf62c9+ZpVZtmyZli9frhUrVmjTpk0KBAK67LLL1NjYaJWpra3V6tWrtWrVKq1fv15NTU2aM2eOEolEpqsMAECfJJJGLbH2niAfc4KyTcZj68aNG/X5z39eV155pSRpwoQJ+tWvfqW//OUvktp6gR588EHddddduuaaayRJTz75pCoqKvTMM8/opptuUjAY1OOPP66nnnpKs2bNkiQ9/fTTqqqq0ksvvaTLL78809UGAKDXUgslSvQEZaOM9wRdfPHF+sMf/qC//e1vkqS//vWvWr9+vT73uc9Jknbv3q26ujrNnj3beo3P59Mll1yiDRs2SJI2b96sWCyWVqayslI1NTVWmWNFIhGFQqG0BwAAAykcbesFcjokn5tpttkm47H129/+toLBoE477TS5XC4lEgn98Ic/1Je+9CVJUl1dnSSpoqIi7XUVFRXas2ePVcbr9aq0tLRDmdTrj7V06VLdc889mf44AAB06ZP7hjkcjiGuDXor47H117/+tZ5++mk988wzeuONN/Tkk0/qJz/5iZ588sm0cseeLMaY455A3ZVZtGiRgsGg9di7d2//PggAAMdxdAd5hsKyUca/tX/+53/Wd77zHX3xi1+UJE2aNEl79uzR0qVLNX/+fAUCAUltvT1jxoyxXldfX2/1DgUCAUWjUTU0NKT1BtXX12v69Omd/l2fzyefz5fpjwMAQJearc1TmRSdjTLeExQOh+V0pr+ty+WybpGvrq5WIBDQ2rVrreej0ajWrVtnBZzJkyfL4/GklTl48KC2bdvWZQgCAGCwNUfYQT6bZfxbu+qqq/TDH/5Q48aN05lnnqk333xTy5cv19e+9jVJbcNgtbW1WrJkiSZOnKiJEydqyZIlKigo0PXXXy9J8vv9uvHGG7Vw4UKNHDlSZWVluuOOOzRp0iTrbjEAAIZac5ThsGyW8W/tZz/7mb7//e/rlltuUX19vSorK3XTTTfpX/7lX6wyd955p1paWnTLLbeooaFBU6dO1Zo1a1RcXGyVeeCBB+R2u3XttdeqpaVFM2fO1MqVK+Vy0eUIABgejvYEcW3KRg5jjBnqSgyEUCgkv9+vYDCokpKSoa4OAMCGHvvTLv3gd2/r8+dU6qdfPHeoq2MLg3n9ZlEDAAD6iH3DshshCACAPgpbO8gzHJaNCEEAAPRRarFEeoKyEyEIAIA+Sm2bwb5h2YkQBABAH1k9QQyHZSVCEAAAfXR0ThA9QdmIEAQAQB81cXdYViMEAQDQR2EWS8xqhCAAAPrIWjGanqCsRAgCAKCPrL3DmBOUlQhBAAD0gTGGvcOyHCEIAIA+iCaSiifbtt+kJyg7EYIAAOiD1L5hklTgoScoGxGCAADog9RQWJ7HKbeLy2k24lsDAKAPmqPcGZbtCEEAAPRBajiMLTOyFyEIAIA+YI2g7EcIAgCgD1L7hnFnWPYiBAEA0AepfcMIQdmLEAQAQB9YPUFe5gRlK0IQAAB90ExPUNYjBAEA0AdHJ0bTE5StCEEAAPRBMxOjsx4hCACAPji6eSohKFsRggAA6IPmaPucIIbDshYhCACAPkj1BBXQE5S1CEEAAPRBuP3usCJCUNYiBAEA0AdNqZ4ghsOyFiEIAIA+SC2WSE9Q9iIEAQDQB6ltMwrYQDVrEYIAAOgDeoKyHyEIAIBeSiaNwu23yBf4mBOUrQhBAAD0UjiWsH6mJyh7EYIAAOil1BpBTofkc3MpzVZ8cwAA9NInt8xwOBxDXBv0FSEIAIBeao6ktsxgKCybEYIAAOilozvIMyk6mxGCAADoJXaQtwdCEAAAvXR0B3lCUDYjBAEA0EtHe4IYDstmhCAAAHqJ4TB7IAQBANBLzewbZguEIAAAeunovmEMh2UzQhAAAL3U1D4cRk9QdiMEAQDQS6nNU9k3LLsRggAA6CWrJ4jhsKxGCAIAoJeOzgmiJyibEYIAAOgl7g6zB0IQAAC9xGKJ9kAIAgCgl8Jsm2ELhCAAAHqpiRWjbYEQBABAL6UmRjMclt0IQQAA9EIknlAsYSTRE5TtCEEAAPRCuP3OMEkq8NATlM0IQQAA9EJqPpDP7ZTbxWU0m/HtAQDQC2yZYR8DEoL279+vL3/5yxo5cqQKCgp0zjnnaPPmzdbzxhgtXrxYlZWVys/P14wZM7R9+/a094hEIlqwYIFGjRqlwsJCzZ07V/v27RuI6gIA0GNsmWEfGQ9BDQ0Nuuiii+TxePT73/9eO3bs0P33368RI0ZYZZYtW6bly5drxYoV2rRpkwKBgC677DI1NjZaZWpra7V69WqtWrVK69evV1NTk+bMmaNEItHJXwUAYHBYd4axRlDWy/g3+KMf/UhVVVV64oknrGMTJkywfjbG6MEHH9Rdd92la665RpL05JNPqqKiQs8884xuuukmBYNBPf7443rqqac0a9YsSdLTTz+tqqoqvfTSS7r88sszXW0AAHqkmTWCbCPjPUHPP/+8pkyZoi984QsqLy/Xueeeq1/84hfW87t371ZdXZ1mz55tHfP5fLrkkku0YcMGSdLmzZsVi8XSylRWVqqmpsYqc6xIJKJQKJT2AAAg01L7hhGCsl/GQ9CuXbv00EMPaeLEifqv//ov3XzzzfrmN7+pX/7yl5Kkuro6SVJFRUXa6yoqKqzn6urq5PV6VVpa2mWZYy1dulR+v996VFVVZfqjAQCgZms4jDlB2S7jISiZTOq8887TkiVLdO655+qmm27S17/+dT300ENp5RwOR9rvxpgOx47VXZlFixYpGAxaj7179/bvgwAA0Al6guwj4yFozJgxOuOMM9KOnX766frggw8kSYFAQJI69OjU19dbvUOBQEDRaFQNDQ1dljmWz+dTSUlJ2gMAgEyz5gTRE5T1Mh6CLrroIu3cuTPt2N/+9jeNHz9eklRdXa1AIKC1a9daz0ejUa1bt07Tp0+XJE2ePFkejyetzMGDB7Vt2zarDAAAQ8EaDqMnKOtl/Bv8p3/6J02fPl1LlizRtddeq9dff12PPvqoHn30UUltw2C1tbVasmSJJk6cqIkTJ2rJkiUqKCjQ9ddfL0ny+/268cYbtXDhQo0cOVJlZWW64447NGnSJOtuMQAAhgJ3h9lHxr/B888/X6tXr9aiRYt07733qrq6Wg8++KBuuOEGq8ydd96plpYW3XLLLWpoaNDUqVO1Zs0aFRcXW2UeeOABud1uXXvttWppadHMmTO1cuVKuVx0PwIAhk5z+4rRDIdlP4cxxgx1JQZCKBSS3+9XMBhkfhAAIGO+8sTremXnIS37H2fp2inciZxpg3n9Zu8wAAB6IbWLPHuHZT9CEAAAvWDtHcZwWNYjBAEA0AupvcPoCcp+hCAAAHqhqX04rIANVLMeIQgAgF6gJ8g+CEEAAPRQMmkUbr9FvsDHnKBsRwgCAKCHwrGE9TM9QdmPEAQAQA+F2+8Mczokn5tLaLbjGwQAoIeaPrFlhsPhGOLaoL8IQQAA9FDY2jKDoTA7IAQBANBDR3uCmBRtB4QgAAB6KHV7PDvI2wMhCACAHkotlMhwmD0QggAA6KEww2G2QggCAKCHPnl3GLIfIQgAgB6yVotmOMwWCEEAAPRQcyS1bxjDYXZACAIAoIea2+8OoyfIHghBAAD0UHP73WHsG2YPhCAAAHooNRzGDvL2QAgCAKCHUsNh9ATZAyEIAIAeSg2HMSfIHghBAAD0UDOLJdoKIQgAgB5iF3l7IQQBANBDrBhtL4QgAAB66Ogu8gyH2QEhCACAHojEE4oljCQmRtsFIQgAgB4It98ZJkmFXnqC7IAQBABAD6TmA/ncTrldXD7tgG8RAIAeSN0ZxkKJ9kEIAgCgB5rYMsN2CEEAAPSAdWcYk6JtgxAEAEAPpLbMYI0g+yAEAQDQA80slGg7hCAAAHrg6HAYc4LsghAEAEAPNDEcZjuEIAAAeoCeIPshBAEA0ANsnmo/hCAAAHogzHCY7RCCAADogSaGw2yHEAQAQA+ErRWj6QmyC0IQAAA9kFoskb3D7IMQBABADzS3D4cVMBxmG4QgAAB6ILViND1B9kEIAgCgB5qjbcNhBWygahuEIAAAeoCeIPshBAEAcBzJpFE41RPkY06QXRCCAAA4jnAsYf1MT5B9EIIAADiO1BpBTofkc3PptAu+SQAAjuOT+4Y5HI4hrg0yhRAEAMBxpOYDFXJnmK0QggAAOI6jPUFMirYTQhAAAMcRjh4dDoN9EIIAADiOpgjDYXZECAIA4DjCDIfZEiEIAIDj+OTdYbAPQhAAAMcRZt8wWxrwELR06VI5HA7V1tZax4wxWrx4sSorK5Wfn68ZM2Zo+/btaa+LRCJasGCBRo0apcLCQs2dO1f79u0b6OoCANBBczS1bxjDYXYyoCFo06ZNevTRR3XWWWelHV+2bJmWL1+uFStWaNOmTQoEArrsssvU2NholamtrdXq1au1atUqrV+/Xk1NTZozZ44SicSxfwYAgAGV2jyVniB7GbAQ1NTUpBtuuEG/+MUvVFpaah03xujBBx/UXXfdpWuuuUY1NTV68sknFQ6H9cwzz0iSgsGgHn/8cd1///2aNWuWzj33XD399NPaunWrXnrppYGqMgAAnQq33x3GvmH2MmAh6NZbb9WVV16pWbNmpR3fvXu36urqNHv2bOuYz+fTJZdcog0bNkiSNm/erFgsllamsrJSNTU1VpljRSIRhUKhtAcAAJmQmhjNDvL2MiCRdtWqVXrjjTe0adOmDs/V1dVJkioqKtKOV1RUaM+ePVYZr9eb1oOUKpN6/bGWLl2qe+65JxPVBwAgTWpiND1B9pLxnqC9e/fqW9/6lp5++mnl5eV1We7YDeiMMcfdlK67MosWLVIwGLQee/fu7X3lAQDoRBNzgmwp4yFo8+bNqq+v1+TJk+V2u+V2u7Vu3Tr967/+q9xut9UDdGyPTn19vfVcIBBQNBpVQ0NDl2WO5fP5VFJSkvYAACATrG0zvAyH2UnGQ9DMmTO1detWbdmyxXpMmTJFN9xwg7Zs2aITTzxRgUBAa9eutV4TjUa1bt06TZ8+XZI0efJkeTyetDIHDx7Utm3brDIAAAyW5tS2GQyH2UrGv83i4mLV1NSkHSssLNTIkSOt47W1tVqyZIkmTpyoiRMnasmSJSooKND1118vSfL7/brxxhu1cOFCjRw5UmVlZbrjjjs0adKkDhOtAQAYaM1Rts2woyGJtHfeeadaWlp0yy23qKGhQVOnTtWaNWtUXFxslXnggQfkdrt17bXXqqWlRTNnztTKlSvlcnECAgAGVzPbZtiSwxhjhroSAyEUCsnv9ysYDDI/CADQZ9F4Uqd87/eSpL/ePVv+fM8Q18jeBvP6zd5hAAB0I9ULJDEx2m4IQQAAdCM1H8jndsrt4rJpJ3ybAAB0o5ktM2yLEAQAQDdSPUFsmWE/hCAAALph3RnGatG2QwgCAKAbLJRoX4QgAAC6wRpB9kUIAgCgG+wbZl+EIAAAutHEcJhtEYIAAOgGPUH2RQgCAKAbTcwJsi1CEAAA3QgzHGZbhCAAALrRxHCYbRGCAADoRjiSWjGaniC7IQQBANAN9g6zL0IQAADdsPYOYzjMdghBAAB0IxylJ8iuCEEAAHQjdYt8ARuo2g4hCACAbqQmRtMTZD+EIAAAupBMGjW3D4cV+JgTZDeEIAAAutASS1g/0xNkP4QgAAC60Nw+FOZ0SD43l0y74RsFAKALqaGwQp9bDodjiGuDTCMEAQDQhVRPUCF3htkSIQgAgC40W1tmMCnajghBAAB0gYUS7Y0QBABAF9gyw94IQQAAdCHcvnkqc4LsiRAEAEAXrJ4ghsNsiRAEAEAXUnOCChkOsyVCEAAAXWhm81RbIwQBANAFqyeIW+RtiRAEAEAX6AmyN0IQAABdoCfI3ghBAAB04eg6QfQE2REhCACALhxdJ4ieIDsiBAEA0AXWCbI3QhAAAF1gnSB7IwQBANAF7g6zN0IQAABd4O4weyMEAQDQCWMMd4fZHCEIAIBOtMaSMqbtZ3qC7IkQBABAJ1K9QA6HlOcmBNkRIQgAgE5Yk6I9LjmdjiGuDQYCIQgAgE40ty+UyBpB9kUIAgCgE+H24TDWCLIvQhAAAJ1obr89njvD7IsQBABAJ8Ltc4K4M8y+CEEAAHSCniD7IwQBANAJa04QPUG2RQgCAKAT1t1h9ATZFiEIAIBOcHeY/RGCAADoBOsE2R8hCACATtATZH+EIAAAOsHdYfZHCAIAoBOsE2R/GQ9BS5cu1fnnn6/i4mKVl5fr6quv1s6dO9PKGGO0ePFiVVZWKj8/XzNmzND27dvTykQiES1YsECjRo1SYWGh5s6dq3379mW6ugAAdCq1izw9QfaV8RC0bt063XrrrXrttde0du1axeNxzZ49W83NzVaZZcuWafny5VqxYoU2bdqkQCCgyy67TI2NjVaZ2tparV69WqtWrdL69evV1NSkOXPmKJFIZLrKAAB0EG4fDqMnyL4cxhgzkH/g0KFDKi8v17p16/TpT39axhhVVlaqtrZW3/72tyW19fpUVFToRz/6kW666SYFg0GNHj1aTz31lK677jpJ0oEDB1RVVaUXXnhBl19++XH/bigUkt/vVzAYVElJyUB+RACADc28/xX9/VCzfv2NCzX1xJFDXZ2cMZjX7wGfExQMBiVJZWVlkqTdu3errq5Os2fPtsr4fD5dcskl2rBhgyRp8+bNisViaWUqKytVU1NjlTlWJBJRKBRKewAA0FdHe4IYDrOrAQ1Bxhjdfvvtuvjii1VTUyNJqqurkyRVVFSkla2oqLCeq6urk9frVWlpaZdljrV06VL5/X7rUVVVlemPAwDIIc2R1JwghsPsakBD0G233aa33npLv/rVrzo853A40n43xnQ4dqzuyixatEjBYNB67N27t+8VBwDktFgiad0iT0+QfQ1YCFqwYIGef/55vfzyyxo7dqx1PBAISFKHHp36+nqrdygQCCgajaqhoaHLMsfy+XwqKSlJewAA0Bd7DjcrkTQq8rk1usg31NXBAMl4CDLG6LbbbtNzzz2nP/7xj6qurk57vrq6WoFAQGvXrrWORaNRrVu3TtOnT5ckTZ48WR6PJ63MwYMHtW3bNqsMAAADZV9DiyRpbGm+nM7uRymQvTLex3frrbfqmWee0W9/+1sVFxdbPT5+v1/5+flyOByqra3VkiVLNHHiRE2cOFFLlixRQUGBrr/+eqvsjTfeqIULF2rkyJEqKyvTHXfcoUmTJmnWrFmZrjIAAGl2HWpb1mVsaf4Q1wQDKeMh6KGHHpIkzZgxI+34E088oa985SuSpDvvvFMtLS265ZZb1NDQoKlTp2rNmjUqLi62yj/wwANyu9269tpr1dLSopkzZ2rlypVyuZigBgAYWK/tOixJOm986XFKIpsN+DpBQ4V1ggAAfZFIGp177xqFWuP6za0X6ZyqEUNdpZxiq3WCAADIJi9uq1Oote32+JpK/ifazghBAAB8wk/WtO13OarIJ7eLy6Sd8e0CAPAJDeGoJOmrF00Y2opgwBGCAAD4BGf7orwzTy8f4ppgoBGCAABo1xpL6OPmtp6gMSXcHm93hCAAANp9GGqVJOV7XCrJZ7sMuyMEAQDQbn/7StEBf95x97NE9iMEAQAgKRyN6/rH/ixJOn1M8XFKww4IQQAASFq+5m/Wz5ecMnoIa4LBQggCAOQ8Y4weW7/b+v2/nTt2CGuDwUIIAgDkvP9866D186++fqG8bi6PuYBvGQCQ8/73X/ZaP19QXTaENcFgIgQBAHLaH97+UH969yNJ0i+/doFcTu4KyxWEIABATnu8fS7Q+RNK9WkmROcUQhAAIGe9vLNeG/5+WJJ091VnDnFtMNgIQQCAnHSoMaLvrd4mSZoyvlRnVpYMcY0w2AhBAICck0wa/cO/v679R9pWiL7/2rNZIToHsTEKACCnROIJnfq9FyVJxT63/nPBxRo/snCIa4WhQE8QACCnLHpuq/XzVy+u1oRRBKBcRQgCAOSMNz5o0HNv7JckXXnWGP3TrIlDXCMMJUIQACBn/Psntsb42RfPZR5QjmNOEADA1oItMcUTSeV5XPrD2/WSpKXXTJKTRRFzHiEIAGBLm97/WB+GWnXbM2+mHQ+U5Om6KVVDVCsMJ4QgAIDt/HbLfn1r1ZZOn/unyybSCwRJhCAAgI18cDisrz25Se/VN1nHLj55lP5xxkny53vkdTt1SkXxENYQwwkhCABgC8//9YC++av0oa//dXWNvjx1HBOg0SlCEAAg6z3/1wP61qqjAejB687R58+pJPygW4QgAEDWCbbEtOtQk4rz3Jq1/FXr+OTxpXryaxeoyMflDcfHWQIAyCoNzVGd+7/Wdjh+QXWZnvjK+SokAKGHOFMAAFmjKRLXd557q8PxyeNL9dSNF8jndg1BrZCtCEEAgGHNGKPVb+7X7f/fX61jDof0rZkTVT2qUFdOGiO3iw0Q0HuEIADAsLZyw/u65z93WL97XU49PO88fea0iiGsFeyAEAQAGLbeq2/Sz/74nvX7T794jq6oCTDshYwgBAEAhqVX/3ZI//Dvr0uSRhR49MI3P6XKEflDXCvYCYOoAIBhwxgjSVqzvc4KQHkep35760UEIGQcPUEAgGHhz7sO66srNykcTVjHfG6nXr3zUpUX5w1hzWBXhCAAwKDY+3FYh5oiOm9cqcLRuB5Zt0tFPreuqAnoU8te7lD+7KoRevjL5xGAMGAIQQCAAffHdz7U11b+RZLkdjqUMEbtI1/64Qtvp5U9f0KpLj8zoH+YNkFeN7M2MHAIQQCAAdMaS2jZizv17/9vt3UsnjSdlr301NF6ZN4Ugg8GDSEIAJAxsURSDeGomlrjuumpzXq3vsl67rRAsa46u1L3r9mpGaeWa+k1k1RW6NX+hhaNLc1nwUMMOkIQACAj3v+oWf/zl3/Re58IPpJUWuDReeNKde/VNTphRL5uvfTktOcnjCoczGoCFkIQAKBPmiNxLX5+u04NFOu1XR/rpbc/7FDmijMDeuC6c5TvZXFDDD+EIADAcX3UFNHWfUEFW2I6a6xfkvSZ+9d1KDeurEDf/dzpOnF0oSqK8+Qv8Ax2VYEeIwQBALq040BI96/ZqT+8U99tuUtPHa2LJ47W/GnjmduDrEEIAgBYDjdF9N8f2qAx/nxt3HX4uOU3LvqMKorz5HQ6BqF2QGYRggAAkqTfbtmvb63aIkl6/3DYOj6iwKNHvjxZZ1SWqDjPo9d3f6x36xt11dmVKsljuAvZixAEADks1BrTf7z2gX71+gf64OOjweeEEfmadXq5pp88SpefGUh7zQXVZbqgumywqwpkHCEIAHJANJ7UW/uOyO1yavUb+7T/SKuqyvL17OZ9CrXGrXIXTCjTv37pXAX8bFUB+yMEAUAW27L3iFa/sU/BlpiuOW+sLj55lCLxpPK9Ln1wOKzn/7pf/3vzPu35xPDWsU4uL9Lcsys17aSROn8CPTzIHYQgAMhCb+07ovvX/E3r/nbIOvabLQesn11OhxJdbE+Rcskpo/WFKWP1uZoxTGxGTiIEAcAwlUgaHTjSoh0HQ9r7cVjr3/tIUtvKzO8f07Mz56wxevmdejVHE9ZrU644M6AragK6/MyAwtG43E4n6/cAIgQBwLCz8e+H9cirf9f2AyEdaox0W3b+tPGqnXWKSgu9ao0ltOdwWO/WN8rndumEEfk6LVCc1svDys3AUYQgAOiHZNIoYY72uiSNUSJpVOBt++fVGKNt+0Pauj+oSDyhSSf4dSDYqnAkLqfToabWuFpiCe04GNJHjRH9effHHf7GCSPyFYknlO916eTRRTppdJH+56dO7DB5Oc/j0qmBYp0aKB7YDw3YBCEIANQWVhwOh/XfaDypxtaYfrf1oPZ+HNaew2GFWmM6d1ypXA6HPvg4rHA0ro1/P2wNQaU4HFK+x6WkMRpd7NPej1v6VKevXjRB//28sao5wZ+JjwjgGIQgAD1mjFE0kVSwJaayAq+SRmoIRxVLJBVPGMUSSe070qJoPKlCr1vFeW7Fk0YnlxfJn9/zOSjGGDVF4vow1KqGcEz5HpfKCr0qK/Qq2BJT0hi5nA65HA4VeN3dDvHEE0nFk0Z5HpcamqN671CTjoRj2rY/qFBrTPGE0Za9R/ROXUj5Hpeao4luJxS/tqtjT03H+kvh9mCUCkDnjRuhg8FWtcQSOhKOaVxZgU4cXahCr1sOR9trnE6H/vj2h1o890x9YUpVj9sLQN8QgoAcEU8k1RxJ6HBzREdaYnJIcjudqgu1as/hZnndTjW2xnUkHFVTJKGxpflKJo3ePxxWQziq1lhCr+06rOPccNSlUyqKdOBIq2KJttu3j4RjKslzy+V0qDmSUFVZvuJJo4NHWhVNJHv13m6nQ/GkUXFe2z9pTodDLqdDTZG4ovGk3E6H/PkeHW6Odvs+sUS8w7Ein1tNkbj+Ydp4Fee5FWppK1M5Il8el0MnlxfpnKoRcqh93o1DisQS+t3Wg0oayet26qKTRurE0UW9+kwABp7DGNPHf9KGt1AoJL/fr2AwqJKSkqGuTkaY9rkGTofDmuhojFEsYRSJJ9QaSyqaSKrI65bb5ZDX7ZTb6ZDD0bdbX40xCkcTisbb3jfUEpPb5ZTP7VRrLCEjKRxJ6ECwReFoXPmetv8j9+d7VORzKWna/u82aYzcToeSRmqOxmVM2/+Ve11OeVxO6+I1osAjr8tpvSZpJCMj84n3SRojI8kk255Lth83RmqOxLXzw0ZF40kV+dwKtsQUiSfkcTmVNFJLNK5wNKHmaELNkbgOBlt0qDEin9ulUUVetcQScjoc8rmd8rldiieNPmqKyKht3kc0kVQ0nlRpgUelBV6VtPdsFPnccjqk1nhS79U3yetyqtDnViyRVCSeaG9LKWnaduIuznPL+YnvxO10aESBV/lep/LcLhV4XXK7nHI5HXI7HSrwuVXkc8nrcunDUFtPQl17j4LH5VCoJW61QV2oVc2Rtou0wyE1RxJqjSXUFIkrEu9dsOgpp0PytH+XbpdD5cU++dwuq5elIRy1ekV6qzjPrdICryLxhD4MHZ0g7HG1hZ6+/OtV6c9TWZFXgZJ8NUViqh5VqJNGF2n6SaPkcTlU6HMr0n4OjSjwyMNmoMCgGszr97DvCfr5z3+uH//4xzp48KDOPPNMPfjgg/rUpz41pHX6v28d0G3PvKmp1WWaMqFUTkfbP8gt0YSSxiieNIrE2i6A0XhSkXjbz/FE20U81BpXJJ5QJNbWTR9LJOVxOVXkcyvP42wbVkgmlUyq7bWxRNsFOJG0/tEvaO/+b40luv0/c6/bqUKvSy6nQ05H20XV6XRoRIHHGq5IJI11QYknk4oljKLxpA41RlQXah2EFsVgKvS6NKLAK4ejLdwZSf58j8aPLFBxnkdlhV55XA7t/bhF8WRSlf58VY8u1JFwTGNL83X+hDKNKvJp0/sfq7zYp5NGF3W7xkwyabRl3xE1tcZVnOfWqCKfmiJx+dxOHW6OyqG2gBhLJOV1OzUi36MRBV4V+lzW5OLU++xtCGuMP19et9M61hiJ66OmiEItMRX63FbgNu1BOs/jlEMOHWpq1cnlxb0algNgb8O6J+jXv/615s2bp5///Oe66KKL9Mgjj+ixxx7Tjh07NG7cuG5fO1BJMhJPaMoPXlJja8du8+HA63L2eiihJ1xOh4p8biWTRq3xhPI8bSGswOvSqCKfRhR41BpLKhxN6FBjRIlkUg6HQ23XRocSybY6FeW55ZBDrbGEYom2wOV06LjzMI7H0d4bcUpFkQq9bcMX/nyPfO62nianw6ECr0v5XpcKvW4VeF0aXezTGH++WmNtQ0T5HpeMpEgsqZZYW6AdW1rQ/vkll7OtFywYjunjcFSNrTFJUlNrXEZtf39sab41vON1t5VPzV8xRiot8CocjVu9cw61Xfw/DkfVGk0oEm/72/GkUSLRFk6PhKNWkC4vzlOhz2X1RMUTRv78tiElORwqLWgLMZIkI/k8LhX62j5zkc+tAp9LPje3SAMYvgazJ2hYh6CpU6fqvPPO00MPPWQdO/3003X11Vdr6dKl3b52oBrx0Vf/riUvvCNJ+tIFVfK52+4AcTocyve65HY65HE5rQugt31oxed2yuNqu/D5873yeZzytpdLreza2BpTJJaUu31Ywe10yO10Kt/rkrf99d724aNwpG14Ic/jlM/jUl77+6XuamkLGEk1tsbVGksoYYzVExVLtF1Ygy0xRePJtgmmToccjrY5Ip72Ia88j0tnVJaoyNd+kR1AyaRRY2tc8U+EJ4cccjjb5nc41P7ftmt925Bg+/G2Y6x2CwB2wHCYpGg0qs2bN+s73/lO2vHZs2drw4YNHcpHIhFFIkfnDIRCoQGp1/zpE/T+4bBmnV6uz5xWMSB/o0e6mWPpbQ9fkjSiwDtIFeofp9PBCrYAgEE1bGf8ffTRR0okEqqoSA8aFRUVqqur61B+6dKl8vv91qOqamBuL/W5XVry3yYNbQACAAD9NmxDUMqxwxyphcyOtWjRIgWDQeuxd+/ewaoiAADIQsN2OGzUqFFyuVwden3q6+s79A5Jks/nk8/nG6zqAQCALDdse4K8Xq8mT56stWvXph1fu3atpk+fPkS1AgAAdjFse4Ik6fbbb9e8efM0ZcoUTZs2TY8++qg++OAD3XzzzUNdNQAAkOWGdQi67rrrdPjwYd177706ePCgampq9MILL2j8+PFDXTUAAJDlhvU6Qf1hx20zAACwu8G8fg/bOUEAAAADiRAEAAByEiEIAADkJEIQAADISYQgAACQkwhBAAAgJxGCAABAThrWiyX2R2r5o1AoNMQ1AQAAPZW6bg/GMoa2DUGNjY2SpKqqqiGuCQAA6K3Gxkb5/f4B/Ru2XTE6mUzqwIEDKi4ulsPhyOh7h0IhVVVVae/evTm/GjVtkY72OIq2SEd7HEVbHEVbpEu1x44dO3TqqafK6RzYWTu27QlyOp0aO3bsgP6NkpISTtp2tEU62uMo2iId7XEUbXEUbZHuhBNOGPAAJDExGgAA5ChCEAAAyEmEoD7w+Xy6++675fP5hroqQ462SEd7HEVbpKM9jqItjqIt0g12e9h2YjQAAEB36AkCAAA5iRAEAAByEiEIAADkJEIQAADISYSgXvr5z3+u6upq5eXlafLkyfrTn/401FXKuMWLF8vhcKQ9AoGA9bwxRosXL1ZlZaXy8/M1Y8YMbd++Pe09IpGIFixYoFGjRqmwsFBz587Vvn37Bvuj9Nqrr76qq666SpWVlXI4HPrNb36T9nymPntDQ4PmzZsnv98vv9+vefPm6ciRIwP86XrveO3xla98pcO5cuGFF6aVsUt7LF26VOeff76Ki4tVXl6uq6++Wjt37kwrkyvnR0/aIpfOjYceekhnnXWWteDhtGnT9Pvf/956PlfOC+n4bTHszguDHlu1apXxeDzmF7/4hdmxY4f51re+ZQoLC82ePXuGumoZdffdd5szzzzTHDx40HrU19dbz993332muLjYPPvss2br1q3muuuuM2PGjDGhUMgqc/PNN5sTTjjBrF271rzxxhvm0ksvNWeffbaJx+ND8ZF67IUXXjB33XWXefbZZ40ks3r16rTnM/XZr7jiClNTU2M2bNhgNmzYYGpqasycOXMG62P22PHaY/78+eaKK65IO1cOHz6cVsYu7XH55ZebJ554wmzbts1s2bLFXHnllWbcuHGmqanJKpMr50dP2iKXzo3nn3/e/O53vzM7d+40O3fuNN/97neNx+Mx27ZtM8bkznlhzPHbYridF4SgXrjgggvMzTffnHbstNNOM9/5zneGqEYD4+677zZnn312p88lk0kTCATMfffdZx1rbW01fr/fPPzww8YYY44cOWI8Ho9ZtWqVVWb//v3G6XSaF198cUDrnknHXvQz9dl37NhhJJnXXnvNKrNx40YjybzzzjsD/Kn6rqsQ9PnPf77L19i5Perr640ks27dOmNMbp8fx7aFMbl9bhhjTGlpqXnsscdy+rxISbWFMcPvvGA4rIei0ag2b96s2bNnpx2fPXu2NmzYMES1GjjvvvuuKisrVV1drS9+8YvatWuXJGn37t2qq6tLawefz6dLLrnEaofNmzcrFoullamsrFRNTU1Wt1WmPvvGjRvl9/s1depUq8yFF14ov9+fle3zyiuvqLy8XKeccoq+/vWvq76+3nrOzu0RDAYlSWVlZZJy+/w4ti1ScvHcSCQSWrVqlZqbmzVt2rScPi+ObYuU4XRe2HYD1Uz76KOPlEgkVFFRkXa8oqJCdXV1Q1SrgTF16lT98pe/1CmnnKIPP/xQP/jBDzR9+nRt377d+qydtcOePXskSXV1dfJ6vSotLe1QJpvbKlOfva6uTuXl5R3ev7y8POva57Of/ay+8IUvaPz48dq9e7e+//3v6zOf+Yw2b94sn89n2/Ywxuj222/XxRdfrJqaGkm5e3501hZS7p0bW7du1bRp09Ta2qqioiKtXr1aZ5xxhnVRzqXzoqu2kIbfeUEI6iWHw5H2uzGmw7Fs99nPftb6edKkSZo2bZpOOukkPfnkk9YEtr60g13aKhOfvbPy2dg+1113nfVzTU2NpkyZovHjx+t3v/udrrnmmi5fl+3tcdttt+mtt97S+vXrOzyXa+dHV22Ra+fGqaeeqi1btujIkSN69tlnNX/+fK1bt856PpfOi67a4owzzhh25wXDYT00atQouVyuDimzvr6+Q8K3m8LCQk2aNEnvvvuudZdYd+0QCAQUjUbV0NDQZZlslKnPHggE9OGHH3Z4/0OHDmV1+0jSmDFjNH78eL377ruS7NkeCxYs0PPPP6+XX35ZY8eOtY7n4vnRVVt0xu7nhtfr1cknn6wpU6Zo6dKlOvvss/XTn/40J8+LrtqiM0N9XhCCesjr9Wry5Mlau3Zt2vG1a9dq+vTpQ1SrwRGJRPT2229rzJgxqq6uViAQSGuHaDSqdevWWe0wefJkeTyetDIHDx7Utm3bsrqtMvXZp02bpmAwqNdff90q8+c//1nBYDCr20eSDh8+rL1792rMmDGS7NUexhjddttteu655/THP/5R1dXVac/n0vlxvLbojJ3Pjc4YYxSJRHLqvOhKqi06M+TnRa+mUee41C3yjz/+uNmxY4epra01hYWF5v333x/qqmXUwoULzSuvvGJ27dplXnvtNTNnzhxTXFxsfc777rvP+P1+89xzz5mtW7eaL33pS53e7jl27Fjz0ksvmTfeeMN85jOfyYpb5BsbG82bb75p3nzzTSPJLF++3Lz55pvWMgiZ+uxXXHGFOeuss8zGjRvNxo0bzaRJk4bdra7GdN8ejY2NZuHChWbDhg1m9+7d5uWXXzbTpk0zJ5xwgi3b4x//8R+N3+83r7zyStrtveFw2CqTK+fH8doi186NRYsWmVdffdXs3r3bvPXWW+a73/2ucTqdZs2aNcaY3DkvjOm+LYbjeUEI6qV/+7d/M+PHjzder9ecd955abeE2kVqDQuPx2MqKyvNNddcY7Zv3249n0wmzd13320CgYDx+Xzm05/+tNm6dWvae7S0tJjbbrvNlJWVmfz8fDNnzhzzwQcfDPZH6bWXX37ZSOrwmD9/vjEmc5/98OHD5oYbbjDFxcWmuLjY3HDDDaahoWGQPmXPddce4XDYzJ4924wePdp4PB4zbtw4M3/+/A6f1S7t0Vk7SDJPPPGEVSZXzo/jtUWunRtf+9rXrOvC6NGjzcyZM60AZEzunBfGdN8Ww/G8cBhjTO/6jgAAALIfc4IAAEBOIgQBAICcRAgCAAA5iRAEAAByEiEIAADkJEIQAADISYQgAACQkwhBAAAgJxGCAABATiIEAQCAnEQIAgAAOYkQBAAActL/D3kHDKuDsztTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ntpath' has no attribute 'jion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m M \u001b[38;5;241m=\u001b[39m NUM_STEPS\n\u001b[0;32m     12\u001b[0m N \u001b[38;5;241m=\u001b[39m NUM_ENVS\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_ENVS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 184\u001b[0m, in \u001b[0;36mtune\u001b[1;34m(envs, agent, optimizer, num_steps, num_envs, num_iterations, update_epochs, label, plot_udpate_freq, history)\u001b[0m\n\u001b[0;32m    181\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(agent, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo.final.torch\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    182\u001b[0m plot(history, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_path\u001b[38;5;241m=\u001b[39mFIG_SAVE_PATH)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjion\u001b[49m(SAVE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    185\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(history, file)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ntpath' has no attribute 'jion'"
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'\n",
    "\n",
    "agent = Agent(envs)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr = 1e-4, eps = 1e-5)\n",
    "\n",
    "M = NUM_STEPS\n",
    "N = NUM_ENVS\n",
    "\n",
    "tune(envs, agent, optimizer, NUM_STEPS, NUM_ENVS, num_iterations=100, label = 'baseline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, device = DEVICE, episodes = 10):\n",
    "\n",
    "    agent = torch.load(model_path)\n",
    "\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(**ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs).to(device)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    print('reward = ', reward)\n",
    "                    total_rewards.append(reward)\n",
    "    \n",
    "    sns.lineplot(y = total_rewards, x = list(range(len(total_rewards))))\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT, 'test', 'ppo.checkpoint.torch')\n",
    "evaluate(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_obs, _ = envs.reset()\n",
    "total_rewards = []\n",
    "episodes = 10\n",
    "\n",
    "while len(total_rewards) < episodes: \n",
    "    next_obs = torch.Tensor(next_obs)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "    if 'final_info' in info:\n",
    "        for data in info['final_info']:\n",
    "            if data:\n",
    "                reward = data['episode']['r'][0]\n",
    "                print(reward)\n",
    "                total_rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
