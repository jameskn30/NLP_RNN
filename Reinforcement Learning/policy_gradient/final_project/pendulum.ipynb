{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "https://github.com/vwxyzjn/cleanrl?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': \"InvertedPendulum-v4\"\n",
    "}\n",
    "NUM_ENVS = 3\n",
    "SEED = 1\n",
    "LR = 1e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output', ENV_ARGS['id'])\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gamma, **env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.ClipAction(env)\n",
    "    env = gym.wrappers.NormalizeObservation(env)\n",
    "    env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    env = gym.wrappers.NormalizeReward(env, gamma = gamma)\n",
    "    env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.Env, hidden_size: int = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_shape = np.array(envs.single_observation_space.shape).prod()\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            #NOTE: why use tanh here? \n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(layer = nn.Linear(hidden_size, self.action_shape), std = 0.01),\n",
    "        )\n",
    "\n",
    "        #shape = (1, state_shape)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, self.action_shape, dtype=torch.float))\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(nn.Linear(hidden_size, 1), std = 1.0),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action = None):\n",
    "        '''\n",
    "        @params:\n",
    "            x: torch.tensor observation, shape = (N, observation size)\n",
    "            action: torch.tensor action\n",
    "        @returns:\n",
    "            action: torch.tensor, shape = (N, action size)\n",
    "            log_prob: torch.tensor, shape = (N,)\n",
    "            entropy: torch.tensor, shape = (N,)\n",
    "            value: torch.tensor, shape = (N,)\n",
    "        '''\n",
    "        action_mean = self.actor_mean(x)\n",
    "        #make action logstd the shape[0] with mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        #exponential trick to remove log\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        probs = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample() \n",
    "        \n",
    "        #get value from critic\n",
    "        value = self.get_value(x)\n",
    "        log_prob = probs.log_prob(action).sum(1)\n",
    "        #entropy for regularization\n",
    "        entropy = probs.entropy().sum(1)\n",
    "        \n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape =  torch.Size([3, 4])\n",
      "action shape =  (1,)\n",
      "log prob shape =  torch.Size([3])\n",
      "entropy shape =  torch.Size([3])\n",
      "value shape =  torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#Test agent\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "obs, info = envs.reset()\n",
    "\n",
    "test_agent = Agent(envs)\n",
    "\n",
    "obs = torch.tensor(obs).float()\n",
    "\n",
    "action, log_prob, entropy, value = test_agent.get_action_and_value(obs)\n",
    "\n",
    "print('state shape = ', obs.shape)\n",
    "print('action shape = ', envs.single_action_space.shape)\n",
    "print('log prob shape = ', log_prob.shape)\n",
    "print('entropy shape = ', entropy.shape)\n",
    "print('value shape = ', value.shape)\n",
    "\n",
    "del test_agent\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, episodes = 10):\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(envs, agent, optimizer, num_steps, num_envs, \n",
    "         num_iterations = 1000, update_epochs = 10, \n",
    "         label = 'test', plot_udpate_freq = 10, history = None):\n",
    "\n",
    "    # label = str(uuid.uuid4()).split('-')[0]\n",
    "    print('run id = ', label)\n",
    "    SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "    FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        print(f'output folder: {SAVE_PATH}')\n",
    "        os.makedirs(SAVE_PATH)\n",
    "    print('save path = ', SAVE_PATH)\n",
    "\n",
    "    M,N = num_steps, num_envs\n",
    "\n",
    "    obs = torch.zeros((M, N) + envs.single_observation_space.shape)\n",
    "    actions = torch.zeros((M,N) + envs.single_action_space.shape)\n",
    "    log_probs = torch.zeros((M,N))\n",
    "    rewards = torch.zeros((M,N))\n",
    "    dones = torch.zeros((M,N)) # for masking\n",
    "    values = torch.zeros((M,N))\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    #Reset env\n",
    "    next_obs, _ = envs.reset()\n",
    "    next_obs = torch.tensor(next_obs).float()\n",
    "    next_done = torch.zeros(N) #N is num envs\n",
    "\n",
    "    print('next obs = ', next_obs.shape)\n",
    "    print('next done = ', next_done.shape)\n",
    "\n",
    "    reward_window = deque(maxlen = 100)\n",
    "\n",
    "    if history == None:\n",
    "        history = defaultdict(list)\n",
    "\n",
    "    loop = tqdm(range(num_iterations))\n",
    "    agent.train()\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    loss = float('inf')\n",
    "\n",
    "    for iter in loop:\n",
    "\n",
    "        #ROLLOUT phase\n",
    "        #M is max steps\n",
    "        if iter % plot_udpate_freq == 0:\n",
    "            plot(history, save_path=FIG_SAVE_PATH)\n",
    "\n",
    "        for step in range(M):\n",
    "            global_step += N\n",
    "\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            #get action\n",
    "            #NOTE: no_grad disables gradient calculation --> reduce memory consumption\n",
    "            #the result of every computation will have requires_grad=False\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "\n",
    "            actions[step] = action\n",
    "            log_probs[step] = log_prob\n",
    "\n",
    "            #make next step with actions\n",
    "            next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "            next_done = np.logical_or(terminated, truncated)\n",
    "\n",
    "            #NOTE: difference between view and reshape\n",
    "            # https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n",
    "            rewards[step] = torch.tensor(reward).view(-1)\n",
    "            next_obs = torch.tensor(next_obs).float()\n",
    "            next_done = torch.tensor(next_done).float()\n",
    "\n",
    "            #NOTE: vector envs will automatically reset, so no need to break \n",
    "            if 'final_info' in info:\n",
    "                for data in info['final_info']:\n",
    "                    if data:\n",
    "                        reward = data['episode']['r']\n",
    "                        reward_window.append(reward)\n",
    "                        avg_reward = np.mean(reward_window)\n",
    "                        history['reward'].append(avg_reward)\n",
    "                        loop.set_description(f\"reward = {avg_reward:.2f}, global_step = {global_step}, best_score = {best_score:.2f}, loss={loss:.2f}\")\n",
    "\n",
    "                        if best_score < avg_reward:\n",
    "                            best_score = avg_reward\n",
    "                            #save model\n",
    "                            torch.save(agent, os.path.join(SAVE_PATH, 'ppo.checkpoint.torch'))\n",
    "            \n",
    "        #update the history for plotting, and printing progress\n",
    "\n",
    "        #OPTIMIZE phase:\n",
    "        with torch.no_grad():\n",
    "            #bootstrap values, compute returns\n",
    "            next_value = agent.get_value(next_obs).reshape(1,-1)\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            last_gae_lambda = 0\n",
    "\n",
    "            for t in reversed(range(NUM_STEPS)):\n",
    "                if t == NUM_STEPS - 1:\n",
    "                    next_none_terminal = np.logical_not(next_done)\n",
    "                    next_values = next_value\n",
    "                else:\n",
    "                    next_none_terminal = np.logical_not(dones[t + 1])\n",
    "                    next_values = values[t + 1]\n",
    "                \n",
    "                #A(s,a) = Q(s,a) - V(s,a) = r(t) + gamma * V(s', a) * mask - V(s)\n",
    "                delta = rewards[t] + GAMMA * next_values * next_none_terminal - values[t]\n",
    "                #NOTE: learn about this formula\n",
    "                advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_none_terminal * last_gae_lambda\n",
    "            returns = advantages + values\n",
    "        \n",
    "        #flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_log_probs = log_probs.reshape(-1)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        #NOTE: randomize the batch to break correlation\n",
    "        batch_size = M * N\n",
    "        mini_batch_size = batch_size // MINI_BATCH_COUNT\n",
    "        b_indicies = np.arange(batch_size)\n",
    "        clip_fracs = []\n",
    "        \n",
    "        for _ in range(update_epochs):\n",
    "            np.random.shuffle(b_indicies)\n",
    "\n",
    "            #NOTE: mini-batch update: \n",
    "            # pros: reduce memory usage, faster updates\n",
    "            # pros: a whole batch may stuck in local minima, mini batches introduce randomness\n",
    "            # cons: estimate a true gradient, larger mini batch size --> more accurate but more memory\n",
    "            for start in range(0, batch_size, mini_batch_size):\n",
    "                end = start + mini_batch_size\n",
    "                mini_indicies = b_indicies[start:end]\n",
    "\n",
    "                _, new_log_prob, entropy, new_value = agent.get_action_and_value(b_obs[mini_indicies], b_actions[mini_indicies])\n",
    "\n",
    "                #NOTE: what formula is this? \n",
    "                log_ratio = new_log_prob - b_log_probs[mini_indicies]\n",
    "\n",
    "                ratio = log_ratio.exp() # trick to remove log\n",
    "\n",
    "                #compute approximate KL: http://joschu.net/blog/kl-approx.html\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kd = (-log_ratio).mean()\n",
    "                    approximate_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                    clip_fracs += [((ratio - 1.0).abs() > CLIP_COEF).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mini_indicies]\n",
    "\n",
    "                #normalize advantage\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                #policy loss (actor)\n",
    "\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2= -mb_advantages * torch.clamp(ratio, 1 - CLIP_COEF, 1 + CLIP_COEF)\n",
    "\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                new_value = new_value.view(-1)\n",
    "\n",
    "                #value loss (MSE)\n",
    "                v_loss = 0.5 * ((new_value - b_returns[mini_indicies]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = pg_loss - ENTROPY_COEF * entropy_loss + v_loss * VF_COEF\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #clip grad\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "        \n",
    "    torch.save(agent, os.path.join(SAVE_PATH, 'ppo.final.torch'))\n",
    "    plot(history, show=True, save_path=FIG_SAVE_PATH)\n",
    "    with open('history.pickle', 'wb') as file:\n",
    "        pickle.dump(history, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id =  test\n",
      "save path =  e:\\ML\\NLP\\Reinforcement Learning\\policy_gradient\\final_project\\output\\InvertedPendulum-v4\\test\n",
      "next obs =  torch.Size([3, 4])\n",
      "next done =  torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward = 911.22, global_step = 614187, best_score = 1000.00, loss=0.02: 100%|██████████| 100/100 [06:20<00:00,  3.80s/it] \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "dump() missing required argument 'obj' (pos 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m M \u001b[38;5;241m=\u001b[39m NUM_STEPS\n\u001b[0;32m     12\u001b[0m N \u001b[38;5;241m=\u001b[39m NUM_ENVS\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_ENVS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 184\u001b[0m, in \u001b[0;36mtune\u001b[1;34m(envs, agent, optimizer, num_steps, num_envs, num_iterations, update_epochs, label, plot_udpate_freq, history)\u001b[0m\n\u001b[0;32m    182\u001b[0m plot(history, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_path\u001b[38;5;241m=\u001b[39mFIG_SAVE_PATH)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: dump() missing required argument 'obj' (pos 1)"
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'\n",
    "\n",
    "agent = Agent(envs)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr = LR, eps = 1e-5)\n",
    "\n",
    "M = NUM_STEPS\n",
    "N = NUM_ENVS\n",
    "\n",
    "tune(envs, agent, optimizer, NUM_STEPS, NUM_ENVS, num_iterations=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, device = DEVICE, episodes = 10):\n",
    "\n",
    "    agent = torch.load(model_path)\n",
    "\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(**ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs).to(device)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    print('reward = ', reward)\n",
    "                    total_rewards.append(reward)\n",
    "    \n",
    "    sns.lineplot(y = total_rewards, x = list(range(len(total_rewards))))\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT, )\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268.0151\n",
      "266.8834\n",
      "260.6238\n",
      "75.81549\n",
      "267.05634\n",
      "263.5786\n",
      "266.1446\n",
      "258.57123\n",
      "263.439\n",
      "-70.78032\n"
     ]
    }
   ],
   "source": [
    "next_obs, _ = envs.reset()\n",
    "total_rewards = []\n",
    "episodes = 10\n",
    "\n",
    "while len(total_rewards) < episodes: \n",
    "    next_obs = torch.Tensor(next_obs)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "    if 'final_info' in info:\n",
    "        for data in info['final_info']:\n",
    "            if data:\n",
    "                reward = data['episode']['r'][0]\n",
    "                print(reward)\n",
    "                total_rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AsyncVectorEnv.__del__ at 0x132e0e310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 546, in __del__\n",
      "    self.close(terminate=True)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/vector_env.py\", line 265, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 461, in close_extras\n",
      "    function(timeout)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 321, in step_wait\n",
      "    obs, rew, terminated, truncated, info = result\n",
      "TypeError: cannot unpack non-iterable NoneType object\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.542747\n",
      "307.70154\n",
      "304.33347\n",
      "-23.915497\n",
      "305.6151\n",
      "304.5836\n",
      "302.65884\n",
      "303.71445\n",
      "305.96878\n",
      "303.6156\n"
     ]
    }
   ],
   "source": [
    "ENV_ARGS['render_mode'] = 'human'\n",
    "envs1 = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "next_obs, _ = envs1.reset()\n",
    "total_rewards = []\n",
    "episodes = 10\n",
    "\n",
    "while len(total_rewards) < episodes: \n",
    "    next_obs = torch.Tensor(next_obs)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = envs1.step(action.numpy())\n",
    "\n",
    "    if 'final_info' in info:\n",
    "        for data in info['final_info']:\n",
    "            if data:\n",
    "                reward = data['episode']['r'][0]\n",
    "                print(reward)\n",
    "                total_rewards.append(reward)\n",
    "    \n",
    "del ENV_ARGS['render_mode']\n",
    "envs1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
