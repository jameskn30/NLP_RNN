{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "https://github.com/vwxyzjn/cleanrl?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': \"InvertedPendulum-v4\"\n",
    "}\n",
    "NUM_ENVS = 3\n",
    "SEED = 1\n",
    "LR = 1e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output', ENV_ARGS['id'])\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gamma, **env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.ClipAction(env)\n",
    "    # env = gym.wrappers.NormalizeObservation(env)\n",
    "    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    env = gym.wrappers.NormalizeReward(env, gamma = gamma)\n",
    "    env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.Env, hidden_size: int = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_shape = np.array(envs.single_observation_space.shape).prod()\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            #NOTE: why use tanh here? \n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(layer = nn.Linear(hidden_size, self.action_shape), std = 0.01),\n",
    "        )\n",
    "\n",
    "        #shape = (1, state_shape)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, self.action_shape, dtype=torch.float))\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(nn.Linear(hidden_size, 1), std = 1.0),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action = None):\n",
    "        '''\n",
    "        @params:\n",
    "            x: torch.tensor observation, shape = (N, observation size)\n",
    "            action: torch.tensor action\n",
    "        @returns:\n",
    "            action: torch.tensor, shape = (N, action size)\n",
    "            log_prob: torch.tensor, shape = (N,)\n",
    "            entropy: torch.tensor, shape = (N,)\n",
    "            value: torch.tensor, shape = (N,)\n",
    "        '''\n",
    "        action_mean = self.actor_mean(x)\n",
    "        #make action logstd the shape[0] with mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        #exponential trick to remove log\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        probs = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample() \n",
    "        \n",
    "        #get value from critic\n",
    "        value = self.get_value(x)\n",
    "        log_prob = probs.log_prob(action).sum(1)\n",
    "        #entropy for regularization\n",
    "        entropy = probs.entropy().sum(1)\n",
    "        \n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape =  torch.Size([3, 4])\n",
      "action shape =  (1,)\n",
      "log prob shape =  torch.Size([3])\n",
      "entropy shape =  torch.Size([3])\n",
      "value shape =  torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#Test agent\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "obs, info = envs.reset()\n",
    "\n",
    "test_agent = Agent(envs)\n",
    "\n",
    "obs = torch.tensor(obs).float()\n",
    "\n",
    "action, log_prob, entropy, value = test_agent.get_action_and_value(obs)\n",
    "\n",
    "print('state shape = ', obs.shape)\n",
    "print('action shape = ', envs.single_action_space.shape)\n",
    "print('log prob shape = ', log_prob.shape)\n",
    "print('entropy shape = ', entropy.shape)\n",
    "print('value shape = ', value.shape)\n",
    "\n",
    "del test_agent\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, episodes = 10):\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(envs, agent, optimizer, num_steps, num_envs, \n",
    "         num_iterations = 1000, update_epochs = 10, \n",
    "         label = 'test', plot_udpate_freq = 10, history = None):\n",
    "\n",
    "    # label = str(uuid.uuid4()).split('-')[0]\n",
    "    print('run id = ', label)\n",
    "    SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "    FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        print(f'output folder: {SAVE_PATH}')\n",
    "        os.makedirs(SAVE_PATH)\n",
    "    print('save path = ', SAVE_PATH)\n",
    "\n",
    "    M,N = num_steps, num_envs\n",
    "\n",
    "    obs = torch.zeros((M, N) + envs.single_observation_space.shape)\n",
    "    actions = torch.zeros((M,N) + envs.single_action_space.shape)\n",
    "    log_probs = torch.zeros((M,N))\n",
    "    rewards = torch.zeros((M,N))\n",
    "    dones = torch.zeros((M,N)) # for masking\n",
    "    values = torch.zeros((M,N))\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    #Reset env\n",
    "    next_obs, _ = envs.reset()\n",
    "    next_obs = torch.tensor(next_obs).float()\n",
    "    next_done = torch.zeros(N) #N is num envs\n",
    "\n",
    "    print('next obs = ', next_obs.shape)\n",
    "    print('next done = ', next_done.shape)\n",
    "\n",
    "    reward_window = deque(maxlen = 100)\n",
    "\n",
    "    if history == None:\n",
    "        history = defaultdict(list)\n",
    "\n",
    "    loop = tqdm(range(num_iterations))\n",
    "    agent.train()\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    loss = float('inf')\n",
    "\n",
    "    for iter in loop:\n",
    "\n",
    "        #ROLLOUT phase\n",
    "        #M is max steps\n",
    "        if iter % plot_udpate_freq == 0:\n",
    "            plot(history, save_path=FIG_SAVE_PATH)\n",
    "\n",
    "        for step in range(M):\n",
    "            global_step += N\n",
    "\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            #get action\n",
    "            #NOTE: no_grad disables gradient calculation --> reduce memory consumption\n",
    "            #the result of every computation will have requires_grad=False\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "\n",
    "            actions[step] = action\n",
    "            log_probs[step] = log_prob\n",
    "\n",
    "            #make next step with actions\n",
    "            next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "            next_done = np.logical_or(terminated, truncated)\n",
    "\n",
    "            #NOTE: difference between view and reshape\n",
    "            # https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n",
    "            rewards[step] = torch.tensor(reward).view(-1)\n",
    "            next_obs = torch.tensor(next_obs).float()\n",
    "            next_done = torch.tensor(next_done).float()\n",
    "\n",
    "            #NOTE: vector envs will automatically reset, so no need to break \n",
    "            if 'final_info' in info:\n",
    "                for data in info['final_info']:\n",
    "                    if data:\n",
    "                        reward = data['episode']['r']\n",
    "                        reward_window.append(reward)\n",
    "                        avg_reward = np.mean(reward_window)\n",
    "                        history['reward'].append(avg_reward)\n",
    "                        loop.set_description(f\"reward = {avg_reward:.2f}, global_step = {global_step}, best_score = {best_score:.2f}, loss={loss:.2f}\")\n",
    "\n",
    "                        if best_score < avg_reward:\n",
    "                            best_score = avg_reward\n",
    "                            #save model\n",
    "                            torch.save(agent, os.path.join(SAVE_PATH, 'ppo.checkpoint.torch'))\n",
    "            \n",
    "        #update the history for plotting, and printing progress\n",
    "\n",
    "        #OPTIMIZE phase:\n",
    "        with torch.no_grad():\n",
    "            #bootstrap values, compute returns\n",
    "            next_value = agent.get_value(next_obs).reshape(1,-1)\n",
    "            advantages = torch.zeros_like(rewards)\n",
    "            last_gae_lambda = 0\n",
    "\n",
    "            for t in reversed(range(NUM_STEPS)):\n",
    "                if t == NUM_STEPS - 1:\n",
    "                    next_none_terminal = np.logical_not(next_done)\n",
    "                    next_values = next_value\n",
    "                else:\n",
    "                    next_none_terminal = np.logical_not(dones[t + 1])\n",
    "                    next_values = values[t + 1]\n",
    "                \n",
    "                #A(s,a) = Q(s,a) - V(s,a) = r(t) + gamma * V(s', a) * mask - V(s)\n",
    "                delta = rewards[t] + GAMMA * next_values * next_none_terminal - values[t]\n",
    "                #NOTE: learn about this formula\n",
    "                advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_none_terminal * last_gae_lambda\n",
    "            returns = advantages + values\n",
    "        \n",
    "        #flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_log_probs = log_probs.reshape(-1)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        #NOTE: randomize the batch to break correlation\n",
    "        batch_size = M * N\n",
    "        mini_batch_size = batch_size // MINI_BATCH_COUNT\n",
    "        b_indicies = np.arange(batch_size)\n",
    "        clip_fracs = []\n",
    "        \n",
    "        for _ in range(update_epochs):\n",
    "            np.random.shuffle(b_indicies)\n",
    "\n",
    "            #NOTE: mini-batch update: \n",
    "            # pros: reduce memory usage, faster updates\n",
    "            # pros: a whole batch may stuck in local minima, mini batches introduce randomness\n",
    "            # cons: estimate a true gradient, larger mini batch size --> more accurate but more memory\n",
    "            for start in range(0, batch_size, mini_batch_size):\n",
    "                end = start + mini_batch_size\n",
    "                mini_indicies = b_indicies[start:end]\n",
    "\n",
    "                _, new_log_prob, entropy, new_value = agent.get_action_and_value(b_obs[mini_indicies], b_actions[mini_indicies])\n",
    "\n",
    "                #NOTE: what formula is this? \n",
    "                log_ratio = new_log_prob - b_log_probs[mini_indicies]\n",
    "\n",
    "                ratio = log_ratio.exp() # trick to remove log\n",
    "\n",
    "                #compute approximate KL: http://joschu.net/blog/kl-approx.html\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kd = (-log_ratio).mean()\n",
    "                    approximate_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                    clip_fracs += [((ratio - 1.0).abs() > CLIP_COEF).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mini_indicies]\n",
    "\n",
    "                #normalize advantage\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                #policy loss (actor)\n",
    "\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2= -mb_advantages * torch.clamp(ratio, 1 - CLIP_COEF, 1 + CLIP_COEF)\n",
    "\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                new_value = new_value.view(-1)\n",
    "\n",
    "                #value loss (MSE)\n",
    "                v_loss = 0.5 * ((new_value - b_returns[mini_indicies]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = pg_loss - ENTROPY_COEF * entropy_loss + v_loss * VF_COEF\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #clip grad\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "        \n",
    "    torch.save(agent, os.path.join(SAVE_PATH, 'ppo.final.torch'))\n",
    "    plot(history, show=True, save_path=FIG_SAVE_PATH)\n",
    "\n",
    "    with open(os.path.jion(SAVE_PATH, 'history.pickle'), 'wb') as file:\n",
    "        pickle.dump(history, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id =  baseline\n",
      "output folder: e:\\ML\\NLP\\Reinforcement Learning\\policy_gradient\\final_project\\output\\InvertedPendulum-v4\\baseline\n",
      "save path =  e:\\ML\\NLP\\Reinforcement Learning\\policy_gradient\\final_project\\output\\InvertedPendulum-v4\\baseline\n",
      "next obs =  torch.Size([3, 4])\n",
      "next done =  torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward = 9.64, global_step = 7662, best_score = 9.65, loss=0.10:   1%|          | 1/100 [00:06<10:04,  6.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m M \u001b[38;5;241m=\u001b[39m NUM_STEPS\n\u001b[0;32m     12\u001b[0m N \u001b[38;5;241m=\u001b[39m NUM_ENVS\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_ENVS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 86\u001b[0m, in \u001b[0;36mtune\u001b[1;34m(envs, agent, optimizer, num_steps, num_envs, num_iterations, update_epochs, label, plot_udpate_freq, history)\u001b[0m\n\u001b[0;32m     84\u001b[0m avg_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(reward_window)\n\u001b[0;32m     85\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(avg_reward)\n\u001b[1;32m---> 86\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_description\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreward = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mavg_reward\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.2f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, global_step = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mglobal_step\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, best_score = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbest_score\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.2f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, loss=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mloss\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.2f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_score \u001b[38;5;241m<\u001b[39m avg_reward:\n\u001b[0;32m     89\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m avg_reward\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\std.py:1391\u001b[0m, in \u001b[0;36mtqdm.set_description\u001b[1;34m(self, desc, refresh)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc \u001b[38;5;241m=\u001b[39m desc \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m desc \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refresh:\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\std.py:1344\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1343\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1344\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\std.py:1492\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1492\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\std.py:347\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    346\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 347\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\std.py:341\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m    340\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[1;32m--> 341\u001b[0m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\utils.py:127\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel\\iostream.py:480\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'\n",
    "\n",
    "agent = Agent(envs)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr = 1e-4, eps = 1e-5)\n",
    "\n",
    "M = NUM_STEPS\n",
    "N = NUM_ENVS\n",
    "\n",
    "tune(envs, agent, optimizer, NUM_STEPS, NUM_ENVS, num_iterations=100, label='baseline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, device = DEVICE, episodes = 10):\n",
    "\n",
    "    agent = torch.load(model_path)\n",
    "\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(**ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs).to(device)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    print('reward = ', reward)\n",
    "                    total_rewards.append(reward)\n",
    "    \n",
    "    sns.lineplot(y = total_rewards, x = list(range(len(total_rewards))))\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_env() missing 1 required positional argument: 'gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo.checkpoint.torch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model_path, device, episodes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model_path, device \u001b[38;5;241m=\u001b[39m DEVICE, episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m     agent \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m----> 5\u001b[0m     envs \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSyncVectorEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mENV_ARGS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     agent\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:53\u001b[0m, in \u001b[0;36mSyncVectorEnv.__init__\u001b[1;34m(self, env_fns, observation_space, action_space, copy)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [env_fn() \u001b[38;5;28;01mfor\u001b[39;00m env_fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [\u001b[43menv_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env_fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36mevaluate.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model_path, device \u001b[38;5;241m=\u001b[39m DEVICE, episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m     agent \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[1;32m----> 5\u001b[0m     envs \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mvector\u001b[38;5;241m.\u001b[39mSyncVectorEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: make_env(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mENV_ARGS)])\n\u001b[0;32m      6\u001b[0m     agent\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      7\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: make_env() missing 1 required positional argument: 'gamma'"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(OUTPUT, 'test', 'ppo.checkpoint.torch')\n",
    "evaluate(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268.0151\n",
      "266.8834\n",
      "260.6238\n",
      "75.81549\n",
      "267.05634\n",
      "263.5786\n",
      "266.1446\n",
      "258.57123\n",
      "263.439\n",
      "-70.78032\n"
     ]
    }
   ],
   "source": [
    "next_obs, _ = envs.reset()\n",
    "total_rewards = []\n",
    "episodes = 10\n",
    "\n",
    "while len(total_rewards) < episodes: \n",
    "    next_obs = torch.Tensor(next_obs)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "    if 'final_info' in info:\n",
    "        for data in info['final_info']:\n",
    "            if data:\n",
    "                reward = data['episode']['r'][0]\n",
    "                print(reward)\n",
    "                total_rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
