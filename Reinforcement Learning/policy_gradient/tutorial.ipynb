{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "from collections import deque, defaultdict\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value\n",
    "\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def trainIters(env, actor, critic, n_iters, device):\n",
    "    optimizerA = optim.Adam(actor.parameters())\n",
    "    optimizerC = optim.Adam(critic.parameters())\n",
    "    loop = tqdm(range(n_iters))\n",
    "    reward_window = deque(maxlen=100)\n",
    "    history = defaultdict(list)\n",
    "\n",
    "    for iter in loop:\n",
    "        state, info = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "            total_reward += reward\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = compute_returns(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n",
    "\n",
    "        reward_window.append(total_reward)\n",
    "\n",
    "        avg_reward = np.mean(reward_window)\n",
    "\n",
    "        history['reward'].append(avg_reward)\n",
    "\n",
    "        loop.set_description(f'avg reward = {avg_reward}')\n",
    "\n",
    "    env.close()\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/envs/box2d/lunar_lander.py:606: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"LunarLander-v2\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "avg reward = -55.46798511919255:   6%|â–Œ         | 608/10000 [05:35<1:26:16,  1.81it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m actor \u001b[38;5;241m=\u001b[39m Actor(state_size, action_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m critic \u001b[38;5;241m=\u001b[39m Critic(state_size, action_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 65\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(env, actor, critic, n_iters, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m dist, value \u001b[38;5;241m=\u001b[39m actor(state), critic(state)\n\u001b[1;32m     64\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 65\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     68\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/envs/box2d/lunar_lander.py:558\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    547\u001b[0m     p\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    548\u001b[0m         (ox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, oy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    549\u001b[0m         impulse_pos,\n\u001b[1;32m    550\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    553\u001b[0m         (\u001b[39m-\u001b[39mox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, \u001b[39m-\u001b[39moy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    554\u001b[0m         impulse_pos,\n\u001b[1;32m    555\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    556\u001b[0m     )\n\u001b[0;32m--> 558\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworld\u001b[39m.\u001b[39;49mStep(\u001b[39m1.0\u001b[39;49m \u001b[39m/\u001b[39;49m FPS, \u001b[39m6\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m)\n\u001b[1;32m    560\u001b[0m pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mposition\n\u001b[1;32m    561\u001b[0m vel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mlinearVelocity\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"LunarLander-v2\").unwrapped\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "epochs = 10000\n",
    "\n",
    "\n",
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "history = trainIters(env, actor, critic, n_iters=epochs, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
