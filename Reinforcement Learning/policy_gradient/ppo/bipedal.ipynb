{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "https://github.com/vwxyzjn/cleanrl?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x169663e30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': 'BipedalWalker-v3'\n",
    "}\n",
    "NUM_ENVS = 3\n",
    "SEED = 1\n",
    "LR = 3e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output')\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gamma, **env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.ClipAction(env)\n",
    "    # env = gym.wrappers.NormalizeObservation(env)\n",
    "    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    # env = gym.wrappers.NormalizeReward(env, gamma = gamma)\n",
    "    # env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.Env, hidden_size: int = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_shape = np.array(envs.single_observation_space.shape).prod()\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            #NOTE: why use tanh here? \n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(layer = nn.Linear(hidden_size, self.action_shape), std = 0.01),\n",
    "        )\n",
    "\n",
    "        #shape = (1, state_shape)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, self.action_shape))\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(nn.Linear(hidden_size, 1), std = 1.0),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action = None):\n",
    "        '''\n",
    "        @params:\n",
    "            x: torch.tensor observation, shape = (N, observation size)\n",
    "            action: torch.tensor action\n",
    "        @returns:\n",
    "            action: torch.tensor, shape = (N, action size)\n",
    "            log_prob: torch.tensor, shape = (N,)\n",
    "            entropy: torch.tensor, shape = (N,)\n",
    "            value: torch.tensor, shape = (N,)\n",
    "        '''\n",
    "        action_mean = self.actor_mean(x)\n",
    "        #make action logstd the shape[0] with mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        #exponential trick to remove log\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        probs = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample() \n",
    "        \n",
    "        #get value from critic\n",
    "        value = self.get_value(x)\n",
    "        log_prob = probs.log_prob(action).sum(1)\n",
    "        #entropy for regularization\n",
    "        entropy = probs.entropy().sum(1)\n",
    "        \n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action shape =  torch.Size([3, 4])\n",
      "log prob shape =  torch.Size([3])\n",
      "entropy shape =  torch.Size([3])\n",
      "value shape =  torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#Test agent\n",
    "test_agent = Agent(envs)\n",
    "\n",
    "sample_state = torch.rand(3, 24)\n",
    "\n",
    "action, log_prob, entropy, value = test_agent.get_action_and_value(sample_state)\n",
    "\n",
    "print('action shape = ', action.shape)\n",
    "print('log prob shape = ', log_prob.shape)\n",
    "print('entropy shape = ', entropy.shape)\n",
    "print('value shape = ', value.shape)\n",
    "\n",
    "del test_agent\n",
    "del envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, episodes = 10):\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id =  f6247724\n",
      "output folder: /Volumes/SanDisk/NLP_RNN/Reinforcement Learning/policy_gradient/ppo/output/f6247724\n",
      "next obs =  torch.Size([3, 24])\n",
      "next done =  torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward = -72.94, global_step = 195420, best_score = -73.85, loss=0.03:   3%|â–Ž         | 31/1000 [01:01<32:09,  1.99s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m         delta \u001b[38;5;241m=\u001b[39m rewards[t] \u001b[38;5;241m+\u001b[39m GAMMA \u001b[38;5;241m*\u001b[39m next_values \u001b[38;5;241m*\u001b[39m next_none_terminal \u001b[38;5;241m-\u001b[39m values[t]\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m#NOTE: learn about this formula\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m         advantages[t] \u001b[38;5;241m=\u001b[39m last_gae_lambda \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m+\u001b[39m \u001b[43mGAMMA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGAE_LAMBDA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnext_none_terminal\u001b[49m \u001b[38;5;241m*\u001b[39m last_gae_lambda\n\u001b[1;32m    120\u001b[0m     returns \u001b[38;5;241m=\u001b[39m advantages \u001b[38;5;241m+\u001b[39m values\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#flatten the batch\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'\n",
    "\n",
    "agent = Agent(envs)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr = LR, eps = 1e-5)\n",
    "\n",
    "M = NUM_STEPS\n",
    "N = NUM_ENVS\n",
    "\n",
    "label = str(uuid.uuid4()).split('-')[0]\n",
    "print('run id = ', label)\n",
    "\n",
    "SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "if os.path.exists(SAVE_PATH) == False:\n",
    "    print(f'output folder: {SAVE_PATH}')\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "obs = torch.zeros((M, N) + envs.single_observation_space.shape)\n",
    "actions = torch.zeros((M,N) + envs.single_action_space.shape)\n",
    "log_probs = torch.zeros((M,N))\n",
    "rewards = torch.zeros((M,N))\n",
    "dones = torch.zeros((M,N)) # for masking\n",
    "values = torch.zeros((M,N))\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "next_obs, _ = envs.reset()\n",
    "next_obs = torch.Tensor(next_obs)\n",
    "next_done = torch.zeros(N) #N is num envs\n",
    "\n",
    "print('next obs = ', next_obs.shape)\n",
    "print('next done = ', next_done.shape)\n",
    "\n",
    "reward_window = deque(maxlen = 100)\n",
    "history = defaultdict(list)\n",
    "\n",
    "loop = tqdm(range(NUM_ITERATIONS))\n",
    "agent.train()\n",
    "\n",
    "best_score = -float('inf')\n",
    "evaluation = 0\n",
    "loss = float('inf')\n",
    "\n",
    "for iter in loop:\n",
    "\n",
    "    #ROLLOUT phase\n",
    "    #M is max steps\n",
    "    if iter % UPDATE_PLOTS == 0:\n",
    "        plot(history, save_path=FIG_SAVE_PATH)\n",
    "\n",
    "    for step in range(M):\n",
    "        global_step += N\n",
    "\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        #get action\n",
    "        #NOTE: no_grad disables gradient calculation --> reduce memory consumption\n",
    "        #the result of every computation will have requires_grad=False\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "\n",
    "        actions[step] = action\n",
    "        log_probs[step] = log_prob\n",
    "\n",
    "        #make next step with actions\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        next_done = np.logical_or(terminated, truncated)\n",
    "\n",
    "        #NOTE: difference between view and reshape\n",
    "        # https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n",
    "        rewards[step] = torch.tensor(reward).view(-1)\n",
    "        next_obs = torch.tensor(next_obs)\n",
    "        next_done = torch.tensor(next_done)\n",
    "\n",
    "        #NOTE: vector envs will automatically reset, so no need to break \n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r']\n",
    "                    reward_window.append(reward)\n",
    "                    avg_reward = np.mean(reward_window)\n",
    "                    history['reward'].append(avg_reward)\n",
    "                    loop.set_description(f\"reward = {avg_reward:.2f}, global_step = {global_step}, best_score = {best_score:.2f}, loss={loss:.2f}\")\n",
    "\n",
    "                    if best_score < avg_reward:\n",
    "                        best_score = avg_reward\n",
    "                        #save model\n",
    "                        torch.save(agent, os.path.join(SAVE_PATH, 'ppo.checkpoint.torch'))\n",
    "\n",
    "        \n",
    "    #update the history for plotting, and printing progress\n",
    "\n",
    "    #OPTIMIZE phase:\n",
    "    with torch.no_grad():\n",
    "        #bootstrap values, compute returns\n",
    "        next_value = agent.get_value(next_obs).reshape(1,-1)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_gae_lambda = 0\n",
    "\n",
    "        for t in reversed(range(NUM_STEPS)):\n",
    "            if t == NUM_STEPS - 1:\n",
    "                next_none_terminal = np.logical_not(next_done)\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_none_terminal = np.logical_not(dones[t + 1])\n",
    "                next_values = values[t + 1]\n",
    "            \n",
    "            #A(s,a) = Q(s,a) - V(s,a) = r(t) + gamma * V(s', a) * mask - V(s)\n",
    "            delta = rewards[t] + GAMMA * next_values * next_none_terminal - values[t]\n",
    "            #NOTE: learn about this formula\n",
    "            advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_none_terminal * last_gae_lambda\n",
    "        returns = advantages + values\n",
    "    \n",
    "    #flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_log_probs = log_probs.reshape(-1)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    #NOTE: randomize the batch to break correlation\n",
    "    batch_size = M * N\n",
    "    mini_batch_size = batch_size // MINI_BATCH_COUNT\n",
    "    b_indicies = np.arange(batch_size)\n",
    "    clip_fracs = []\n",
    "    \n",
    "    for epoch in range(UPDATE_EPOCHS):\n",
    "        np.random.shuffle(b_indicies)\n",
    "\n",
    "        #NOTE: mini-batch update: \n",
    "        # pros: reduce memory usage, faster updates\n",
    "        # pros: a whole batch may stuck in local minima, mini batches introduce randomness\n",
    "        # cons: estimate a true gradient, larger mini batch size --> more accurate but more memory\n",
    "        for start in range(0, batch_size, mini_batch_size):\n",
    "            end = start + mini_batch_size\n",
    "            mini_indicies = b_indicies[start:end]\n",
    "\n",
    "            _, new_log_prob, entropy, new_value = agent.get_action_and_value(b_obs[mini_indicies], b_actions[mini_indicies])\n",
    "\n",
    "            #NOTE: what formula is this? \n",
    "            log_ratio = new_log_prob - b_log_probs[mini_indicies]\n",
    "\n",
    "            ratio = log_ratio.exp() # trick to remove log\n",
    "\n",
    "            #compute approximate KL: http://joschu.net/blog/kl-approx.html\n",
    "            with torch.no_grad():\n",
    "                old_approx_kd = (-log_ratio).mean()\n",
    "                approximate_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                clip_fracs += [((ratio - 1.0).abs() > CLIP_COEF).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mini_indicies]\n",
    "\n",
    "            #normalize advantage\n",
    "            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            #policy loss (actor)\n",
    "\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2= -mb_advantages * torch.clamp(ratio, 1 - CLIP_COEF, 1 + CLIP_COEF)\n",
    "\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            new_value = new_value.view(-1)\n",
    "\n",
    "            #value loss (MSE)\n",
    "            v_loss = 0.5 * ((new_value - b_returns[mini_indicies]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "\n",
    "            loss = pg_loss - ENTROPY_COEF * entropy_loss + v_loss * VF_COEF\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #clip grad\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "    \n",
    "evaluation = np.mean(evaluate(agent))\n",
    "print('evaluation = ', np.mean(evaluation))\n",
    "torch.save(agent, os.path.join(SAVE_PATH, 'ppo.final.torch'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268.0151\n",
      "266.8834\n",
      "260.6238\n",
      "75.81549\n",
      "267.05634\n",
      "263.5786\n",
      "266.1446\n",
      "258.57123\n",
      "263.439\n",
      "-70.78032\n"
     ]
    }
   ],
   "source": [
    "next_obs, _ = envs.reset()\n",
    "total_rewards = []\n",
    "episodes = 10\n",
    "\n",
    "while len(total_rewards) < episodes: \n",
    "    next_obs = torch.Tensor(next_obs)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "    if 'final_info' in info:\n",
    "        for data in info['final_info']:\n",
    "            if data:\n",
    "                reward = data['episode']['r'][0]\n",
    "                print(reward)\n",
    "                total_rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AsyncVectorEnv.__del__ at 0x132e0e310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 546, in __del__\n",
      "    self.close(terminate=True)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/vector_env.py\", line 265, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 461, in close_extras\n",
      "    function(timeout)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 321, in step_wait\n",
      "    obs, rew, terminated, truncated, info = result\n",
      "TypeError: cannot unpack non-iterable NoneType object\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.542747\n",
      "307.70154\n",
      "304.33347\n",
      "-23.915497\n",
      "305.6151\n",
      "304.5836\n",
      "302.65884\n",
      "303.71445\n",
      "305.96878\n",
      "303.6156\n"
     ]
    }
   ],
   "source": [
    "ENV_ARGS['render_mode'] = 'human'\n",
    "envs1 = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "next_obs, _ = envs1.reset()\n",
    "total_rewards = []\n",
    "episodes = 10\n",
    "\n",
    "while len(total_rewards) < episodes: \n",
    "    next_obs = torch.Tensor(next_obs)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = envs1.step(action.numpy())\n",
    "\n",
    "    if 'final_info' in info:\n",
    "        for data in info['final_info']:\n",
    "            if data:\n",
    "                reward = data['episode']['r'][0]\n",
    "                print(reward)\n",
    "                total_rewards.append(reward)\n",
    "    \n",
    "del ENV_ARGS['render_mode']\n",
    "envs1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
