{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "https://github.com/vwxyzjn/cleanrl?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1892fb299f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': 'LunarLander-v2',\n",
    "    'continuous': True\n",
    "}\n",
    "NUM_ENVS = 3\n",
    "SEED = 1\n",
    "LR = 3e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output')\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gamma, **env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.ClipAction(env)\n",
    "    # env = gym.wrappers.NormalizeObservation(env)\n",
    "    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    env = gym.wrappers.NormalizeReward(env, gamma = gamma)\n",
    "    env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, run `pip install gymnasium[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test env\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m envs \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSyncVectorEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mENV_ARGS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUM_ENVS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#check to make sure this is continous action\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(envs\u001b[38;5;241m.\u001b[39msingle_action_space, gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly continous action is supported\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:53\u001b[0m, in \u001b[0;36mSyncVectorEnv.__init__\u001b[1;34m(self, env_fns, observation_space, action_space, copy)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [env_fn() \u001b[38;5;28;01mfor\u001b[39;00m env_fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Vectorized environment that serially runs multiple environments.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m        (or, by default, the observation space of the first sub-environment).\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [\u001b[43menv_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env_fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test env\u001b[39;00m\n\u001b[0;32m      2\u001b[0m envs \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mvector\u001b[38;5;241m.\u001b[39mSyncVectorEnv(\n\u001b[1;32m----> 3\u001b[0m     [\u001b[38;5;28;01mlambda\u001b[39;00m : make_env(gamma\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mENV_ARGS) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_ENVS)]\n\u001b[0;32m      4\u001b[0m ) \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#check to make sure this is continous action\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(envs\u001b[38;5;241m.\u001b[39msingle_action_space, gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly continous action is supported\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mmake_env\u001b[1;34m(gamma, **env_args)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_env\u001b[39m(gamma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_args):\n\u001b[1;32m----> 2\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_args)\n\u001b[0;32m      3\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mFlattenObservation(env)\n\u001b[0;32m      4\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mRecordEpisodeStatistics(env)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\envs\\registration.py:755\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    752\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[0;32m    758\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\envs\\registration.py:553\u001b[0m, in \u001b[0;36mload_env_creator\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    552\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 553\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:972\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\envs\\box2d\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:25\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox2D is not installed, run `pip install gymnasium[box2d]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, run `pip install gymnasium[box2d]`"
     ]
    }
   ],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.Env, hidden_size: int = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_shape = np.array(envs.single_observation_space.shape).prod()\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            #NOTE: why use tanh here? \n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(layer = nn.Linear(hidden_size, self.action_shape), std = 0.01),\n",
    "        )\n",
    "\n",
    "        #shape = (1, state_shape)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, self.action_shape))\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.ReLU(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(nn.Linear(hidden_size, 1), std = 1.0),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action = None):\n",
    "        '''\n",
    "        @params:\n",
    "            x: torch.tensor observation, shape = (N, observation size)\n",
    "            action: torch.tensor action\n",
    "        @returns:\n",
    "            action: torch.tensor, shape = (N, action size)\n",
    "            log_prob: torch.tensor, shape = (N,)\n",
    "            entropy: torch.tensor, shape = (N,)\n",
    "            value: torch.tensor, shape = (N,)\n",
    "        '''\n",
    "        action_mean = self.actor_mean(x)\n",
    "        #make action logstd the shape[0] with mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        #exponential trick to remove log\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        probs = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample() \n",
    "        \n",
    "        #get value from critic\n",
    "        value = self.get_value(x)\n",
    "        log_prob = probs.log_prob(action).sum(1)\n",
    "        #entropy for regularization\n",
    "        entropy = probs.entropy().sum(1)\n",
    "        \n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action shape =  torch.Size([3, 4])\n",
      "log prob shape =  torch.Size([3])\n",
      "entropy shape =  torch.Size([3])\n",
      "value shape =  torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#Test agent\n",
    "test_agent = Agent(envs)\n",
    "\n",
    "sample_state = torch.rand(3, 24)\n",
    "\n",
    "action, log_prob, entropy, value = test_agent.get_action_and_value(sample_state)\n",
    "\n",
    "print('action shape = ', action.shape)\n",
    "print('log prob shape = ', log_prob.shape)\n",
    "print('entropy shape = ', entropy.shape)\n",
    "print('value shape = ', value.shape)\n",
    "\n",
    "del test_agent\n",
    "del envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, episodes = 10):\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id =  f6247724\n",
      "output folder: /Volumes/SanDisk/NLP_RNN/Reinforcement Learning/policy_gradient/ppo/output/f6247724\n",
      "next obs =  torch.Size([3, 24])\n",
      "next done =  torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward = -72.94, global_step = 195420, best_score = -73.85, loss=0.03:   3%|â–Ž         | 31/1000 [01:01<32:09,  1.99s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m         delta \u001b[38;5;241m=\u001b[39m rewards[t] \u001b[38;5;241m+\u001b[39m GAMMA \u001b[38;5;241m*\u001b[39m next_values \u001b[38;5;241m*\u001b[39m next_none_terminal \u001b[38;5;241m-\u001b[39m values[t]\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m#NOTE: learn about this formula\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m         advantages[t] \u001b[38;5;241m=\u001b[39m last_gae_lambda \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m+\u001b[39m \u001b[43mGAMMA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGAE_LAMBDA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnext_none_terminal\u001b[49m \u001b[38;5;241m*\u001b[39m last_gae_lambda\n\u001b[1;32m    120\u001b[0m     returns \u001b[38;5;241m=\u001b[39m advantages \u001b[38;5;241m+\u001b[39m values\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#flatten the batch\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'\n",
    "\n",
    "agent = Agent(envs)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr = LR, eps = 1e-5)\n",
    "\n",
    "M = NUM_STEPS\n",
    "N = NUM_ENVS\n",
    "\n",
    "label = str(uuid.uuid4()).split('-')[0]\n",
    "print('run id = ', label)\n",
    "\n",
    "SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "if os.path.exists(SAVE_PATH) == False:\n",
    "    print(f'output folder: {SAVE_PATH}')\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "obs = torch.zeros((M, N) + envs.single_observation_space.shape)\n",
    "actions = torch.zeros((M,N) + envs.single_action_space.shape)\n",
    "log_probs = torch.zeros((M,N))\n",
    "rewards = torch.zeros((M,N))\n",
    "dones = torch.zeros((M,N)) # for masking\n",
    "values = torch.zeros((M,N))\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "next_obs, _ = envs.reset()\n",
    "next_obs = torch.Tensor(next_obs)\n",
    "next_done = torch.zeros(N) #N is num envs\n",
    "\n",
    "print('next obs = ', next_obs.shape)\n",
    "print('next done = ', next_done.shape)\n",
    "\n",
    "reward_window = deque(maxlen = 100)\n",
    "history = defaultdict(list)\n",
    "\n",
    "loop = tqdm(range(NUM_ITERATIONS))\n",
    "agent.train()\n",
    "\n",
    "best_score = -float('inf')\n",
    "evaluation = 0\n",
    "loss = float('inf')\n",
    "\n",
    "for iter in loop:\n",
    "\n",
    "    #ROLLOUT phase\n",
    "    #M is max steps\n",
    "    if iter % UPDATE_PLOTS == 0:\n",
    "        plot(history, save_path=FIG_SAVE_PATH)\n",
    "\n",
    "    for step in range(M):\n",
    "        global_step += N\n",
    "\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        #get action\n",
    "        #NOTE: no_grad disables gradient calculation --> reduce memory consumption\n",
    "        #the result of every computation will have requires_grad=False\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "\n",
    "        actions[step] = action\n",
    "        log_probs[step] = log_prob\n",
    "\n",
    "        #make next step with actions\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        next_done = np.logical_or(terminated, truncated)\n",
    "\n",
    "        #NOTE: difference between view and reshape\n",
    "        # https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n",
    "        rewards[step] = torch.tensor(reward).view(-1)\n",
    "        next_obs = torch.tensor(next_obs)\n",
    "        next_done = torch.tensor(next_done)\n",
    "\n",
    "        #NOTE: vector envs will automatically reset, so no need to break \n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r']\n",
    "                    reward_window.append(reward)\n",
    "                    avg_reward = np.mean(reward_window)\n",
    "                    history['reward'].append(avg_reward)\n",
    "                    loop.set_description(f\"reward = {avg_reward:.2f}, global_step = {global_step}, best_score = {best_score:.2f}, loss={loss:.2f}\")\n",
    "\n",
    "                    if best_score < avg_reward:\n",
    "                        best_score = avg_reward\n",
    "                        #save model\n",
    "                        torch.save(agent, os.path.join(SAVE_PATH, 'ppo.checkpoint.torch'))\n",
    "\n",
    "        \n",
    "    #update the history for plotting, and printing progress\n",
    "\n",
    "    #OPTIMIZE phase:\n",
    "    with torch.no_grad():\n",
    "        #bootstrap values, compute returns\n",
    "        next_value = agent.get_value(next_obs).reshape(1,-1)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_gae_lambda = 0\n",
    "\n",
    "        for t in reversed(range(NUM_STEPS)):\n",
    "            if t == NUM_STEPS - 1:\n",
    "                next_none_terminal = np.logical_not(next_done)\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_none_terminal = np.logical_not(dones[t + 1])\n",
    "                next_values = values[t + 1]\n",
    "            \n",
    "            #A(s,a) = Q(s,a) - V(s,a) = r(t) + gamma * V(s', a) * mask - V(s)\n",
    "            delta = rewards[t] + GAMMA * next_values * next_none_terminal - values[t]\n",
    "            #NOTE: learn about this formula\n",
    "            advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_none_terminal * last_gae_lambda\n",
    "        returns = advantages + values\n",
    "    \n",
    "    #flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_log_probs = log_probs.reshape(-1)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    #NOTE: randomize the batch to break correlation\n",
    "    batch_size = M * N\n",
    "    mini_batch_size = batch_size // MINI_BATCH_COUNT\n",
    "    b_indicies = np.arange(batch_size)\n",
    "    clip_fracs = []\n",
    "    \n",
    "    for epoch in range(UPDATE_EPOCHS):\n",
    "        np.random.shuffle(b_indicies)\n",
    "\n",
    "        #NOTE: mini-batch update: \n",
    "        # pros: reduce memory usage, faster updates\n",
    "        # pros: a whole batch may stuck in local minima, mini batches introduce randomness\n",
    "        # cons: estimate a true gradient, larger mini batch size --> more accurate but more memory\n",
    "        for start in range(0, batch_size, mini_batch_size):\n",
    "            end = start + mini_batch_size\n",
    "            mini_indicies = b_indicies[start:end]\n",
    "\n",
    "            _, new_log_prob, entropy, new_value = agent.get_action_and_value(b_obs[mini_indicies], b_actions[mini_indicies])\n",
    "\n",
    "            #NOTE: what formula is this? \n",
    "            log_ratio = new_log_prob - b_log_probs[mini_indicies]\n",
    "\n",
    "            ratio = log_ratio.exp() # trick to remove log\n",
    "\n",
    "            #compute approximate KL: http://joschu.net/blog/kl-approx.html\n",
    "            with torch.no_grad():\n",
    "                old_approx_kd = (-log_ratio).mean()\n",
    "                approximate_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                clip_fracs += [((ratio - 1.0).abs() > CLIP_COEF).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mini_indicies]\n",
    "\n",
    "            #normalize advantage\n",
    "            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            #policy loss (actor)\n",
    "\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2= -mb_advantages * torch.clamp(ratio, 1 - CLIP_COEF, 1 + CLIP_COEF)\n",
    "\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            new_value = new_value.view(-1)\n",
    "\n",
    "            #value loss (MSE)\n",
    "            v_loss = 0.5 * ((new_value - b_returns[mini_indicies]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "\n",
    "            loss = pg_loss - ENTROPY_COEF * entropy_loss + v_loss * VF_COEF\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #clip grad\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "    \n",
    "evaluation = np.mean(evaluate(agent))\n",
    "print('evaluation = ', np.mean(evaluation))\n",
    "torch.save(agent, os.path.join(SAVE_PATH, 'ppo.final.torch'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, device = DEVICE, episodes = 10):\n",
    "\n",
    "    agent = torch.load(model_path)\n",
    "\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(**ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs).to(device)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    print('reward = ', reward)\n",
    "                    total_rewards.append(reward)\n",
    "    \n",
    "    sns.lineplot(y = total_rewards, x = list(range(len(total_rewards))))\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT, )\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268.0151\n",
      "266.8834\n",
      "260.6238\n",
      "75.81549\n",
      "267.05634\n",
      "263.5786\n",
      "266.1446\n",
      "258.57123\n",
      "263.439\n",
      "-70.78032\n"
     ]
    }
   ],
   "source": [
    "next_obs, _ = envs.reset()\n",
    "total_rewards = []\n",
    "episodes = 10\n",
    "\n",
    "while len(total_rewards) < episodes: \n",
    "    next_obs = torch.Tensor(next_obs)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "    if 'final_info' in info:\n",
    "        for data in info['final_info']:\n",
    "            if data:\n",
    "                reward = data['episode']['r'][0]\n",
    "                print(reward)\n",
    "                total_rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AsyncVectorEnv.__del__ at 0x132e0e310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 546, in __del__\n",
      "    self.close(terminate=True)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/vector_env.py\", line 265, in close\n",
      "    self.close_extras(**kwargs)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 461, in close_extras\n",
      "    function(timeout)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py\", line 321, in step_wait\n",
      "    obs, rew, terminated, truncated, info = result\n",
      "TypeError: cannot unpack non-iterable NoneType object\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.542747\n",
      "307.70154\n",
      "304.33347\n",
      "-23.915497\n",
      "305.6151\n",
      "304.5836\n",
      "302.65884\n",
      "303.71445\n",
      "305.96878\n",
      "303.6156\n"
     ]
    }
   ],
   "source": [
    "ENV_ARGS['render_mode'] = 'human'\n",
    "envs1 = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "next_obs, _ = envs1.reset()\n",
    "total_rewards = []\n",
    "episodes = 10\n",
    "\n",
    "while len(total_rewards) < episodes: \n",
    "    next_obs = torch.Tensor(next_obs)\n",
    "    with torch.no_grad():\n",
    "        action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "    next_obs, reward, terminated, truncated, info = envs1.step(action.numpy())\n",
    "\n",
    "    if 'final_info' in info:\n",
    "        for data in info['final_info']:\n",
    "            if data:\n",
    "                reward = data['episode']['r'][0]\n",
    "                print(reward)\n",
    "                total_rewards.append(reward)\n",
    "    \n",
    "del ENV_ARGS['render_mode']\n",
    "envs1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
