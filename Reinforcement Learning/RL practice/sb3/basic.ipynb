{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if os.path.exists('output') == False:\n",
    "    os.makedirs('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_env(env_id = 'LunarLander-v3', record_name = None):\n",
    "    # Delete all contents in lunar-agent folder\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "\n",
    "    if record_name != None and record_name != \"\":\n",
    "        path = os.path.join(\"output\", record_name)\n",
    "\n",
    "        env = RecordVideo(\n",
    "            env,\n",
    "            video_folder=path,\n",
    "            episode_trigger=lambda x: True,  # Record every episode\n",
    "            name_prefix=\"training\",\n",
    "            video_length=3000,  # Maximum number of steps to record per episode\n",
    "        )\n",
    "\n",
    "    return env\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494e60882b254487b24fad2953fe1be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "# env = build_env()\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env)\n",
    "\n",
    "model.learn(total_timesteps=1000, progress_bar=True)\n",
    "\n",
    "model.save('test_dqn')\n",
    "model = DQN.load('test_dqn', env = env)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "total = 0\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"rgb_array\")\n",
    "    total += rewards\n",
    "\n",
    "print(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/nguyen/anaconda3/envs/rl/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/nguyen/anaconda3/envs/rl/lib/python3.11/site-packages/rich/live.py:231: UserWarning: install \"ipywidgets\" for\n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/anaconda3/envs/rl/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "np average  10.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "num_cpu = 4  # Number of processes to use\n",
    "# Create the vectorized environment\n",
    "\n",
    "# Stable Baselines provides you with make_vec_env() helper\n",
    "# which does exactly the previous steps for you.\n",
    "# You can choose between `DummyVecEnv` (usually faster) and `SubprocVecEnv`\n",
    "vec_env = make_vec_env(env_id, n_envs=num_cpu, seed=0, vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env)\n",
    "model.learn(total_timesteps=1000, progress_bar=True)\n",
    "\n",
    "# model.save(\"test_vec_env\")\n",
    "\n",
    "model.load(\"test_vec_env\", env = vec_env)\n",
    "\n",
    "obs = vec_env.reset()\n",
    "print(obs.shape)\n",
    "total_rewards = np.zeros(obs.shape[0])\n",
    "\n",
    "for _ in range(10):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    print(rewards)\n",
    "    total_rewards += rewards\n",
    "\n",
    "print('np average ', np.average(total_rewards))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
