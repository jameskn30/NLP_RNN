{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reference\n",
    "\n",
    "https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "https://huggingface.co/learn/deep-rl-course/en/unit8/hands-on-cleanrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import time\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = (8,)  type = <class 'numpy.ndarray'>\n",
      "action shape =  ()  type = <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "def build_env(name = 'LunarLander-v3', record_name = 'lunar', max_record_steps = int(1e3)):\n",
    "    # Delete all contents in lunar-agent folder\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(name, render_mode=\"rgb_array\")\n",
    "\n",
    "    if record_name != None and record_name != \"\":\n",
    "        path = os.path.join('output', record_name)\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "        env = RecordVideo(\n",
    "            env,\n",
    "            video_folder=path,\n",
    "            episode_trigger=lambda x: True,  # Record every episode\n",
    "            name_prefix=\"training\",\n",
    "            video_length=max_record_steps,  # Maximum number of steps to record per episode\n",
    "        )\n",
    "\n",
    "    return env\n",
    "\n",
    "env = build_env()\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "print('state =', observation.shape ,' type =', type(observation))\n",
    "print(\"action shape = \", env.action_space.sample().shape, ' type =', type(env.action_space.sample()))\n",
    "\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO\n",
    "- improve training stability by limit the changes the agent make at each step -> avoid large policy update\n",
    "- small update -> converge to optimal solution\n",
    "- measure how much policy changed, ratio calculation between the current and former policy.\n",
    "- [ 1 - e , 1 + e ]\n",
    "- Clipped Surrogate Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape =  (8,)\n",
      "Agent(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor(1)\n",
      "tensor(-1.2665, grad_fn=<SqueezeBackward1>)\n",
      "tensor(1.3814, grad_fn=<NegBackward0>)\n",
      "tensor([0.1223], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Agent\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "        )\n",
    "\n",
    "    def value(self, state):\n",
    "        return self.critic(state)\n",
    "    \n",
    "    def action_and_value(self,x, action = None):\n",
    "        #This only support discrete action space\n",
    "        logits = self.actor(x) # TODO: ????\n",
    "        probs = torch.distributions.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        value = self.value(x)\n",
    "        \n",
    "        #TODO: why use entropy? some optimize tips \n",
    "        return action, probs.log_prob(action), probs.entropy(), value\n",
    "\n",
    "# ====================================\n",
    "# Test the agent class\n",
    "\n",
    "env = build_env()\n",
    "\n",
    "state,info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "print('state shape = ', state.shape)\n",
    "\n",
    "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n)\n",
    "\n",
    "print(agent)\n",
    "\n",
    "action, log_prob, entropy, value =  agent.action_and_value(torch.tensor(state).float())\n",
    "print(action)\n",
    "print(log_prob)\n",
    "print(entropy)\n",
    "print(value)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "# init policy theta param, init value function param\n",
    "\n",
    "env = build_env()\n",
    "state, info = env.reset()\n",
    "\n",
    "\n",
    "agent = Agent(state_size=state.shape[0], action_size=env.action_space.n)\n",
    "optim = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "loop = tqdm(range(1000))\n",
    "\n",
    "for iter in loop:\n",
    "\n",
    "    # for k = 0, 1, 2 ... M do \n",
    "    for k in range(2048):\n",
    "\n",
    "        # collect trajectory D by running policy pi in the environment\n",
    "        states = [state]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# compute rewards to go (Gt)\n",
    "\n",
    "# compute advantage estimates A_t\n",
    "\n",
    "# update policy by maximize the PPO-clip objective with Adam\n",
    "\n",
    "# fit value function by regression on mean squared error with gradient descent algo\n",
    "\n",
    "\n",
    "# Clean up\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
