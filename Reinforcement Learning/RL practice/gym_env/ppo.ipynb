{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reference\n",
    "\n",
    "https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "https://huggingface.co/learn/deep-rl-course/en/unit8/hands-on-cleanrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import time\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = (8,)  type = <class 'numpy.ndarray'>\n",
      "action shape =  ()  type = <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "def build_env(name = 'LunarLander-v3', record_name = 'lunar', max_record_steps = int(1e3)):\n",
    "    # Delete all contents in lunar-agent folder\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(name, render_mode=\"rgb_array\")\n",
    "\n",
    "    if record_name != None and record_name != \"\":\n",
    "        path = os.path.join('output', record_name)\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "        env = RecordVideo(\n",
    "            env,\n",
    "            video_folder=path,\n",
    "            episode_trigger=lambda x: True,  # Record every episode\n",
    "            name_prefix=\"training\",\n",
    "            video_length=max_record_steps,  # Maximum number of steps to record per episode\n",
    "        )\n",
    "\n",
    "    return env\n",
    "\n",
    "env = build_env()\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "print('state =', observation.shape ,' type =', type(observation))\n",
    "print(\"action shape = \", env.action_space.sample().shape, ' type =', type(env.action_space.sample()))\n",
    "\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO\n",
    "- improve training stability by limit the changes the agent make at each step -> avoid large policy update\n",
    "- small update -> converge to optimal solution\n",
    "- measure how much policy changed, ratio calculation between the current and former policy.\n",
    "- [ 1 - e , 1 + e ]\n",
    "- Clipped Surrogate Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "state shape =  torch.Size([10, 8])\n",
      "actions shape =  torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Agent\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "        )\n",
    "\n",
    "    def value(self, state):\n",
    "        return self.critic(state).squeeze(-1)\n",
    "    \n",
    "    def action_and_value(self,x, action = None):\n",
    "        #This only support discrete action space\n",
    "        logits = self.actor(x) # TODO: ????\n",
    "        probs = torch.distributions.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        value = self.value(x)\n",
    "        \n",
    "        #TODO: why use entropy? some optimize tips \n",
    "        return action, probs.log_prob(action), probs.entropy(), value\n",
    "\n",
    "# ====================================\n",
    "# Test the agent class\n",
    "\n",
    "env = build_env()\n",
    "\n",
    "state,info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "\n",
    "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n)\n",
    "\n",
    "print(agent)\n",
    "# state = torch.tensor(state).unsqueeze(0).float()\n",
    "state = torch.rand(10,8)\n",
    "print('state shape = ', state.shape)\n",
    "actions = torch.ones(10)\n",
    "print('actions shape = ', actions.shape)\n",
    "\n",
    "action, log_prob, entropy, value =  agent.action_and_value(state, actions)\n",
    "print(action.shape)\n",
    "print(log_prob.shape)\n",
    "print(entropy.shape)\n",
    "print(value.shape)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_8064/1950886774.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action, log_prob, entropy, value = agent.action_and_value(torch.tensor(state).float())\n",
      "Episode reward: -233.49834527274174, avg reward: -179.68684201059799:   0%|          | 4/1000 [00:10<41:40,  2.51s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'b2Transform' and 'b2Vec2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[1;32m     48\u001b[0m log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n\u001b[0;32m---> 50\u001b[0m state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mnumpy()) \n\u001b[1;32m     51\u001b[0m current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     53\u001b[0m done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(terminated \u001b[38;5;129;01mor\u001b[39;00m truncated)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:356\u001b[0m, in \u001b[0;36mRecordVideo.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_recording(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-step-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_capture_frame()\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_length:\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_recording()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:312\u001b[0m, in \u001b[0;36mRecordVideo._capture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_capture_frame\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot capture a frame, recording wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt started.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 312\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame, List):\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frame) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# render was called\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/wrappers/common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/wrappers/common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/envs/box2d/lunar_lander.py:728\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    723\u001b[0m trans \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mtransform\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(f\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;129;01mis\u001b[39;00m circleShape:\n\u001b[1;32m    725\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mcircle(\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf,\n\u001b[1;32m    727\u001b[0m         color\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolor1,\n\u001b[0;32m--> 728\u001b[0m         center\u001b[38;5;241m=\u001b[39mtrans \u001b[38;5;241m*\u001b[39m f\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m*\u001b[39m SCALE,\n\u001b[1;32m    729\u001b[0m         radius\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mradius \u001b[38;5;241m*\u001b[39m SCALE,\n\u001b[1;32m    730\u001b[0m     )\n\u001b[1;32m    731\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mcircle(\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf,\n\u001b[1;32m    733\u001b[0m         color\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolor2,\n\u001b[1;32m    734\u001b[0m         center\u001b[38;5;241m=\u001b[39mtrans \u001b[38;5;241m*\u001b[39m f\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m*\u001b[39m SCALE,\n\u001b[1;32m    735\u001b[0m         radius\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mradius \u001b[38;5;241m*\u001b[39m SCALE,\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'b2Transform' and 'b2Vec2'"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.98\n",
    "collect_steps = 1000\n",
    "total_episodes = 1000\n",
    "\n",
    "# init policy theta param, init value function param\n",
    "\n",
    "env = build_env()\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state).float()\n",
    "done = torch.zeros(1)\n",
    "\n",
    "agent = Agent(state_size=state.shape[0], action_size=env.action_space.n)\n",
    "optim = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "loop = tqdm(range(total_episodes))\n",
    "\n",
    "# debug = True\n",
    "debug = False\n",
    "\n",
    "avg_rewards = deque(maxlen=100)\n",
    "current_rewards = 0\n",
    "\n",
    "# for k = 0, 1, 2 ... M do \n",
    "for k in loop:\n",
    "    states = []\n",
    "    dones = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    values = []\n",
    "\n",
    "    for step in range(collect_steps if debug == False else 0):\n",
    "\n",
    "        # collect trajectory D by running policy pi in the environment\n",
    "\n",
    "        states.append(state)\n",
    "        dones.append(done)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, entropy, value = agent.action_and_value(torch.tensor(state).float())\n",
    "            values.append(value)\n",
    "        \n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        state, reward, terminated, truncated, info = env.step(action.numpy()) \n",
    "        current_rewards += reward\n",
    "\n",
    "        done = torch.tensor(terminated or truncated)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        state = torch.tensor(state).float()\n",
    "\n",
    "        if done:\n",
    "            #collect stats\n",
    "            avg_rewards.append(current_rewards)\n",
    "            loop.set_description(f\"Episode reward: {current_rewards}, avg reward: {np.mean(avg_rewards)}\")\n",
    "            current_rewards = 0\n",
    "            env.reset()\n",
    "    \n",
    "    \n",
    "    states = torch.vstack(states)\n",
    "    dones = torch.tensor(dones)\n",
    "    actions = torch.tensor(actions)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    log_probs = torch.tensor(log_probs)\n",
    "    values = torch.tensor(values)\n",
    "\n",
    "    if debug:\n",
    "        print('state = ', states.shape)\n",
    "        print('dones = ', dones.shape)\n",
    "        print('actions = ', actions.shape)\n",
    "        print('rewards = ', rewards.shape)\n",
    "        print('log_probs = ', log_probs.shape)\n",
    "        print('values = ', values.shape)\n",
    "\n",
    "        break\n",
    "\n",
    "T = len(rewards)\n",
    "\n",
    "# compute rewards to go (Gt)\n",
    "with torch.no_grad():\n",
    "\n",
    "    next_value = agent.value(state)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    last_gae_lambda = 0\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        if t == T - 1:\n",
    "            next_none_terminal = 1 - dones[t + 1]\n",
    "            next_values = next_value\n",
    "        else:\n",
    "            next_none_terminal = dones[t+1]\n",
    "            next_values = values[t+1]\n",
    "\n",
    "        delta = rewards[t] + 0.99 * next_values * next_none_terminal - values[t]\n",
    "\n",
    "        advantages[t] = last_gae_lambda = delta + gamma * gae_lambda * next_none_terminal * last_gae_lambda\n",
    "    \n",
    "\n",
    "    returns = advantages + values\n",
    "\n",
    "if debug:\n",
    "    print()\n",
    "    print('adv ', advantages.shape)\n",
    "    print('values ', values.shape)\n",
    "    print('return ', returns.shape)\n",
    "\n",
    "\n",
    "    \n",
    "_, new_log_prob, entropy, new_value = agent.action_and_value(states, actions)\n",
    "\n",
    "log_ratio = new_log_prob - log_probs\n",
    "ratio = log_ratio.exp()\n",
    "\n",
    "    \n",
    "#normalize adv\n",
    "advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# policy loss (actor)\n",
    "pg_loss1 = advantages * ratio\n",
    "pg_loss2 = advantages * ratio.clamp(1-0.2, 1+0.2)\n",
    "\n",
    "pg_loss = torch.min(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "new_value = new_value.view(-1)\n",
    "\n",
    "# value loss\n",
    "value_loss = (returns - new_value).pow(2).mean()\n",
    "\n",
    "entropy_mean = entropy.mean()\n",
    "\n",
    "loss = pg_loss - 0.5 * value_loss - 0.01 * entropy_mean\n",
    "\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()\n",
    "\n",
    "if debug:\n",
    "    print()\n",
    "    print('new_log_prob ', new_log_prob.shape)\n",
    "    print('entropy ', entropy.shape)\n",
    "    print('new_value ', new_value.shape)\n",
    "    print('log ratio ', log_ratio.shape)\n",
    "    print('ratio shape = ', ratio.shape)\n",
    "    print('pg loss = ', pg_loss.shape)\n",
    "    print('value loss = ', value_loss.shape)\n",
    "    print('entropy shape = ', entropy.shape)\n",
    "    print('loss = ', loss.item())\n",
    "\n",
    "\n",
    "# compute advantage estimates A_t\n",
    "\n",
    "# update policy by maximize the PPO-clip objective with Adam\n",
    "\n",
    "# fit value function by regression on mean squared error with gradient descent algo\n",
    "\n",
    "\n",
    "# Clean up\n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
