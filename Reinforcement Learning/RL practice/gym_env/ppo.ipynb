{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reference\n",
    "\n",
    "https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "https://huggingface.co/learn/deep-rl-course/en/unit8/hands-on-cleanrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import time\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = (8,)  type = <class 'numpy.ndarray'>\n",
      "action shape =  ()  type = <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "def build_env(name = 'LunarLander-v3', record_name = 'lunar', max_record_steps = int(1e3)):\n",
    "    # Delete all contents in lunar-agent folder\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(name, render_mode=\"rgb_array\")\n",
    "\n",
    "    if record_name != None and record_name != \"\":\n",
    "        path = os.path.join('output', record_name)\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "        env = RecordVideo(\n",
    "            env,\n",
    "            video_folder=path,\n",
    "            episode_trigger=lambda x: True,  # Record every episode\n",
    "            name_prefix=\"training\",\n",
    "            video_length=max_record_steps,  # Maximum number of steps to record per episode\n",
    "        )\n",
    "\n",
    "    return env\n",
    "\n",
    "env = build_env()\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "print('state =', observation.shape ,' type =', type(observation))\n",
    "print(\"action shape = \", env.action_space.sample().shape, ' type =', type(env.action_space.sample()))\n",
    "\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO\n",
    "- improve training stability by limit the changes the agent make at each step -> avoid large policy update\n",
    "- small update -> converge to optimal solution\n",
    "- measure how much policy changed, ratio calculation between the current and former policy.\n",
    "- [ 1 - e , 1 + e ]\n",
    "- Clipped Surrogate Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "state shape =  torch.Size([10, 8])\n",
      "actions shape =  torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Agent\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "        )\n",
    "\n",
    "    def value(self, state):\n",
    "        return self.critic(state).squeeze(-1)\n",
    "    \n",
    "    def action_and_value(self,x, action = None):\n",
    "        #This only support discrete action space\n",
    "        logits = self.actor(x) # TODO: ????\n",
    "        probs = torch.distributions.Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        \n",
    "        value = self.value(x)\n",
    "        \n",
    "        #TODO: why use entropy? some optimize tips \n",
    "        return action, probs.log_prob(action), probs.entropy(), value\n",
    "\n",
    "# ====================================\n",
    "# Test the agent class\n",
    "\n",
    "env = build_env()\n",
    "\n",
    "state,info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "\n",
    "\n",
    "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.n)\n",
    "\n",
    "print(agent)\n",
    "# state = torch.tensor(state).unsqueeze(0).float()\n",
    "state = torch.rand(10,8)\n",
    "print('state shape = ', state.shape)\n",
    "actions = torch.ones(10)\n",
    "print('actions shape = ', actions.shape)\n",
    "\n",
    "action, log_prob, entropy, value =  agent.action_and_value(state, actions)\n",
    "print(action.shape)\n",
    "print(log_prob.shape)\n",
    "print(entropy.shape)\n",
    "print(value.shape)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode reward: -158.32285640307734, avg reward: -159.26805912297067:   1%|â–         | 14/1000 [01:14<1:26:53,  5.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m         loop\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_rewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, avg reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(avg_rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m         current_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 65\u001b[0m         state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Convert collected lists to tensors\u001b[39;00m\n\u001b[1;32m     68\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(states)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:338\u001b[0m, in \u001b[0;36mRecordVideo.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_recording()\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_trigger \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_trigger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_id):\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_recording(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-episode-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_capture_frame()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:385\u001b[0m, in \u001b[0;36mRecordVideo.start_recording\u001b[0;34m(self, video_name)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Start a new recording. If it is already recording, stops the current recording before starting the new one.\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_recording()\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_video_name \u001b[38;5;241m=\u001b[39m video_name\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:407\u001b[0m, in \u001b[0;36mRecordVideo.stop_recording\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m     moviepy_logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_logger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_video_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 407\u001b[0m     clip\u001b[38;5;241m.\u001b[39mwrite_videofile(path, logger\u001b[38;5;241m=\u001b[39mmoviepy_logger)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/moviepy/decorators.py:125\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    120\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[1;32m    121\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[1;32m    122\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    123\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m k\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39mnew_a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kw)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/moviepy/decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip\u001b[38;5;241m.\u001b[39mismask:\n\u001b[1;32m     21\u001b[0m     clip \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mto_RGB()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(clip, \u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mk)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/moviepy/video/VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_audio:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[1;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[1;32m    295\u001b[0m                                audio_codec, bitrate\u001b[38;5;241m=\u001b[39maudio_bitrate,\n\u001b[1;32m    296\u001b[0m                                write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[1;32m    297\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    298\u001b[0m                                logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[0;32m--> 300\u001b[0m ffmpeg_write_video(\u001b[38;5;28mself\u001b[39m, filename, fps, codec,\n\u001b[1;32m    301\u001b[0m                    bitrate\u001b[38;5;241m=\u001b[39mbitrate,\n\u001b[1;32m    302\u001b[0m                    preset\u001b[38;5;241m=\u001b[39mpreset,\n\u001b[1;32m    303\u001b[0m                    write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[1;32m    304\u001b[0m                    audiofile\u001b[38;5;241m=\u001b[39maudiofile,\n\u001b[1;32m    305\u001b[0m                    verbose\u001b[38;5;241m=\u001b[39mverbose, threads\u001b[38;5;241m=\u001b[39mthreads,\n\u001b[1;32m    306\u001b[0m                    ffmpeg_params\u001b[38;5;241m=\u001b[39mffmpeg_params,\n\u001b[1;32m    307\u001b[0m                    logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_temp \u001b[38;5;129;01mand\u001b[39;00m make_audio:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audiofile):\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:228\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    226\u001b[0m             frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdstack([frame,mask])\n\u001b[0;32m--> 228\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_frame(frame)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m write_logfile:\n\u001b[1;32m    231\u001b[0m     logfile\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/moviepy/video/io/ffmpeg_writer.py:136\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.write_frame\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PY3:\n\u001b[0;32m--> 136\u001b[0m        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(img_array\u001b[38;5;241m.\u001b[39mtobytes())\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(img_array\u001b[38;5;241m.\u001b[39mtostring())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-3\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.98\n",
    "collect_steps = 2048\n",
    "total_episodes = 1000\n",
    "\n",
    "# init policy theta param, init value function param\n",
    "env = build_env()\n",
    "state, info = env.reset()\n",
    "done = torch.zeros(1)\n",
    "\n",
    "agent = Agent(state_size=state.shape[0], action_size=env.action_space.n)\n",
    "optim = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "\n",
    "loop = tqdm(range(total_episodes))\n",
    "\n",
    "# debug = True\n",
    "debug = False\n",
    "\n",
    "avg_rewards = deque(maxlen=100)\n",
    "current_rewards = 0\n",
    "\n",
    "# for k = 0, 1, 2 ... M do \n",
    "for k in loop:\n",
    "    states = []\n",
    "    dones = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    values = []\n",
    "\n",
    "    for step in range(collect_steps if not debug else 100):\n",
    "\n",
    "        # collect trajectory D by running policy pi in the environment\n",
    "        state = torch.tensor(state).float()  # ensure state is a float tensor\n",
    "\n",
    "        states.append(state)\n",
    "        dones.append(done)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, entropy, value = agent.action_and_value(state)  # state is already a tensor\n",
    "            values.append(value)\n",
    "        \n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Old: state, reward, terminated, truncated, info = env.step(action.numpy())\n",
    "        # fixed: use .item() to convert action tensor to a scalar\n",
    "        state, reward, terminated, truncated, info = env.step(action.item())\n",
    "        current_rewards += reward\n",
    "\n",
    "        done = torch.tensor(terminated or truncated)\n",
    "        # Old: (no conversion)\n",
    "        # fixed: convert done to float for proper arithmetic later\n",
    "        done = done.float()\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done.item() == 1:  # fixed: check if done using .item()\n",
    "            # collect stats\n",
    "            avg_rewards.append(current_rewards)\n",
    "            loop.set_description(f\"Episode reward: {current_rewards}, avg reward: {np.mean(avg_rewards)}\")\n",
    "            current_rewards = 0\n",
    "            state, info = env.reset()\n",
    "    \n",
    "    # Convert collected lists to tensors\n",
    "    states = torch.vstack(states)\n",
    "    # Old: dones = torch.tensor(dones)\n",
    "    dones = torch.tensor(dones).float()  # fixed: ensure dones is a float tensor (0 for non-terminal, 1 for terminal)\n",
    "    actions = torch.tensor(actions)\n",
    "    rewards = torch.tensor(rewards).float()\n",
    "    log_probs = torch.tensor(log_probs)\n",
    "    values = torch.tensor(values).float()\n",
    "\n",
    "    if debug:\n",
    "        print('states = ', states.shape)\n",
    "        print('dones = ', dones.shape)\n",
    "        print('actions = ', actions.shape)\n",
    "        print('rewards = ', rewards.shape)\n",
    "        print('log_probs = ', log_probs.shape)\n",
    "        print('values = ', values.shape)\n",
    "        break\n",
    "\n",
    "    T = len(rewards)\n",
    "\n",
    "    # compute rewards-to-go (Gt) and advantages\n",
    "    with torch.no_grad():\n",
    "        # Old: new_state = torch.tensor(state).float().unsqueeze(0)  (same)\n",
    "        # fixed: add unsqueeze so that state has a batch dimension if needed\n",
    "        new_state = torch.tensor(state).float().unsqueeze(0)\n",
    "        next_value = agent.value(new_state)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_gae_lambda = 0\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            if t == T - 1:\n",
    "                # Old: next_none_terminal = 1 - done.float()  \n",
    "                # fixed: use the last element from dones (inverted) for proper terminal mask\n",
    "                next_none_terminal = 1 - dones[-1]  # fixed: use dones[-1] instead of 'done'\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                # Old: next_none_terminal = 1 - dones[t+1]  (same)\n",
    "                # fixed: invert dones to get non-terminal mask (1 for non-terminal, 0 for terminal)\n",
    "                next_none_terminal = 1 - dones[t+1]\n",
    "                next_values = values[t+1]\n",
    "\n",
    "            # Old: delta = rewards[t] + 0.99 * next_values * next_none_terminal - values[t]\n",
    "            # fixed: use gamma variable instead of hard-coded 0.99\n",
    "            delta = rewards[t] + gamma * next_values * next_none_terminal - values[t]\n",
    "            last_gae_lambda = delta + gamma * gae_lambda * next_none_terminal * last_gae_lambda\n",
    "            advantages[t] = last_gae_lambda\n",
    "\n",
    "        returns = advantages + values\n",
    "\n",
    "    if debug:\n",
    "        print()\n",
    "        print('advantages ', advantages.shape)\n",
    "        print('values ', values.shape)\n",
    "        print('returns ', returns.shape)\n",
    "\n",
    "    # Compute new log probabilities and value predictions for the collected batch\n",
    "    _, new_log_prob, entropy, new_value = agent.action_and_value(states, actions)\n",
    "\n",
    "    log_ratio = new_log_prob - log_probs\n",
    "    ratio = log_ratio.exp()\n",
    "\n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    advantages = advantages.detach()  # fixed: detach so gradients do not flow through advantage normalization\n",
    "\n",
    "    # PPO Clipped Objective for policy loss (actor)\n",
    "    # Old:\n",
    "    # pg_loss1 = -advantages * ratio\n",
    "    # pg_loss2 = -advantages * ratio.clamp(1 - 0.2, 1 + 0.2)\n",
    "    # pg_loss = torch.min(pg_loss1, pg_loss2).mean()\n",
    "    # fixed: use the same clipping but ensure signs are correct; note that we want to maximize the surrogate,\n",
    "    # so we minimize the negative surrogate objective.\n",
    "    pg_loss1 = -advantages * ratio\n",
    "    pg_loss2 = -advantages * ratio.clamp(1 - 0.2, 1 + 0.2)\n",
    "    pg_loss = torch.min(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "    new_value = new_value.view(-1)\n",
    "\n",
    "    # Value function loss\n",
    "    value_loss = (returns - new_value).pow(2).mean()\n",
    "\n",
    "    entropy_mean = entropy.mean()\n",
    "\n",
    "    # Combined loss: policy loss + value loss coefficient - entropy bonus\n",
    "    # Old: loss = pg_loss - 0.5 * value_loss - 0.01 * entropy_mean\n",
    "    loss = pg_loss + 0.5 * value_loss - 0.01 * entropy_mean  # fixed: add value loss (not subtract it)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if debug:\n",
    "        print()\n",
    "        print('new_log_prob ', new_log_prob.shape)\n",
    "        print('entropy ', entropy.shape)\n",
    "        print('new_value ', new_value.shape)\n",
    "        print('log ratio ', log_ratio.shape)\n",
    "        print('ratio shape = ', ratio.shape)\n",
    "        print('pg loss = ', pg_loss.shape)\n",
    "        print('value loss = ', value_loss.shape)\n",
    "        print('loss = ', loss.item())\n",
    "\n",
    "# Clean up\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
