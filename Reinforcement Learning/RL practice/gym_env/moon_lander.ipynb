{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import time\n",
    "from torch import functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_env(name = 'LunarLander-v3', record_name = 'lunar_agent'):\n",
    "    # Delete all contents in lunar-agent folder\n",
    "    if os.path.exists(record_name):\n",
    "        shutil.rmtree(record_name)\n",
    "\n",
    "    # Initialise the environment\n",
    "    env = gym.make(name, render_mode=\"rgb_array\")\n",
    "\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=record_name,\n",
    "        episode_trigger=lambda x: True,  # Record every episode\n",
    "        name_prefix=\"training\",\n",
    "        video_length=3000,  # Maximum number of steps to record per episode\n",
    "    )\n",
    "\n",
    "    return env\n",
    "\n",
    "env = build_env()\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "- ReplayBuffer\n",
    "- NN\n",
    "- training formula: Q(s,a) = Q(s,a) + alpha * (R_t+1 + gamma * max Q(s_t+1 ,a) - Q(s_t, a))\n",
    "https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm\n",
    "- Q-target:\n",
    "\n",
    "known problems:\n",
    "- forgetting: agent forgot best behavior when new experience comes in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque()\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, terminated):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.popleft()\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        '''state, action, reward, next_state, terminated'''\n",
    "        state, action, reward, next_state, terminated = zip(*random.sample(self.buffer, batch_size))\n",
    "        return torch.vstack([torch.tensor(s) for s in state]), torch.tensor(action), torch.tensor(reward), torch.vstack([torch.tensor(ns) for ns in next_state]), torch.tensor(terminated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 8])\n",
      "tensor([ True,  True, False, False, False, False, False,  True,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "env = build_env()\n",
    "env.reset()\n",
    "rb = ReplayBuffer(capacity=50)\n",
    "#warmup\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    rb.push(state, action, reward, next_state, terminated)\n",
    "    state = next_state\n",
    "\n",
    "state, action, reward, next_state, terminated = rb.sample(10)\n",
    "print(state.shape)\n",
    "print( action.shape)\n",
    "print( reward.shape)\n",
    "print( next_state.shape)\n",
    "print( terminated)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qnet(state_dim, action_dim):\n",
    "    state_dim = state_dim\n",
    "    action_dim = action_dim\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(state_dim, 120),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(120, 84),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(84, action_dim)\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2086, -0.0825,  0.1554, -0.1457]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "testnet = build_qnet(8, 4)\n",
    "y = testnet(torch.randn(1, 8))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy policy (DQN is off-policy, use greedy policy for training, but use only model for reference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(start, end, duration, t):\n",
    "    slope = (end - start) / duration\n",
    "    return max(slope * t + start, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = build_qnet(8, 4)\n",
    "target_qnet = build_qnet(8, 4)\n",
    "target_qnet.load_state_dict(qnet.state_dict())\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "tau = 0.01 #to sync target qnet and training qnet\n",
    "total_steps = 10000\n",
    "sync_steps = 100\n",
    "warm_up = 100\n",
    "trainng_freq = 10 # traing every 10 steps\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(qnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "1. given a state, take an action\n",
    "2. add that action, reward, state to buffer\n",
    "3. if global step > learning starts (pass warmup) and global step % training freq == 0: \n",
    "    - get data from buffer\n",
    "    - Q_max = target_nn(data.obs).max(1)\n",
    "    - td_target = data.rewards + gamma * Q_max * (1 - dones)\n",
    "    - old val = q_net(data.obs)\n",
    "\n",
    "    loss = mse(td_target, old_val)\n",
    "\n",
    "    optim.zero_grad\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    sync q_net and target q_net \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "tensor([1, 0, 0, 1], dtype=torch.int32)\n",
      "tensor([0, 1, 1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[True, False, False, True]])\n",
    "print(a.shape)\n",
    "print(a.flatten().int())\n",
    "print(1 - a.flatten().int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/anaconda3/envs/torch/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:416: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_10660/1246515201.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  td_target = torch.tensor(rb_reward) + gamma * target_max * (1 - rb_done.flatten().int())\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565.35595703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = build_env()\n",
    "env.reset()\n",
    "\n",
    "#warmup\n",
    "for _ in range(warm_up):\n",
    "    next_state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "    state = next_state\n",
    "\n",
    "for step in tqdm(range(10)):\n",
    "\n",
    "    epsilon = linear_schedule(1, 0.01, total_steps, step)\n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = qnet(torch.tensor(state).float()).argmax().item()\n",
    "\n",
    "    # pretraining\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "\n",
    "    # training\n",
    "    if step % trainng_freq == 0:\n",
    "        # greedy epsilon\n",
    "        \n",
    "        data = replay_buffer.sample(batch_size)\n",
    "        rb_state, rb_action, rb_reward, rb_nextstate, rb_done = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_max = target_qnet(rb_nextstate).float().max(dim=1).values\n",
    "            td_target = torch.tensor(rb_reward) + gamma * target_max * (1 - rb_done.flatten().int())\n",
    "        \n",
    "        \n",
    "        old_value = qnet(rb_state).gather(1, rb_action.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(old_value, td_target.float())\n",
    "        print(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    # sync every sync_steps\n",
    "    if step % sync_steps == 0:\n",
    "        target_qnet.load_state_dict(qnet.state_dict())\n",
    "\n",
    "    #post training\n",
    "    state = next_state\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
