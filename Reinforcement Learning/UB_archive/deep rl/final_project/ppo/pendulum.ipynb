{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "https://github.com/vwxyzjn/cleanrl?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import deque, Counter, namedtuple, defaultdict\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import math\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ARGS = {\n",
    "    'id': \"InvertedPendulum-v4\"\n",
    "}\n",
    "NUM_ENVS = 6\n",
    "SEED = 1\n",
    "LR = 1e-4\n",
    "NUM_STEPS = 2048\n",
    "NUM_ITERATIONS = 1000\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "UPDATE_EPOCHS = 10\n",
    "CLIP_COEF = 0.2 # the epsilon in KL divergece in PPO paper\n",
    "ENTROPY_COEF = 0.0\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "MINI_BATCH_COUNT = 32\n",
    "UPDATE_PLOTS = 10\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#output directory\n",
    "ROOT = os.getcwd()\n",
    "OUTPUT = os.path.join(ROOT, 'output', ENV_ARGS['id'])\n",
    "\n",
    "if os.path.exists(OUTPUT) == False:\n",
    "    os.makedirs(OUTPUT)\n",
    "\n",
    "#seeding\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gamma, **env_args):\n",
    "    env = gym.make(**env_args)\n",
    "    env = gym.wrappers.FlattenObservation(env)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.ClipAction(env)\n",
    "    # env = gym.wrappers.NormalizeObservation(env)\n",
    "    # env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "    env = gym.wrappers.NormalizeReward(env, gamma = gamma)\n",
    "    env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "\n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer: nn.Linear, std = np.sqrt(2), bias_const = 0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "\n",
    "    def __init__(self, envs: gym.Env, hidden_size: int = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.state_shape = np.array(envs.single_observation_space.shape).prod()\n",
    "        self.action_shape = np.prod(envs.single_action_space.shape)\n",
    "\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            #NOTE: why use tanh here? \n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(layer = nn.Linear(hidden_size, self.action_shape), std = 0.01),\n",
    "        )\n",
    "\n",
    "        #shape = (1, state_shape)\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, self.action_shape, dtype=torch.float))\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.state_shape, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(hidden_size, hidden_size)),\n",
    "            nn.Tanh(),\n",
    "            # NOTE: what's the STD do in layer initialization???\n",
    "            layer_init(nn.Linear(hidden_size, 1), std = 1.0),\n",
    "        )\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "    \n",
    "    def get_action_and_value(self, x, action = None):\n",
    "        '''\n",
    "        @params:\n",
    "            x: torch.tensor observation, shape = (N, observation size)\n",
    "            action: torch.tensor action\n",
    "        @returns:\n",
    "            action: torch.tensor, shape = (N, action size)\n",
    "            log_prob: torch.tensor, shape = (N,)\n",
    "            entropy: torch.tensor, shape = (N,)\n",
    "            value: torch.tensor, shape = (N,)\n",
    "        '''\n",
    "        action_mean = self.actor_mean(x)\n",
    "        #make action logstd the shape[0] with mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        #exponential trick to remove log\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        probs = torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample() \n",
    "        \n",
    "        #get value from critic\n",
    "        value = self.get_value(x)\n",
    "        log_prob = probs.log_prob(action).sum(1)\n",
    "        #entropy for regularization\n",
    "        entropy = probs.entropy().sum(1)\n",
    "        \n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape =  torch.Size([6, 4])\n",
      "action shape =  (1,)\n",
      "log prob shape =  torch.Size([6])\n",
      "entropy shape =  torch.Size([6])\n",
      "value shape =  torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "#Test agent\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "obs, info = envs.reset()\n",
    "\n",
    "test_agent = Agent(envs)\n",
    "\n",
    "obs = torch.tensor(obs).float()\n",
    "\n",
    "action, log_prob, entropy, value = test_agent.get_action_and_value(obs)\n",
    "\n",
    "print('state shape = ', obs.shape)\n",
    "print('action shape = ', envs.single_action_space.shape)\n",
    "print('log prob shape = ', log_prob.shape)\n",
    "print('entropy shape = ', entropy.shape)\n",
    "print('value shape = ', value.shape)\n",
    "\n",
    "del test_agent\n",
    "\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, show = False, save_path = None):\n",
    "    sns.lineplot(y = history['reward'], x = list(range(len(history['reward']))))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "    \n",
    "def pickle_load(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        obj = pickle.dump(file)\n",
    "    return obj\n",
    "\n",
    "def evaluate(agent, episodes = 10, human_render = False):\n",
    "    if human_render:\n",
    "        ENV_ARGS['render_mode'] = 'human'\n",
    "    envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "    agent.eval()\n",
    "    total_rewards = []\n",
    "    next_obs, _ = envs.reset()\n",
    "\n",
    "    while len(total_rewards) < episodes: \n",
    "        next_obs = torch.Tensor(next_obs)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "        if 'final_info' in info:\n",
    "            for data in info['final_info']:\n",
    "                if data:\n",
    "                    reward = data['episode']['r'][0]\n",
    "                    total_rewards.append(reward)\n",
    "    \n",
    "    if human_render:\n",
    "        del ENV_ARGS['render_mode']\n",
    "\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(agent, episodes = 10):\n",
    "#     envs = gym.vector.SyncVectorEnv([lambda: make_env(gamma = GAMMA, **ENV_ARGS)])\n",
    "#     agent.eval()\n",
    "#     total_rewards = []\n",
    "#     next_obs, _ = envs.reset()\n",
    "\n",
    "#     while len(total_rewards) < episodes: \n",
    "#         next_obs = torch.Tensor(next_obs)\n",
    "#         with torch.no_grad():\n",
    "#             action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "\n",
    "#         next_obs, reward, terminated, truncated, info = envs.step(action.numpy())\n",
    "\n",
    "#         if 'final_info' in info:\n",
    "#             for data in info['final_info']:\n",
    "#                 if data:\n",
    "#                     reward = data['episode']['r'][0]\n",
    "#                     total_rewards.append(reward)\n",
    "\n",
    "#     return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(envs, agent, optimizer, num_steps, num_envs, device, \n",
    "         num_iterations = 1000, update_epochs = 10,\n",
    "         label = 'baseline', plot_update_freq = 10, history = None):\n",
    "\n",
    "    # label = str(uuid.uuid4()).split('-')[0]\n",
    "    agent.to(device)\n",
    "\n",
    "    SAVE_PATH = os.path.join(OUTPUT, label)\n",
    "    FIG_SAVE_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "    HISTORY_SAVE_PATH = os.path.join(SAVE_PATH, 'history.pickle')\n",
    "\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        print(f'output folder: {SAVE_PATH}')\n",
    "        os.makedirs(SAVE_PATH)\n",
    "    print('save path = ', SAVE_PATH)\n",
    "\n",
    "    M,N = num_steps, num_envs\n",
    "\n",
    "    obs = torch.zeros((M, N) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((M,N) + envs.single_action_space.shape).to(device)\n",
    "    log_probs = torch.zeros((M,N)).to(device)\n",
    "    rewards = torch.zeros((M,N)).to(device)\n",
    "    dones = torch.zeros((M,N)).to(device) # for masking\n",
    "    values = torch.zeros((M,N)).to(device)\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    #Reset env\n",
    "    next_obs, _ = envs.reset()\n",
    "    next_obs = torch.tensor(next_obs).float().to(device)\n",
    "    next_done = torch.zeros(N).to(device) #N is num envs\n",
    "\n",
    "    print('next obs = ', next_obs.shape)\n",
    "    print('next done = ', next_done.shape)\n",
    "\n",
    "    reward_window = deque(maxlen = 100)\n",
    "\n",
    "    if history == None:\n",
    "        history = defaultdict(list)\n",
    "\n",
    "    loop = tqdm(range(num_iterations))\n",
    "    agent.train()\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    loss = float('inf')\n",
    "\n",
    "    for iter in loop:\n",
    "\n",
    "        if iter % plot_update_freq == 0: \n",
    "            loop.set_description(f\"avg_reward = {np.mean(reward_window):.2f}, best_score = {best_score}, episode_count = {len(history['reward'])}\")\n",
    "            plot(history, save_path = FIG_SAVE_PATH)\n",
    "            pickle_dump(history, HISTORY_SAVE_PATH)\n",
    "\n",
    "        #ROLLOUT phase\n",
    "        #M is max steps\n",
    "        for step in range(M):\n",
    "            global_step += N\n",
    "\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            #get action\n",
    "            #NOTE: no_grad disables gradient calculation --> reduce memory consumption\n",
    "            #the result of every computation will have requires_grad=False\n",
    "            with torch.no_grad():\n",
    "                action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "\n",
    "            actions[step] = action\n",
    "            log_probs[step] = log_prob\n",
    "\n",
    "            #make next step with actions\n",
    "            next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "\n",
    "            next_done = np.logical_or(terminated, truncated)\n",
    "\n",
    "            #NOTE: difference between view and reshape\n",
    "            # https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n",
    "            rewards[step] = torch.tensor(reward).view(-1)\n",
    "            next_obs = torch.tensor(next_obs).float().to(device)\n",
    "            next_done = torch.tensor(next_done).float().to(device)\n",
    "\n",
    "            #NOTE: vector envs will automatically reset, so no need to break \n",
    "            if 'final_info' in info:\n",
    "                for data in info['final_info']:\n",
    "                    if data:\n",
    "                        reward = data['episode']['r']\n",
    "                        reward_window.append(reward)\n",
    "                        avg_reward = np.mean(reward_window)\n",
    "                        history['reward'].append(avg_reward)\n",
    "\n",
    "                        if best_score < avg_reward:\n",
    "                            best_score = avg_reward\n",
    "                            #save model\n",
    "                            torch.save(agent, os.path.join(SAVE_PATH, 'ppo.checkpoint.torch'))\n",
    "            \n",
    "        #update the history for plotting, and printing progress\n",
    "\n",
    "        #OPTIMIZE phase:\n",
    "        with torch.no_grad():\n",
    "            #bootstrap values, compute returns\n",
    "            next_value = agent.get_value(next_obs).reshape(1,-1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            last_gae_lambda = 0\n",
    "\n",
    "            for t in reversed(range(NUM_STEPS)):\n",
    "                if t == NUM_STEPS - 1:\n",
    "                    next_none_terminal = np.logical_not(next_done.cpu())\n",
    "                    next_values = next_value\n",
    "                else:\n",
    "                    next_none_terminal = np.logical_not(dones[t + 1].cpu())\n",
    "                    next_values = values[t + 1]\n",
    "\n",
    "                next_none_terminal = next_none_terminal.to(device)\n",
    "                \n",
    "                #A(s,a) = Q(s,a) - V(s,a) = r(t) + gamma * V(s', a) * mask - V(s)\n",
    "                delta = rewards[t] + GAMMA * next_values * next_none_terminal - values[t]\n",
    "                #NOTE: learn about this formula\n",
    "                advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_none_terminal * last_gae_lambda\n",
    "            returns = advantages + values\n",
    "        \n",
    "        #flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_log_probs = log_probs.reshape(-1)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        #NOTE: randomize the batch to break correlation\n",
    "        batch_size = M * N\n",
    "        mini_batch_size = batch_size // MINI_BATCH_COUNT\n",
    "        b_indicies = np.arange(batch_size)\n",
    "        clip_fracs = []\n",
    "        \n",
    "        for _ in range(update_epochs):\n",
    "            np.random.shuffle(b_indicies)\n",
    "\n",
    "            #NOTE: mini-batch update: \n",
    "            # pros: reduce memory usage, faster updates\n",
    "            # pros: a whole batch may stuck in local minima, mini batches introduce randomness\n",
    "            # cons: estimate a true gradient, larger mini batch size --> more accurate but more memory\n",
    "            for start in range(0, batch_size, mini_batch_size):\n",
    "                end = start + mini_batch_size\n",
    "                mini_indicies = b_indicies[start:end]\n",
    "\n",
    "                _, new_log_prob, entropy, new_value = agent.get_action_and_value(b_obs[mini_indicies], b_actions[mini_indicies])\n",
    "\n",
    "                #NOTE: what formula is this? \n",
    "                log_ratio = new_log_prob - b_log_probs[mini_indicies]\n",
    "\n",
    "                ratio = log_ratio.exp() # trick to remove log\n",
    "\n",
    "                #compute approximate KL: http://joschu.net/blog/kl-approx.html\n",
    "                with torch.no_grad():\n",
    "                    old_approx_kd = (-log_ratio).mean()\n",
    "                    approximate_kl = ((ratio - 1) - log_ratio).mean()\n",
    "                    clip_fracs += [((ratio - 1.0).abs() > CLIP_COEF).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mini_indicies]\n",
    "\n",
    "                #normalize advantage\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                #policy loss (actor)\n",
    "\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2= -mb_advantages * torch.clamp(ratio, 1 - CLIP_COEF, 1 + CLIP_COEF)\n",
    "\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                new_value = new_value.view(-1)\n",
    "\n",
    "                #value loss (MSE)\n",
    "                v_loss = 0.5 * ((new_value - b_returns[mini_indicies]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "\n",
    "                loss = pg_loss - ENTROPY_COEF * entropy_loss + v_loss * VF_COEF\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #clip grad\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "\n",
    "        \n",
    "    torch.save(agent, os.path.join(SAVE_PATH, 'ppo.final.torch'))\n",
    "    plot(history, show=True, save_path=FIG_SAVE_PATH)\n",
    "\n",
    "    pickle_dump(history, HISTORY_SAVE_PATH)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path =  /Volumes/SanDisk/NLP_RNN/Reinforcement Learning/deep rl/final_project/ppo/output/InvertedPendulum-v4/baseline\n",
      "next obs =  torch.Size([6, 4])\n",
      "next done =  torch.Size([6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "avg_reward = 1000.00, best_score = 1000.0, episode_count = 6126:   2%|▏         | 40/2000 [02:03<1:41:07,  3.10s/it]           \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m N \u001b[38;5;241m=\u001b[39m NUM_ENVS\n\u001b[1;32m     13\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_ENVS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_update_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 73\u001b[0m, in \u001b[0;36mtune\u001b[0;34m(envs, agent, optimizer, num_steps, num_envs, device, num_iterations, update_epochs, label, plot_update_freq, history)\u001b[0m\n\u001b[1;32m     70\u001b[0m log_probs[step] \u001b[38;5;241m=\u001b[39m log_prob\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#make next step with actions\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m next_done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terminated, truncated)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#NOTE: difference between view and reshape\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/vector_env.py:202\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[1;32m    166\u001b[0m     \u001b[39mself\u001b[39m, actions\n\u001b[1;32m    167\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, NDArray[Any], NDArray[Any], NDArray[Any], \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m        {}\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_async(actions)\n\u001b[1;32m    203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/vector/async_vector_env.py:286\u001b[0m, in \u001b[0;36mAsyncVectorEnv.step_async\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    284\u001b[0m actions \u001b[39m=\u001b[39m iterate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, actions)\n\u001b[1;32m    285\u001b[0m \u001b[39mfor\u001b[39;00m pipe, action \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent_pipes, actions):\n\u001b[0;32m--> 286\u001b[0m     pipe\u001b[39m.\u001b[39;49msend((\u001b[39m\"\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m\"\u001b[39;49m, action))\n\u001b[1;32m    287\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m=\u001b[39m AsyncState\u001b[39m.\u001b[39mWAITING_STEP\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol)\u001b[39m.\u001b[39mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/multiprocessing/reduction.py:41\u001b[0m, in \u001b[0;36mForkingPickler.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs)\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyreg_dispatch_table\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 41\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_table\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extra_reducers)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create env\n",
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [lambda : make_env(gamma= 0.99, **ENV_ARGS) for _ in range(NUM_ENVS)]\n",
    ") \n",
    "#check to make sure this is continous action\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), 'Only continous action is supported'\n",
    "\n",
    "agent = Agent(envs)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr = 1e-4, eps = 1e-5)\n",
    "\n",
    "M = NUM_STEPS\n",
    "N = NUM_ENVS\n",
    "num_iterations = 2000\n",
    "\n",
    "tune(envs, agent, optimizer, NUM_STEPS, NUM_ENVS, device=  DEVICE, num_iterations=num_iterations, label = 'baseline', plot_update_freq=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOAD_MODEL_PATH = os.path.join(OUTPUT, 'baseline', 'ppo.checkpoint.torch')\n",
    "agent = torch.load(LOAD_MODEL_PATH)\n",
    "\n",
    "total_rewards = evaluate(agent)\n",
    "\n",
    "sns.lineplot(y = total_rewards, x = list(range(len(total_rewards))))\n",
    "EVALUATION_PLOT_SAVE = os.path.join(OUTPUT, 'baseline', 'evaluation.png') \n",
    "plt.savefig(EVALUATION_PLOT_SAVE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WindowViewer.__del__ at 0x16cce6700>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 335, in __del__\n",
      "    self.free()\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py\", line 330, in free\n",
      "    glfw.destroy_window(self.window)\n",
      "  File \"/Users/jamesnguyen/anaconda3/envs/torch/lib/python3.9/site-packages/glfw/__init__.py\", line 1279, in destroy_window\n",
      "    window_addr = ctypes.cast(ctypes.pointer(window),\n",
      "TypeError: _type_ must have storage info\n"
     ]
    }
   ],
   "source": [
    "total_rewards = evaluate(agent, episodes=1, human_render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
