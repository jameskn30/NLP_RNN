{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://haystack.deepset.ai/tutorials/28_structured_output_with_loop\n",
    "\n",
    "Loop-based autocorrection\n",
    "\n",
    "This tutorial uses gpt-4o-mini to change unstructured passages into JSON outputs that follow the Pydantic schema. It uses a custom OutputValidator component to validate the JSON and loop back to make corrections, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('canls.pipeline.pipeline').setLevel(logging.DEBUG)\n",
    "\n",
    "from typing import List,Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "#Haystack\n",
    "from haystack import component\n",
    "from haystack.dataclasses import ChatMessage\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_key = False\n",
    "hf_token = False \n",
    "with open(\"secrets\") as file:\n",
    "    for line in file.readlines():\n",
    "        key,value = line.strip().split(\"=\")\n",
    "        if key == 'OPENAI_API_KEY':\n",
    "            openai_key = True\n",
    "            os.environ[key]=value\n",
    "        elif key == 'HF_TOKEN':\n",
    "            hf_token = True\n",
    "            os.environ[key]=value\n",
    "assert openai_key, 'OPENAI_API_KEY not found'\n",
    "assert hf_token, 'HF_TOKEN not found'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class City(BaseModel):\n",
    "    name: str\n",
    "    country: str\n",
    "    population: int\n",
    "\n",
    "class CitiesData(BaseModel):\n",
    "    cities: List[City]\n",
    "\n",
    "#Createa a json schemd\n",
    "json_schema = CitiesData.schema_json(indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the output generated by LLM\n",
    "@component\n",
    "class OutputValidator:\n",
    "    def __init__(self, pydantic_model: BaseModel):\n",
    "        self.pydantic_model = pydantic_model\n",
    "        self.iter_count = 0\n",
    "    \n",
    "    @component.output_types(valid_replies = List[str], invalid_replies=Optional[List[str]], error_msg = Optional[str])\n",
    "    def run(self, replies: List[ChatMessage]):\n",
    "        self.iter_count += 1\n",
    "\n",
    "        try:\n",
    "            output_dict = json.loads(replies[0].text)\n",
    "            self.pydantic_model.model_validate(output_dict)\n",
    "            print(f'[OK], iter={self.iter_count} valid json')\n",
    "            return {'valid_replies': replies}\n",
    "        \n",
    "        except ValueError as e:\n",
    "            print(f'[NOT OK] error from validator {e}\\niteration={self.iter_count}\\nLLM output:{str(e)}')\n",
    "            return {'invalid_replies' : replies, 'error_msg': str(e)}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the prompt\n",
    "\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "\n",
    "\n",
    "prompt_template = [\n",
    "    ChatMessage.from_user(\n",
    "        \"\"\"\n",
    "Create a JSON object from the information present in this passage: {{passage}}.\n",
    "Only use information that is present in the passage. Follow this JSON schema, but only return the actual instances without any additional schema definition:\n",
    "{{schema}}\n",
    "Make sure your response is a dict and not a list.\n",
    "{% if invalid_replies and error_message %}\n",
    "  You already created the following output in a previous attempt: {{invalid_replies}}\n",
    "  However, this doesn't comply with the format requirements from above and triggered this Python exception: {{error_message}}\n",
    "  Correct the output and try again. Just return the corrected output without any extra explanations.\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init chat generator\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(max_runs_per_component=5)\n",
    "\n",
    "pipeline.add_component(name=\"prompt_builder\", instance = ChatPromptBuilder(template=prompt_template))\n",
    "pipeline.add_component(name=\"llm\", instance = OpenAIChatGenerator())\n",
    "pipeline.add_component(name=\"output_validator\", instance = OutputValidator(pydantic_model=CitiesData))\n",
    "\n",
    "#connect\n",
    "pipeline.connect('prompt_builder.prompt', 'llm.messages')\n",
    "pipeline.connect('llm.replies', 'output_validator')\n",
    "pipeline.connect('output_validator.invalid_replies', \"prompt_builder.invalid_replies\")\n",
    "pipeline.connect('output_validator.error_msg', \"prompt_builder.error_message\")\n",
    "\n",
    "pipeline.draw('sample.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cities': [{'country': 'Germany', 'name': 'Berlin', 'population': 3850809},\n",
      "            {'country': 'France', 'name': 'Paris', 'population': 2161000},\n",
      "            {'country': 'Portugal', 'name': 'Lisbon', 'population': 504718}]}\n"
     ]
    }
   ],
   "source": [
    "test = json.loads('''\n",
    "{\n",
    "  \"cities\": [\n",
    "    {\n",
    "      \"name\": \"Berlin\",\n",
    "      \"country\": \"Germany\",\n",
    "      \"population\": 3850809\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Paris\",\n",
    "      \"country\": \"France\",\n",
    "      \"population\": 2161000\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Lisbon\",\n",
    "      \"country\": \"Portugal\",\n",
    "      \"population\": 504718\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "''')\n",
    "\n",
    "pprint(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK], iter=1 valid json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen/anaconda3/envs/torch/lib/python3.12/site-packages/haystack/core/pipeline/pipeline.py:523: RuntimeWarning: Pipeline is stuck running in a loop. Partial outputs will be returned. Check the Pipeline graph for possible issues.\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "passage = \"Berlin is the capital of Germany. It has a population of 3,850,809. Paris, France's capital, has 2.161 million residents. Lisbon is the capital and the largest city of Portugal with the population of 504,718.\"\n",
    "result = pipeline.run({\"prompt_builder\": {\"passage\": passage, \"schema\": json_schema}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n'\n",
      " '  \"cities\": [\\n'\n",
      " '    {\\n'\n",
      " '      \"name\": \"Berlin\",\\n'\n",
      " '      \"country\": \"Germany\",\\n'\n",
      " '      \"population\": 3850809\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"name\": \"Paris\",\\n'\n",
      " '      \"country\": \"France\",\\n'\n",
      " '      \"population\": 2161000\\n'\n",
      " '    },\\n'\n",
      " '    {\\n'\n",
      " '      \"name\": \"Lisbon\",\\n'\n",
      " '      \"country\": \"Portugal\",\\n'\n",
      " '      \"population\": 504718\\n'\n",
      " '    }\\n'\n",
      " '  ]\\n'\n",
      " '}')\n"
     ]
    }
   ],
   "source": [
    "pprint(result['output_validator']['valid_replies'][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cities': [{'name': 'Berlin', 'country': 'Germany', 'population': 3850809}, {'name': 'Paris', 'country': 'France', 'population': 2161000}, {'name': 'Lisbon', 'country': 'Portugal', 'population': 504718}]}\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(result['output_validator']['valid_replies'][0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
