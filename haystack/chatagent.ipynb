{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://haystack.deepset.ai/tutorials/40_building_chat_application_with_function_calling\n",
    "\n",
    "components: \n",
    "InMemoryDocumentStore, SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder, InMemoryEmbeddingRetriever, ChatPromptBuilder, OpenAIChatGenerator, ToolInvoker\n",
    "\n",
    "OpenAPI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_key = False\n",
    "hf_token = False \n",
    "with open(\"secrets\") as file:\n",
    "    for line in file.readlines():\n",
    "        key,value = line.strip().split(\"=\")\n",
    "        if key == 'OPENAI_API_KEY':\n",
    "            openai_key = True\n",
    "            os.environ[key]=value\n",
    "        elif key == 'HF_TOKEN':\n",
    "            hf_token = True\n",
    "            os.environ[key]=value\n",
    "assert openai_key, 'OPENAI_API_KEY not found'\n",
    "\n",
    "# assert hf_token, 'HF_TOKEN not found'\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack import Document, Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "#setup OPENAI_API_KEY\n",
    "from pprint import pprint\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage.from_system('Always respond in German'),\n",
    "    ChatMessage.from_user('Briefly explain about the current state of health care in the US'),\n",
    "]\n",
    "\n",
    "chat_gen = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
    "# chat_gen = OpenAIChatGenerator(model=\"gpt-4o-mini\", streaming_callback = callback_fn)\n",
    "res = chat_gen.run(messages=messages)\n",
    "\n",
    "# document_store = InMemoryDocumentStore(embedding_similarity_function='cosine')\n",
    "# text_embeder = SentenceTransformersTextEmbedder()\n",
    "# retriever = InMemoryEmbeddingRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['replies'][0]._content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = [\n",
    "    \"The Earth is the only planet in our solar system not named after a god.\",\n",
    "    \"The Amazon rainforest produces more than 20% of the world's oxygen supply.\",\n",
    "    \"Antarctica is the driest, windiest, and coldest continent.\",\n",
    "    \"There are more than 24 time zones around the world.\",\n",
    "    \"The Great Wall of China is the longest man-made structure in the world.\",\n",
    "    \"Mount Everest is the highest point on Earth.\",\n",
    "    \"The Pacific Ocean is the largest and deepest ocean on Earth.\",\n",
    "    \"Russia is the largest country by land area.\",\n",
    "    \"The Sahara Desert is the largest hot desert in the world.\",\n",
    "    \"The Nile River is the longest river in the world.\"\n",
    "]\n",
    "\n",
    "docs = [Document(content=fact) for fact in facts]\n",
    "doc_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "pipeline.add_component(\n",
    "    instance=SentenceTransformersDocumentEmbedder(model=embedding_model), \n",
    "    name=\"doc_embedder\"\n",
    ")\n",
    "\n",
    "pipeline.add_component(\n",
    "    name=\"doc_writer\",\n",
    "    instance = DocumentWriter(document_store=doc_store) \n",
    ")\n",
    "\n",
    "pipeline.connect('doc_embedder.documents', 'doc_writer.documents')\n",
    "\n",
    "pipeline.run({'doc_embedder': {\"documents\": docs}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "haystack.components.embedders.SentenceTransformersTextEmbedder\n",
    "haystack.components.retrievers.in_memory.InMemoryEmbeddingRetriever\n",
    "haystack.components.builders.ChatPromptBuilder\n",
    "haystack.dataclasses.ChatMessage\n",
    "haystack.components.generators.chat.OpenAIChatGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = [\n",
    "    ChatMessage.from_system(\n",
    "    \"\"\"\n",
    "    Answer the questions based on the given context.\n",
    "\n",
    "    Context:\n",
    "    {% for document in documents %}\n",
    "        {{ document.content }}\n",
    "    {% endfor %}\n",
    "    Question: {{ question }}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "rag = Pipeline()\n",
    "\n",
    "rag.add_component('embedder', SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "rag.add_component('retriever', InMemoryEmbeddingRetriever(document_store = doc_store))\n",
    "rag.add_component('prompt_builder', ChatPromptBuilder(template = template))\n",
    "rag.add_component('llm', OpenAIChatGenerator(model = 'gpt-4o-mini'))\n",
    "\n",
    "rag.connect('embedder', 'retriever.query_embedding')\n",
    "rag.connect('retriever', 'prompt_builder.documents')\n",
    "rag.connect('prompt_builder.prompt', 'llm.messages')\n",
    "\n",
    "rag.draw(\"sample.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    return rag.run({'embedder': {'text': question}, 'prompt_builder': {'question' : question} })\n",
    "\n",
    "\n",
    "res = ask(\"What is some facts about earth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)\n",
    "print(res['llm']['replies'][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tools in Haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline as a tool\n",
    "from haystack.tools import Tool\n",
    "\n",
    "params = {\n",
    "    'type': 'object',\n",
    "    'properties': {\n",
    "        'question': {\n",
    "            'type': 'string',\n",
    "            'description': 'Query used for search. Infer this information from user message'\n",
    "        }\n",
    "    },\n",
    "    'required': ['question']\n",
    "}\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name='rag_pipeline_tool',\n",
    "    description=\"Get information about provided facts in the document_store\",\n",
    "    parameters=params,\n",
    "    function = ask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Function as a tool\n",
    "from typing import Annotated, Literal\n",
    "from haystack.tools import create_tool_from_function\n",
    "\n",
    "WEATHER_INFO = {\n",
    "    \"Berlin\": {\"weather\": \"mostly sunny\", \"temperature\": 7, \"unit\": \"celsius\"},\n",
    "    \"Paris\": {\"weather\": \"mostly cloudy\", \"temperature\": 8, \"unit\": \"celsius\"},\n",
    "    \"Rome\": {\"weather\": \"sunny\", \"temperature\": 14, \"unit\": \"celsius\"},\n",
    "    \"Madrid\": {\"weather\": \"sunny\", \"temperature\": 10, \"unit\": \"celsius\"},\n",
    "    \"London\": {\"weather\": \"cloudy\", \"temperature\": 9, \"unit\": \"celsius\"},\n",
    "}\n",
    "\n",
    "\n",
    "def get_weather(\n",
    "        city: Annotated[str, 'the city for which to get weather'] = 'Berlin',\n",
    "        unit: Annotated[Literal[\"Celsius\", \"Fahrenheit\"], 'the temperature unit'] = 'Berlin'):\n",
    "    '''A simple function to get the current weather for a location'''\n",
    "    if city in WEATHER_INFO:\n",
    "        return WEATHER_INFO[city]\n",
    "    else:\n",
    "        return {\"weather\": \"sunny\", \"temperature\": 21.8, \"unit\": \"fahrenheit\"}\n",
    "\n",
    "weather_tool = create_tool_from_function(get_weather)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running OpenAIChatGenerator with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.tools import ToolInvoker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"Use the tool that you're provided with. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\",\n",
    "    ),\n",
    "    ChatMessage.from_user('Can you tell me 1 interesting fact')\n",
    "]\n",
    "\n",
    "res = chat_gen.run(messages = user_messages, tools = [rag_tool, weather_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(res['replies'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_invoker = ToolInvoker(tools = [rag_tool, weather_tool])\n",
    "\n",
    "tool_res_message = tool_invoker.run(messages = res['replies'])['tool_messages']\n",
    "# ToolCallResult is a json. so you can call the tool in your code\n",
    "print(tool_res_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res['replies'])\n",
    "\n",
    "final_message = user_messages + res['replies'] + tool_res_message\n",
    "\n",
    "final_rep = chat_gen.run(messages = final_message, tools = [rag_tool, weather_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(final_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
