{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import tiktoken\n",
    "from llm.previous_chapters import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu available\")\n",
    "else:\n",
    "    print(\"no gpu\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dataset_path = os.path.join(os.getcwd(), \"datasets\", \"spam_dataset\")\n",
    "dataset_path = os.path.join(root_dataset_path, \"SMSSpamCollection\")\n",
    "assert os.path.exists(dataset_path), f\"path to dataset not exists {dataset_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path, sep=\"\\t\", header=None, names =['label', 'text'])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#undersample for testing\n",
    "def create_balanced_dataset(df: pd.DataFrame):\n",
    "    num_spam = df[df[\"label\"] == \"spam\"].shape[0]\n",
    "    ham_subset = df[df[\"label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    #make # of ham == # spam\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"label\"] == 'spam']])\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train df shape =  (3900, 2)\n",
      "valid df shape =  (557, 2)\n",
      "test df shape =  (1115, 2)\n"
     ]
    }
   ],
   "source": [
    "def random_split(df, train_frac, valid_frac):\n",
    "    #shuffle\n",
    "    df = df.sample(frac = 1, random_state = 123).reset_index(drop=True)\n",
    "\n",
    "    train_idx = int(train_frac * len(df))\n",
    "    val_idx = train_idx + int(valid_frac * len(df))\n",
    "    #the rest is test\n",
    "\n",
    "    train_df = df[:train_idx]\n",
    "    val_df = df[train_idx:val_idx]\n",
    "    test_df = df[val_idx:]\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = random_split(df, 0.7, 0.1)\n",
    "\n",
    "print(\"train df shape = \", train_df.shape)\n",
    "print(\"valid df shape = \", val_df.shape)\n",
    "print(\"test df shape = \", test_df.shape)\n",
    "\n",
    "train_df.to_csv(os.path.join(root_dataset_path, \"sample_train.csv\"), index = False)\n",
    "val_df.to_csv(os.path.join(root_dataset_path, \"sample_val.csv\"), index = False)\n",
    "test_df.to_csv(os.path.join(root_dataset_path, \"sample_test.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map label\n",
    "balanced_df['label'] = balanced_df['label'].map({'ham': 0, 'spam': 1})\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad tokens with longest token len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "pad_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "print(pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2]\n",
    "a = a + [0] * 3\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv, tokenizer, pad_token_id, max_len = 200):\n",
    "        self.df = pd.read_csv(csv)\n",
    "        self.df.drop(0, inplace=True)\n",
    "        self.df['label'] = self.df['label'].map({'ham': 0, 'spam': 1})\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "        self.encoded_text = [self._pad_tokens(tokenizer.encode(text)) for text in self.df['text']]\n",
    "\n",
    "    def _pad_tokens(self, tokens):\n",
    "        tokens = tokens + [self.pad_token_id] * (self.max_len - len(tokens))\n",
    "        return tokens[:self.max_len]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.encoded_text[index]\n",
    "        y = self.df.iloc[index]['label']\n",
    "        \n",
    "        return (torch.tensor(X).long(), torch.tensor(y).long())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text)\n",
    "    \n",
    "    def get_dataloader(self, batch_size, num_workers, drop_last = False):\n",
    "        return DataLoader(dataset = self, batch_size=batch_size, num_workers=num_workers, drop_last = drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 5122,  1736,  1077,   415, 17442,   272,   338,  2802,  3804,  1497,\n",
      "          938,  1755,    13, 12472,   329,   607,   290,  1641,    13, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]), tensor(0))\n",
      "3899\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(os.path.join(root_dataset_path, \"sample_train.csv\"), tokenizer, pad_id, max_len = 100)\n",
    "train_dataset.df.head()\n",
    "print(train_dataset[0])\n",
    "print(len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556\n",
      "1114\n"
     ]
    }
   ],
   "source": [
    "val_dataset = SpamDataset(os.path.join(root_dataset_path, \"sample_val.csv\"), tokenizer, pad_id, max_len = 100)\n",
    "print(len(val_dataset))\n",
    "test_dataset = SpamDataset(os.path.join(root_dataset_path, \"sample_test.csv\"), tokenizer, pad_id, max_len = 100)\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader len =  487\n",
      "X shape = torch.Size([8, 100]), y shape = torch.Size([8])\n",
      "val loader len =  70\n",
      "test loader len =  140\n"
     ]
    }
   ],
   "source": [
    "#Get dataloader\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = train_dataset.get_dataloader(batch_size=batch_size, num_workers=num_workers, drop_last=True) #drop last unmatched element for training\n",
    "val_loader = val_dataset.get_dataloader(batch_size=batch_size, num_workers=num_workers, drop_last=False)\n",
    "test_loader = test_dataset.get_dataloader(batch_size=batch_size, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "print(\"train loader len = \", len(train_loader))\n",
    "sample = next(iter(train_loader))\n",
    "print(f\"X shape = {sample[0].shape}, y shape = {sample[1].shape}\")\n",
    "print(\"val loader len = \", len(val_loader))\n",
    "print(\"test loader len = \", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking the correctness of loader \n",
    "xshape = None\n",
    "yshape = None\n",
    "for i, (X,y) in enumerate(train_loader):\n",
    "    if xshape == None or yshape == None: \n",
    "        xshape = X.shape\n",
    "        yshape = y.shape\n",
    "    else:\n",
    "        if xshape != X.shape: raise Exception(f\"shape not consistent, new shape = {X.shape}, old shape = {xshape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader len =  487\n",
      "val loader len =  70\n",
      "test loader len =  140\n"
     ]
    }
   ],
   "source": [
    "print(\"train loader len = \", len(train_loader))\n",
    "print(\"val loader len = \", len(val_loader))\n",
    "print(\"test loader len = \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pretrained model. \n",
    "Code in Chapt 5, load from book's code, my GPT code is messed up\n",
    "because my GPTCode is slightly different than theirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = torch.load(os.path.join(\"output\", 'gpt.torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are the world's only two states that can afford the kind of\n"
     ]
    }
   ],
   "source": [
    "text = \"we are the world\" \n",
    "tokens = text_to_token_ids(text, tokenizer)\n",
    "\n",
    "output_tokens = generate(gpt, tokens, max_new_tokens=10, context_size=BASE_CONFIG[\"context_length\"], temperature=2.0, top_k=10)\n",
    "\n",
    "decoded = token_ids_to_text(output_tokens, tokenizer)\n",
    "\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are the world's largest producer and supplier. The company employs nearly\n"
     ]
    }
   ],
   "source": [
    "# Sample instruction\n",
    "text2 = (\"Is the following spam, answer with yes or no\", \n",
    "\"'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\")\n",
    "\n",
    "tokens = text_to_token_ids(text, tokenizer)\n",
    "\n",
    "output_tokens = generate(gpt, tokens, max_new_tokens=10, context_size=BASE_CONFIG[\"context_length\"], temperature=2.0, top_k=10)\n",
    "\n",
    "decoded = token_ids_to_text(output_tokens, tokenizer)\n",
    "\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(path):\n",
    "    gpt = torch.load(path)\n",
    "\n",
    "    num_classes = 2\n",
    "    #Freeze model params for finetuning\n",
    "    for param in gpt.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    gpt.out_head = nn.Linear(\n",
    "        BASE_CONFIG[\"emb_dim\"],\n",
    "        out_features=num_classes\n",
    "    )\n",
    "\n",
    "    #NOTE from the book: \n",
    "    # training the output layer we just added is sufficient. \n",
    "    # However, as I found in experiments, finetuning additional layers can \n",
    "    # noticeably improve the predictive performance of the finetuned model.\n",
    "\n",
    "    for p in gpt.trf_blocks[-1].parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in gpt.final_norm.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    gpt.to(device)\n",
    "    return gpt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape =  torch.Size([2, 5])\n",
      "output shape =  torch.Size([2, 5, 2])\n",
      "labels =  tensor([1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test new model architecture\n",
    "text = [\n",
    "    \"This definitely a spam email\", \n",
    "    \"this is not spam email\"\n",
    "    ]\n",
    "ids = [text_to_token_ids(t, tokenizer).squeeze(0) for t in text]\n",
    "ids = torch.vstack(ids).to(device)\n",
    "\n",
    "print('input shape = ', ids.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = gpt(ids)\n",
    "\n",
    "print('output shape = ', preds.shape)\n",
    "# we want to use last output token [-1] to get optimize\n",
    "# probas = torch.softmax(preds[:, -1, :], dim = -1)\n",
    "# print(f\"probas ({probas.shape})= \", probas)\n",
    "logits = preds[:, -1, :]\n",
    "label = torch.argmax(logits, dim = -1)\n",
    "\n",
    "print('labels = ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(dataloader:DataLoader, model:GPTModel, device, num_batches = None):\n",
    "    model.eval()\n",
    "\n",
    "    correct_pred, num_examples = 0,0\n",
    "\n",
    "    if num_batches != None: \n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    else:\n",
    "        num_batches = len(dataloader)\n",
    "    \n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        if i >= num_batches: break\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(X)[:,-1,:]\n",
    "        \n",
    "        preds = torch.argmax(logits, dim = -1)\n",
    "\n",
    "        num_examples += preds.shape[0]\n",
    "\n",
    "        correct_pred += (preds == y).sum().item()\n",
    "    \n",
    "    return correct_pred / num_examples\n",
    "\n",
    "def calc_loss_loader(dataloader:DataLoader, model:GPTModel, device, num_batches = None):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, num_examples = 0,0\n",
    "\n",
    "    if num_batches != None: \n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    else:\n",
    "        num_batches = len(dataloader)\n",
    "    \n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        if i >= num_batches: break\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(X)[:,-1,:]\n",
    "\n",
    "        num_examples += preds.shape[0]\n",
    "        total_loss  += calc_loss_batch(X, y, model, device)\n",
    "    \n",
    "    return total_loss / num_examples\n",
    "\n",
    "def calc_loss_batch(X: torch.tensor, y, model: torch.tensor, device: GPTModel):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    logits = model(X)[:, -1, :]\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def eval_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    \n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc =  0.1375\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device = device, num_batches= 10)\n",
    "print(\"train acc = \", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss =  tensor(4.8201, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(train_loader))\n",
    "train_loss = calc_loss_batch(X, y, gpt, device = device)\n",
    "print(\"train loss = \", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss =  tensor(2.3591, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_loss = calc_loss_loader(train_loader, gpt, device = device, num_batches= 10)\n",
    "print(\"train loss = \", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss =  tensor(2.4640, device='cuda:0')\n",
      "val loss =  tensor(2.1279, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = eval_model(gpt, train_loader,  val_loader, device, eval_iter=5)\n",
    "print('train loss = ', train_loss)\n",
    "print('val loss = ', val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(\n",
    "    model: GPTModel, train_loader: DataLoader, val_loader: DataLoader, \n",
    "    optimizer, device, tokenizer, num_epochs = 10, eval_freq = 1, eval_iter = 1):\n",
    "\n",
    "    train_losses, val_losses, train_accs, val_accs = [],[],[],[]\n",
    "    examples_seen, global_steps = 0,-1\n",
    "    loop = tqdm(range(num_epochs))\n",
    "\n",
    "    try:\n",
    "        for e in loop:\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for X,y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = calc_loss_batch(X, y, model, device)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                examples_seen += X.shape[0]\n",
    "\n",
    "                global_steps += 1\n",
    "            \n",
    "                if global_steps % eval_freq == 0: \n",
    "                    train_loss, val_loss = eval_model(model, train_loader, val_loader, device, eval_iter)\n",
    "\n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "                \n",
    "                    print(f\"epochs = {e}, global step ={global_steps}, train_loss={train_loss:.3f}, val_loss={val_loss:.3f}\")\n",
    "                \n",
    "            train_acc = calc_accuracy_loader(train_loader, model, device,num_batches=eval_iter)\n",
    "            val_acc = calc_accuracy_loader(val_loader, model, device,num_batches=eval_iter)\n",
    "\n",
    "            loop.set_description(f\"train acc = {train_acc:.3f}, val_acc = {val_acc:.3f}\")\n",
    "\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "    except Exception as e:\n",
    "        print('x shape = ', X.shape)\n",
    "        print('y shape = ', y.shape)\n",
    "        raise e \n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 0, global step =0, train_loss=0.413, val_loss=0.412\n",
      "epochs = 0, global step =100, train_loss=0.187, val_loss=0.184\n",
      "epochs = 0, global step =200, train_loss=0.120, val_loss=0.115\n",
      "epochs = 0, global step =300, train_loss=0.058, val_loss=0.023\n",
      "epochs = 0, global step =400, train_loss=0.029, val_loss=0.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc = 0.988, val_acc = 0.988:  33%|███▎      | 1/3 [00:26<00:52, 26.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 1, global step =500, train_loss=0.028, val_loss=0.028\n",
      "epochs = 1, global step =600, train_loss=0.027, val_loss=0.039\n",
      "epochs = 1, global step =700, train_loss=0.026, val_loss=0.030\n",
      "epochs = 1, global step =800, train_loss=0.026, val_loss=0.032\n",
      "epochs = 1, global step =900, train_loss=0.026, val_loss=0.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc = 0.975, val_acc = 0.975:  67%|██████▋   | 2/3 [00:52<00:26, 26.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 2, global step =1000, train_loss=0.022, val_loss=0.024\n",
      "epochs = 2, global step =1100, train_loss=0.020, val_loss=0.028\n",
      "epochs = 2, global step =1200, train_loss=0.029, val_loss=0.007\n",
      "epochs = 2, global step =1300, train_loss=0.020, val_loss=0.023\n",
      "epochs = 2, global step =1400, train_loss=0.019, val_loss=0.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc = 0.975, val_acc = 0.963: 100%|██████████| 3/3 [01:19<00:00, 26.51s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gpt = load_model(os.path.join(\"output\", 'gpt.torch'))\n",
    "start_time = time.time()\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr = 5e-5, weight_decay=0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "print('device = ', device)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "train(gpt, train_loader, val_loader, optimizer = optimizer, \n",
    "tokenizer=tokenizer, device = device, eval_freq = 100, eval_iter = 10, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "                                                                  #A\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "#B\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    fig.tight_layout()                                            #C\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(len(train_accs), examples_seen, train_accs, val_accs, label = \"accuracy\")\n",
    "plot_values(len(train_accs), examples_seen, train_losses, val_losses, label = \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79bd7a421c8fa56dafc3a7d5d6c958c1d5d73add0a33880867e978912e0c760c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
