{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llm.components import MultiHeadAttention, GPTModel\n",
    "from torch import nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = { \n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'embed_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "#BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "#sample\n",
    "batch = [\n",
    "    torch.tensor(tokenizer.encode(\"Every effort moves you\")),\n",
    "    torch.tensor(tokenizer.encode(\"Every day holds a\")),\n",
    "]\n",
    "\n",
    "batch = torch.stack(batch, dim = 0) #stach the elements along dim = 0 \n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[0.1954, 0.3820, 0.5577],\n",
      "        [0.4217, 0.0082, 0.6459],\n",
      "        [0.4851, 0.2461, 0.4296],\n",
      "        [0.8600, 0.8492, 0.4592],\n",
      "        [0.2715, 0.8509, 0.4746]])\n",
      "tensor([[-1.2366,  0.0247,  1.2119],\n",
      "        [ 0.2389, -1.3265,  1.0876],\n",
      "        [ 0.9606, -1.3783,  0.4177],\n",
      "        [ 0.7358,  0.6778, -1.4136],\n",
      "        [-1.0866,  1.3270, -0.2405]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "layer_norm = nn.LayerNorm(3)\n",
    "\n",
    "x = layer_norm(x)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original = \n",
      " tensor([[[ 0.8634,  0.6201,  1.2964, -0.4779],\n",
      "         [ 1.1506, -1.5850, -0.4435, -0.3155],\n",
      "         [ 0.8499, -0.2465, -0.1780, -0.2128]],\n",
      "\n",
      "        [[-0.1979,  1.1857,  0.2796, -0.0232],\n",
      "         [ 0.2557, -1.3692, -0.3413, -0.7217],\n",
      "         [ 0.4607, -0.1092,  0.8432,  0.7599]]])\n",
      "gelu = \n",
      " tensor([[[ 0.6959,  0.4541,  1.1701, -0.1512],\n",
      "         [ 1.0069, -0.0895, -0.1458, -0.1187],\n",
      "         [ 0.6819, -0.0992, -0.0764, -0.0885]],\n",
      "\n",
      "        [[-0.0834,  1.0460,  0.1706, -0.0114],\n",
      "         [ 0.1537, -0.1170, -0.1251, -0.1698],\n",
      "         [ 0.3121, -0.0498,  0.6749,  0.5899]]])\n",
      "relu = \n",
      " tensor([[[0.8634, 0.6201, 1.2964, 0.0000],\n",
      "         [1.1506, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8499, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 1.1857, 0.2796, 0.0000],\n",
      "         [0.2557, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4607, 0.0000, 0.8432, 0.7599]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3,4)\n",
    "print('original = \\n', x)\n",
    "gelu = nn.GELU()\n",
    "print('gelu = \\n', gelu(x))\n",
    "print('relu = \\n', torch.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(d_in, 4 * d_in),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_in, d_in),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "x = torch.randn(2,3,768)\n",
    "\n",
    "ffn = FeedForward(768)\n",
    "\n",
    "y = ffn(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        vocab_size = cfg['vocab_size']\n",
    "        context_length = cfg['context_length']\n",
    "        embed_dim = cfg['embed_dim']\n",
    "        n_heads = cfg['n_heads']\n",
    "        n_layers = cfg['n_layers']\n",
    "        drop_rate = cfg['drop_rate']\n",
    "        qkv_bias = cfg['qkv_bias']\n",
    "\n",
    "        self.ff = FeedForward(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, embed_dim, context_length, n_heads, drop_rate, qkv_bias = qkv_bias)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "test_block = TransformerBlock(GPT_CONFIG)\n",
    "\n",
    "x = torch.rand(2,3,768)\n",
    "\n",
    "y = test_block(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        vocab_size = cfg['vocab_size']\n",
    "        context_length = cfg['context_length']\n",
    "        embed_dim = cfg['embed_dim']\n",
    "        n_heads = cfg['n_heads']\n",
    "        n_layers = cfg['n_layers']\n",
    "        drop_rate = cfg['drop_rate']\n",
    "        qkv_bias = cfg['qkv_bias']\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(context_length, embed_dim)\n",
    "        self.drop_emb = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(n_layers)])\n",
    "        self.final_norm = nn.LayerNorm(embed_dim)\n",
    "        self.out_head = nn.Linear(embed_dim, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor)->torch.Tensor:\n",
    "\n",
    "        batch, num_tokens = x.shape\n",
    "\n",
    "        token_embeddings = self.token_emb(x)\n",
    "\n",
    "        pos_embeddings = self.pos_emb(torch.arange(num_tokens, device = x.device))\n",
    "\n",
    "        x = token_embeddings + pos_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate_text_simple(self, ids, max_new_tokens, context_size):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = ids[:, -context_size:]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self(idx_cond)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "            #The softmax function is monotonic, meaning it preserves the order of its inputs when transformed into outputs\n",
    "            probas = torch.softmax(logits, dim = -1) #not neccessary. Explained in chapt 4 page 144\n",
    "\n",
    "            idx_next = torch.argmax(probas, dim = -1, keepdim = True)\n",
    "\n",
    "            ids = torch.cat((ids, idx_next), dim = -1)\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        return ids\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0,1000, (2,4))\n",
    "out = model(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of params; 163,009,536\n"
     ]
    }
   ],
   "source": [
    "# Calculate params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"total number of params; {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1443, 0.4197, 0.9350, 0.4903, 0.1319],\n",
      "        [0.3416, 0.0168, 0.6511, 0.4401, 0.4722]])\n",
      "tensor([[0.9350, 0.4903, 0.1319],\n",
      "        [0.6511, 0.4401, 0.4722]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,5)\n",
    "print(a)\n",
    "print(a[:, -3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22, 46, 81, 76]])\n",
      "tensor([[22, 46, 81, 76, 15]])\n",
      "tensor([[22, 46, 81, 76, 15, 48]])\n",
      "tensor([[22, 46, 81, 76, 15, 48, 96]])\n",
      "tensor([[22, 46, 81, 76, 15, 48, 96, 43]])\n",
      "tensor([[22, 46, 81, 76, 15, 48, 96, 43, 93]])\n",
      "tensor([[22, 46, 81, 76, 15, 48, 96, 43, 93, 12]])\n",
      "tensor([[22, 46, 81, 76, 15, 48, 96, 43, 93, 12, 25]])\n",
      "tensor([[22, 46, 81, 76, 15, 48, 96, 43, 93, 12, 25, 43]])\n",
      "tensor([[22, 46, 81, 76, 15, 48, 96, 43, 93, 12, 25, 43, 98]])\n",
      "tensor([[22, 46, 81, 76, 15, 48, 96, 43, 93, 12, 25, 43, 98, 23]])\n"
     ]
    }
   ],
   "source": [
    "idx = torch.randint(0,100,(1,4))\n",
    "print(idx)\n",
    "\n",
    "for _ in range(10):\n",
    "    idx_cond = idx[:, -3:]\n",
    "    idx_next = torch.randint(0,100,(1,1))\n",
    "\n",
    "    idx = torch.cat([idx, idx_next], dim = 1)\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        #The softmax function is monotonic, meaning it preserves the order of its inputs when transformed into outputs\n",
    "        probas = torch.softmax(logits, dim = -1) #not neccessary. Explained in chapt 4 page 144\n",
    "\n",
    "        idx_next = torch.argmax(probas, dim = -1, keepdim = True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, ids, max_new_tokens, context_size, temperature = 0.0, top_k = None):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k:\n",
    "            #use top_k sampling\n",
    "            top_logits, top_pos = torch.topk(logits, top_k)\n",
    "\n",
    "            logits = torch.where(\n",
    "                logits < top_logits[:,-1],\n",
    "                input = torch.tensor(-float('inf')).to(logits.device), #if condition is true\n",
    "                other = logits #if condition is false\n",
    "            )\n",
    "        \n",
    "        if temperature > 0.0:\n",
    "            #use temp scaling\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim = -1)\n",
    "            # sample from probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim = -1, keepdim = True) \n",
    "        \n",
    "        # if idx_next == EOS_ID: # stop generaton if EOS is predicted\n",
    "        #     break\n",
    "            \n",
    "        ids = torch.cat((ids, idx_next), dim = 1)\n",
    "    \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original input =  tensor([[31373,    11,   616,  1438,   318]])\n",
      "tensor([[31373,    11,   616,  1438,   318,  9691, 32850,  9691, 14878, 32850,\n",
      "         32850, 20788, 14838, 27446, 27446]])\n",
      "hello, my name is mining MF mining Pokemon MF MF appetite unlockedepheph\n"
     ]
    }
   ],
   "source": [
    "sample_input = \"hello, my name is\"\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "idx = tokenizer.encode(sample_input)\n",
    "idx = torch.tensor(idx).unsqueeze(0)\n",
    "print('original input = ', idx)\n",
    "# pred_idx = model.generate_text_simple(idx, 10, 100)\n",
    "pred_idx = generate(model, idx, 10, 100)\n",
    "# print(pred_idx)\n",
    "# decoded = tokenizer.decode(pred_idx.squeeze(0).tolist())\n",
    "# print(decoded)\n",
    "#=====\n",
    "pred_idx = generate(model, idx, 10, 100, temperature=2.0, top_k = 10)\n",
    "print(pred_idx)\n",
    "decoded = tokenizer.decode(pred_idx.squeeze(0).tolist())\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding strategies: this is from chapter 5 but I put it here for convinient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.randn(10)\n",
    "print('next token logits = ', next_token_logits)\n",
    "probas = torch.softmax(next_token_logits, dim = 0)\n",
    "print('probas = ', probas)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(next_token_id)\n",
    "\n",
    "print(\"multi nomial\")\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top K sampling\n",
    "\n",
    "print(\"logits = \", next_token_logits)\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(top_logits)\n",
    "print(top_pos)\n",
    "\n",
    "new_logits = torch.where(\n",
    "    #top logits is sorted descending, \n",
    "    # last element is minmum value \n",
    "    condition=next_token_logits < top_logits[-1], \n",
    "    input=torch.tensor(float('-inf')), #when condition is true\n",
    "    other=next_token_logits, #when condition is false\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_scaling(logits, temperature):\n",
    "    \n",
    "    # Temperatures greater than 1 result in more uniformly distributed token probabilities, \n",
    "    # and temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions.\n",
    "    # temperature of 1 is the same as not using\n",
    "    scaled = logits / temperature\n",
    "    return torch.softmax(scaled, dim = 0)\n",
    "\n",
    "print(temperature_scaling(torch.randn(5), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79bd7a421c8fa56dafc3a7d5d6c958c1d5d73add0a33880867e978912e0c760c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
