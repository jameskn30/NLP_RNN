{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llm.components import MultiHeadAttention, GPTModel\n",
    "from torch import nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG = { \n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'embed_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "#BPE tokenizer\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "#sample\n",
    "batch = [\n",
    "    torch.tensor(tokenizer.encode(\"Every effort moves you\")),\n",
    "    torch.tensor(tokenizer.encode(\"Every day holds a\")),\n",
    "]\n",
    "\n",
    "batch = torch.stack(batch, dim = 0) #stach the elements along dim = 0 \n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[0.4697, 0.0384, 0.9540],\n",
      "        [0.5054, 0.3758, 0.5965],\n",
      "        [0.8573, 0.3089, 0.2593],\n",
      "        [0.4583, 0.9476, 0.1771],\n",
      "        [0.7077, 0.8432, 0.2631]])\n",
      "tensor([[-0.0472, -1.2004,  1.2476],\n",
      "        [ 0.1415, -1.2886,  1.1471],\n",
      "        [ 1.4102, -0.6137, -0.7965],\n",
      "        [-0.2178,  1.3190, -1.1012],\n",
      "        [ 0.4159,  0.9626, -1.3784]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "layer_norm = nn.LayerNorm(3)\n",
    "\n",
    "x = layer_norm(x)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original = \n",
      " tensor([[[-1.0694,  0.8192, -0.6045, -1.9654],\n",
      "         [ 0.8322,  0.8429, -0.9024, -0.4621],\n",
      "         [-0.9452, -0.4569, -1.3467,  1.4069]],\n",
      "\n",
      "        [[-0.9557,  0.6398, -0.5620,  0.4425],\n",
      "         [-0.4646, -0.0563,  1.1198,  0.1933],\n",
      "         [-0.7056,  0.1581, -1.5766,  0.5918]]])\n",
      "gelu = \n",
      " tensor([[[-0.1523,  0.6502, -0.1649, -0.0485],\n",
      "         [ 0.6636,  0.6747, -0.1655, -0.1488],\n",
      "         [-0.1628, -0.1480, -0.1199,  1.2947]],\n",
      "\n",
      "        [[-0.1621,  0.4727, -0.1613,  0.2969],\n",
      "         [-0.1492, -0.0269,  0.9726,  0.1114],\n",
      "         [-0.1695,  0.0890, -0.0906,  0.4278]]])\n",
      "relu = \n",
      " tensor([[[0.0000, 0.8192, 0.0000, 0.0000],\n",
      "         [0.8322, 0.8429, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.4069]],\n",
      "\n",
      "        [[0.0000, 0.6398, 0.0000, 0.4425],\n",
      "         [0.0000, 0.0000, 1.1198, 0.1933],\n",
      "         [0.0000, 0.1581, 0.0000, 0.5918]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3,4)\n",
    "print('original = \\n', x)\n",
    "gelu = nn.GELU()\n",
    "print('gelu = \\n', gelu(x))\n",
    "print('relu = \\n', torch.relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(d_in, 4 * d_in),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_in, d_in),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "x = torch.randn(2,3,768)\n",
    "\n",
    "ffn = FeedForward(768)\n",
    "\n",
    "y = ffn(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        vocab_size = cfg['vocab_size']\n",
    "        context_length = cfg['context_length']\n",
    "        embed_dim = cfg['embed_dim']\n",
    "        n_heads = cfg['n_heads']\n",
    "        n_layers = cfg['n_layers']\n",
    "        drop_rate = cfg['drop_rate']\n",
    "        qkv_bias = cfg['qkv_bias']\n",
    "\n",
    "        self.ff = FeedForward(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, embed_dim, context_length, n_heads, drop_rate, qkv_bias = qkv_bias)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "test_block = TransformerBlock(GPT_CONFIG)\n",
    "\n",
    "x = torch.rand(2,3,768)\n",
    "\n",
    "y = test_block(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        vocab_size = cfg['vocab_size']\n",
    "        context_length = cfg['context_length']\n",
    "        embed_dim = cfg['embed_dim']\n",
    "        n_heads = cfg['n_heads']\n",
    "        n_layers = cfg['n_layers']\n",
    "        drop_rate = cfg['drop_rate']\n",
    "        qkv_bias = cfg['qkv_bias']\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(context_length, embed_dim)\n",
    "        self.drop_emb = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(n_layers)])\n",
    "        self.final_norm = nn.LayerNorm(embed_dim)\n",
    "        self.out_head = nn.Linear(embed_dim, vocab_size, bias = False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor)->torch.Tensor:\n",
    "\n",
    "        batch, num_tokens = x.shape\n",
    "\n",
    "        token_embeddings = self.token_emb(x)\n",
    "\n",
    "        pos_embeddings = self.pos_emb(torch.arange(num_tokens, device = x.device))\n",
    "\n",
    "        x = token_embeddings + pos_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0,1000, (2,4))\n",
    "out = model(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of params; 163,009,536\n"
     ]
    }
   ],
   "source": [
    "# Calculate params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"total number of params; {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0856, 0.6913, 0.4619, 0.3784, 0.4054],\n",
      "        [0.6121, 0.0259, 0.9582, 0.5707, 0.4206]])\n",
      "tensor([[0.4619, 0.3784, 0.4054],\n",
      "        [0.9582, 0.5707, 0.4206]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,5)\n",
    "print(a)\n",
    "print(a[:, -3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8, 41, 89, 69]])\n",
      "tensor([[ 8, 41, 89, 69, 53]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76, 90]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76, 90, 29]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76, 90, 29, 10]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76, 90, 29, 10, 20]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76, 90, 29, 10, 20, 22]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76, 90, 29, 10, 20, 22, 15]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76, 90, 29, 10, 20, 22, 15,  8]])\n",
      "tensor([[ 8, 41, 89, 69, 53, 76, 90, 29, 10, 20, 22, 15,  8, 95]])\n"
     ]
    }
   ],
   "source": [
    "idx = torch.randint(0,100,(1,4))\n",
    "print(idx)\n",
    "\n",
    "for _ in range(10):\n",
    "    idx_cond = idx[:, -3:]\n",
    "    idx_next = torch.randint(0,100,(1,1))\n",
    "\n",
    "    idx = torch.cat([idx, idx_next], dim = 1)\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        #The softmax function is monotonic, meaning it preserves the order of its inputs when transformed into outputs\n",
    "        probas = torch.softmax(logits, dim = -1) #not neccessary. Explained in chapt 4 page 144\n",
    "\n",
    "        idx_next = torch.argmax(probas, dim = -1, keepdim = True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original input =  tensor([[31373,    11,   616,  1438,   318]])\n",
      "tensor([[31373,    11,   616,  1438,   318,  2499, 47413,  3903,  4336, 21597,\n",
      "         48297,  4549, 34509, 29029, 34430]])\n",
      "hello, my name is workshenyband fanmissible EmergingicagoLINherty cartel\n"
     ]
    }
   ],
   "source": [
    "sample_input = \"hello, my name is\"\n",
    "model = GPTModel(GPT_CONFIG)\n",
    "idx = tokenizer.encode(sample_input)\n",
    "idx = torch.tensor(idx).unsqueeze(0)\n",
    "print('original input = ', idx)\n",
    "pred_idx = model.generate_text_simple(idx, 10, 100)\n",
    "print(pred_idx)\n",
    "decoded = tokenizer.decode(pred_idx.squeeze(0).tolist())\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
