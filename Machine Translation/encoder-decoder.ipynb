{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import unittest\n",
    "from d2l import torch as d2l\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format='%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "###  Process the english spanish translation\n",
    "### link: https://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - start building the dataset\n",
      "INFO - done tokenizing source and target, source len = 141370, target len = 141370\n",
      "INFO - done building source and target arrays\n",
      "DEBUG - source array shape (141370,10)\n",
      "DEBUG - source vocab len = 9538\n",
      "DEBUG - valid_len shape  = torch.Size([141370])\n",
      "DEBUG - target array shape = (141370,10)\n",
      "DEBUG - target vocab len = 16679\n"
     ]
    }
   ],
   "source": [
    "class SpanishDataset(Dataset):\n",
    "\n",
    "    def setup_logger(self, level = logging.DEBUG):\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.setLevel(level)\n",
    "\n",
    "\n",
    "    def log(self, message:str, level: str = 'debug'):\n",
    "        if level == 'debug':\n",
    "            self.logger.debug(message)\n",
    "        if level == 'info':\n",
    "            self.logger.info(message)\n",
    "        if level == 'warning':\n",
    "            self.logger.warning(message)\n",
    "        if level == 'error':\n",
    "            self.logger.error(message)\n",
    "        \n",
    "\n",
    "    def __init__(self, debug = False, num_steps = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setup_logger()\n",
    "\n",
    "        self.DATASET_PATH = '../data/translation/spa.txt'\n",
    "        assert os.path.exists(self.DATASET_PATH), 'English spanish dataset is not found'\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "\n",
    "        self.log('start building the dataset')\n",
    "\n",
    "        with open(self.DATASET_PATH, 'r') as file:\n",
    "            for idx, line in enumerate(file.readlines()):\n",
    "                processed = self._preprocess(line)\n",
    "                source_tokens, target_tokens = self._tokenize(processed)\n",
    "                self.source.append(source_tokens)\n",
    "                self.target.append(target_tokens)\n",
    "\n",
    "        self.log(f'done tokenizing source and target, source len = {len(self.source)}, target len = {len(self.target)}', 'info')\n",
    "\n",
    "        (self.source_array, self.target_array, self.valid_len, self.label_target_array), self.source_vocab, self.target_vocab = \\\n",
    "            self._build_arrays(self.source, self.target)\n",
    "        \n",
    "        shape2d = lambda a: f'({len(a)},{len(a[0])})'\n",
    "        self.log(f'done building source and target arrays', 'info')\n",
    "        self.log(f'source array shape {shape2d(self.source_array)}') \n",
    "        self.log(f'source vocab len = {len(self.source_vocab)}')\n",
    "        self.log(f'valid_len shape  = {self.valid_len.shape}')\n",
    "        self.log(f'target array shape = {shape2d(self.target_array)}')\n",
    "        self.log(f'target vocab len = {len(self.target_vocab)}')\n",
    "        \n",
    "    def _preprocess(self, text):\n",
    "        # from D2L processing step in chapter 10\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        # Insert space between words and punctuation marks\n",
    "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "            for i, char in enumerate(text.lower())]\n",
    "        return ''.join(out)\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        # Tokenization method in D2L processing step in chapter 10\n",
    "        if len(text.split('\\t')[:-1]) == 2:\n",
    "            part = text.split('\\t')[:-1]\n",
    "            src = [token for token in f'{part[0]} <eos>'.split(' ') if token]\n",
    "            tgt = [token for token in f'{part[1]} <eos>'.split(' ') if token]\n",
    "            return src, tgt\n",
    "        else:\n",
    "            return '',''\n",
    "    def _build_arrays(self, source, target):\n",
    "        '''\n",
    "        @params:\n",
    "            source_raw: list[list[string]], source sequence, eg: [['a', 'b', '<eos>'], ...]\n",
    "            target_rwa: list[list[string]], target sequence\n",
    "        @return\n",
    "            (\n",
    "                source_array: list[list[int]]\n",
    "                target_array_with_bos: list[list[int]]\n",
    "                valid_len: list[int]\n",
    "                target_array_with_eos: list[list[int]]\n",
    "            ),\n",
    "            source_vocab: Vocab\n",
    "            target_vocab: Vocab\n",
    "        '''\n",
    "        #pad with <pad> token if sequence len < time step, else truncate\n",
    "        #NOTE: in the book, they just truncated without adding <eos> at the end, \n",
    "        # I don't think that is correct\n",
    "        pad_or_truncate = lambda sentence, numstep: \\\n",
    "            sentence[:numstep - 1] + ['<eos>'] if len(sentence) > numstep \\\n",
    "                else sentence + ['<pad>'] * (numstep - len(sentence))\n",
    "\n",
    "        def _build_array(sequence, is_target = False):\n",
    "            '''\n",
    "            @params:\n",
    "                sentence: string\n",
    "                is_target: boolean, if sentence is target, append <bos> to beginning of sentence\n",
    "            @return\n",
    "                array: list[str] \n",
    "                vocab: Vocab object\n",
    "            '''\n",
    "            new_sequence = [ ]\n",
    "            for sentence in sequence:\n",
    "                sentence = pad_or_truncate(sentence, self.num_steps)\n",
    "                if is_target: \n",
    "                    sentence = ['<bos>'] + sentence\n",
    "                \n",
    "                new_sequence.append(sentence)\n",
    "\n",
    "            vocab = d2l.Vocab(new_sequence, min_freq = 2)\n",
    "\n",
    "            #calculate valid_len for training later\n",
    "            array = torch.tensor([vocab[sentence] for sentence in new_sequence])\n",
    "            valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "            return array,vocab,valid_len\n",
    "\n",
    "        source_array, source_vocab, valid_len = _build_array(source)        \n",
    "        target_array, target_vocab, _ = _build_array(target, is_target= True)        \n",
    "\n",
    "        return (source_array, target_array[:,:-1], valid_len, target_array[:,1:]), source_vocab, target_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        @return\n",
    "            int: length of the english - spanish pairs\n",
    "        '''\n",
    "        return len(self.source_array)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        '''\n",
    "        @params:\n",
    "            idx: int, datapoint index\n",
    "        @return\n",
    "            source_array, target_array, valid_len, label_target_array\n",
    "        '''\n",
    "        return (self.source_array[idx], self.target_array[idx], self.valid_len[idx], self.label_target_array[idx])\n",
    "    \n",
    "    def get_dataloader(self, batch_size = 32):\n",
    "        return DataLoader(self, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "dataset = SpanishDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['put', 'it', 'on', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.source[1000])\n",
    "source = dataset.source\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3825,   84,  201,  202,  202,  202,  202,  202,  202,  202])\n",
      "[\"i'm\", 'warm', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.source_array[0])\n",
    "print(dataset.source_vocab.to_tokens(dataset.source_array[500].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = torch.Size([32, 10]),\t data = tensor([4370, 5115, 3555, 8491, 4793,   84,  201,  202,  202,  202])\n",
      "shape = torch.Size([32, 10]),\t data = tensor([ 189, 6750, 2399, 9021, 9360,   78,  190,  191,  191,  191])\n",
      "shape = torch.Size([32]),\t data = 7\n",
      "shape = torch.Size([32, 10]),\t data = tensor([6750, 2399, 9021, 9360,   78,  190,  191,  191,  191,  191])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = dataset.get_dataloader()\n",
    "sample = next(iter(train_dataloader))\n",
    "\n",
    "for data in sample:\n",
    "    print(f'shape = {data.shape},\\t data = {data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, num_hiddens, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(input_size = embed_size, num_layers = num_layers, hidden_size = num_hiddens, dropout = dropout)\n",
    "        #custom initialization\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        #why x.t(), still confused in the book\n",
    "        emb = self.embedding(x.t())\n",
    "        output, state = self.rnn(emb)\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape =  torch.Size([10, 32, 16])  dtype =  torch.float32\n",
      "state shape =  torch.Size([2, 32, 16])  dtype =  torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Test with sample\n",
    "vocab_size, embed_size, num_layers, num_hiddens = 1000, 8, 2, 16\n",
    "batch_size, num_steps = 32, 10\n",
    "\n",
    "x = torch.randint(0,vocab_size,(batch_size, num_steps))\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_size, num_layers, num_hiddens)\n",
    "\n",
    "output, state = encoder(x)\n",
    "\n",
    "print('output shape = ',output.shape, ' dtype = ', output.dtype)\n",
    "print('state shape = ', state.shape, ' dtype = ', state.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout = 0):\n",
    "        #note that the vocab size in decoder is target language vocab size, not source language vocab size\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(input_size = embed_size + num_hiddens, num_layers = num_layers, hidden_size = num_hiddens, dropout = dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        #custom init module\n",
    "    \n",
    "    def init_state(self, enc_output, *args):\n",
    "        '''\n",
    "        use encoder's output to initialize state\n",
    "        @params:\n",
    "            encoder_output\n",
    "        @return:\n",
    "            decoder_input\n",
    "        '''\n",
    "        return enc_output\n",
    "    \n",
    "    def forward(self, x, state):\n",
    "        emb = self.embedding(x.t())\n",
    "\n",
    "        enc_output, enc_state = state\n",
    "        #context variable \n",
    "        context = enc_output[-1]\n",
    "        context = context.repeat(emb.shape[0], 1, 1)\n",
    "        emb_and_context = torch.cat((emb, context), -1)\n",
    "        dec_output, dec_state = self.rnn(emb_and_context, enc_state)\n",
    "\n",
    "        #pass to dense layer and swap back (batch_size, num_steps)\n",
    "        y_pred = self.dense(dec_output).swapaxes(0,1)\n",
    "        # print(x.shape)\n",
    "        # print('embedding shape = ', emb.shape)\n",
    "        # print('context shape = ', context.shape)\n",
    "        # print('emb and context shape = ', emb_and_context.shape)\n",
    "        # print('decoder output shape = ', output.shape)\n",
    "        # print('decoder state shape = ', dec_state.shape)\n",
    "        return y_pred, (dec_output, dec_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape =  torch.Size([32, 10, 1000])\n",
      "dec state shape  =  torch.Size([2, 32, 16])\n"
     ]
    }
   ],
   "source": [
    "# Test with sample\n",
    "vocab_size, embed_size, num_layers, num_hiddens = 1000, 8, 2, 16\n",
    "batch_size, num_steps = 32, 10\n",
    "\n",
    "x = torch.randint(0,vocab_size,(batch_size, num_steps))\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_size, num_layers, num_hiddens)\n",
    "\n",
    "enc_output, enc_state = encoder(x)\n",
    "\n",
    "decoder = Decoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "decoder_state = decoder.init_state((enc_output, enc_state))\n",
    "\n",
    "y, (dec_output, dec_state) = decoder(x, decoder_state)\n",
    "\n",
    "print('y_pred shape = ', y.shape)\n",
    "print('dec state shape  = ', dec_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq Translation with Encoder-Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x_enc, x_dec, *args):\n",
    "        enc_outputs, enc_state = self.encoder(x_enc, *args)\n",
    "        dec_init_state = self.decoder.init_state((enc_outputs, enc_state), *args)\n",
    "        #only returns decoder output, decode state is not used for final pred\n",
    "        return decoder(x_dec, dec_init_state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq output shape =  torch.Size([32, 10, 2000])\n"
     ]
    }
   ],
   "source": [
    "#Test seq 2 seq \n",
    "# Test with sample\n",
    "vocab_size_source, embed_size, num_layers, num_hiddens = 1000, 8, 2, 16\n",
    "vocab_size_target = 2000\n",
    "batch_size, num_steps = 32, 10\n",
    "\n",
    "x_enc = torch.randint(0,vocab_size_source,(batch_size, num_steps))\n",
    "x_dec = torch.randint(0,vocab_size_target,(batch_size, num_steps))\n",
    "\n",
    "encoder = Encoder(vocab_size_source, embed_size, num_layers, num_hiddens)\n",
    "decoder = Decoder(vocab_size_target, embed_size, num_hiddens, num_layers)\n",
    "\n",
    "# enc_output, enc_state = encoder(x)\n",
    "\n",
    "# decoder_state = decoder.init_state((enc_output, enc_state))\n",
    "\n",
    "# y, (dec_output, dec_state) = decoder(x, decoder_state)\n",
    "\n",
    "seq2seq = Seq2Seq(encoder, decoder)\n",
    "\n",
    "output = seq2seq(x_enc, x_dec)\n",
    "print('seq2seq output shape = ', output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F...\n",
      "======================================================================\n",
      "FAIL: test4 (__main__.SpanishDatasetTest.test4)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_s/m9kt_szd4qq9brspl9hd44mc0000gn/T/ipykernel_51936/1831953952.py\", line 15, in test4\n",
      "    self.assertEqual('foo', 'foo1')\n",
      "AssertionError: 'foo' != 'foo1'\n",
      "- foo\n",
      "+ foo1\n",
      "?    +\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x15f37ca10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SpanishDatasetTest(unittest.TestCase):\n",
    "\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "    \n",
    "    def test4(self):\n",
    "        self.assertEqual('foo', 'foo1')\n",
    "\n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
