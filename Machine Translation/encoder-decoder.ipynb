{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import unittest\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "###  Process the english spanish translation\n",
    "### link: https://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start building the dataset\n",
      "done tokenizing source and target, source len = 141370, target len = 141370\n",
      "done building source and target arrays\n",
      "==========\n",
      "STATS\n",
      "==========\n",
      "source array shape (141370,10)\n",
      "source vocab len =  9538\n",
      "valid_len shape  =  torch.Size([141370])\n",
      "target array shape =  (141370,10)\n",
      "target vocab len =  16679\n"
     ]
    }
   ],
   "source": [
    "class SpanishDataset(Dataset):\n",
    "    def __init__(self, debug = False, num_steps = 10, batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.DATASET_PATH = '../data/translation/spa.txt'\n",
    "        assert os.path.exists(self.DATASET_PATH), 'English spanish dataset is not found'\n",
    "\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        print('start building the dataset')\n",
    "\n",
    "        with open(self.DATASET_PATH, 'r') as file:\n",
    "            for idx, line in enumerate(file.readlines()):\n",
    "                processed = self._preprocess(line)\n",
    "                source_tokens, target_tokens = self._tokenize(processed)\n",
    "                self.source.append(source_tokens)\n",
    "                self.target.append(target_tokens)\n",
    "\n",
    "        print(f'done tokenizing source and target, source len = {len(self.source)}, target len = {len(self.target)}')\n",
    "\n",
    "        (self.source_array, self.target_array, self.valid_len, self.label_target_array), self.source_vocab, self.target_vocab = \\\n",
    "            self._build_arrays(self.source, self.target)\n",
    "        \n",
    "        print(f'done building source and target arrays')\n",
    "        shape2d = lambda a: f'({len(a)},{len(a[0])})'\n",
    "        print(\"=\" * 10)\n",
    "        print('STATS')\n",
    "        print(\"=\" * 10)\n",
    "        print(f'source array shape', shape2d(self.source_array)) \n",
    "        print('source vocab len = ', len(self.source_vocab))\n",
    "        print('valid_len shape  = ', self.valid_len.shape)\n",
    "        print('target array shape = ', shape2d(self.target_array))\n",
    "        print('target vocab len = ', len(self.target_vocab))\n",
    "\n",
    "        \n",
    "    def _preprocess(self, text):\n",
    "        # from D2L processing step in chapter 10\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        # Insert space between words and punctuation marks\n",
    "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "            for i, char in enumerate(text.lower())]\n",
    "        return ''.join(out)\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        # Tokenization method in D2L processing step in chapter 10\n",
    "        if len(text.split('\\t')[:-1]) == 2:\n",
    "            part = text.split('\\t')[:-1]\n",
    "            src = [token for token in f'{part[0]} <eos>'.split(' ') if token]\n",
    "            tgt = [token for token in f'{part[1]} <eos>'.split(' ') if token]\n",
    "            return src, tgt\n",
    "        else:\n",
    "            return '',''\n",
    "    def _build_arrays(self, source, target):\n",
    "        '''\n",
    "        @params:\n",
    "            source_raw: list[list[string]], source sequence, eg: [['a', 'b', '<eos>'], ...]\n",
    "            target_rwa: list[list[string]], target sequence\n",
    "        @return\n",
    "            (\n",
    "                source_array: list[list[int]]\n",
    "                target_array_with_bos: list[list[int]]\n",
    "                valid_len: list[int]\n",
    "                target_array_with_eos: list[list[int]]\n",
    "            ),\n",
    "            source_vocab: Vocab\n",
    "            target_vocab: Vocab\n",
    "        '''\n",
    "        #pad with <pad> token if sequence len < time step, else truncate\n",
    "        #NOTE: in the book, they just truncated without adding <eos> at the end, \n",
    "        # I don't think that is correct\n",
    "        pad_or_truncate = lambda sentence, numstep: \\\n",
    "            sentence[:numstep - 1] + ['<eos>'] if len(sentence) > numstep \\\n",
    "                else sentence + ['<pad>'] * (numstep - len(sentence))\n",
    "\n",
    "        def _build_array(sequence, is_target = False):\n",
    "            '''\n",
    "            @params:\n",
    "                sentence: string\n",
    "                is_target: boolean, if sentence is target, append <bos> to beginning of sentence\n",
    "            @return\n",
    "                array: list[str] \n",
    "                vocab: Vocab object\n",
    "            '''\n",
    "            new_sequence = [ ]\n",
    "            for sentence in sequence:\n",
    "                sentence = pad_or_truncate(sentence, self.num_steps)\n",
    "                if is_target: \n",
    "                    sentence = ['<bos>'] + sentence\n",
    "                \n",
    "                new_sequence.append(sentence)\n",
    "\n",
    "            vocab = d2l.Vocab(new_sequence, min_freq = 2)\n",
    "\n",
    "            #calculate valid_len for training later\n",
    "            array = torch.tensor([vocab[sentence] for sentence in new_sequence])\n",
    "            valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "            return array,vocab,valid_len\n",
    "\n",
    "        source_array, source_vocab, valid_len = _build_array(source)        \n",
    "        target_array, target_vocab, _ = _build_array(target, is_target= True)        \n",
    "\n",
    "        return (source_array, target_array[:,:-1], valid_len, target_array[:,1:]), source_vocab, target_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        @return\n",
    "            int: length of the english - spanish pairs\n",
    "        '''\n",
    "        return len(self.source_array)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        '''\n",
    "        @params:\n",
    "            idx: int, datapoint index\n",
    "        @return\n",
    "            source_array, target_array, valid_len, label_target_array\n",
    "        '''\n",
    "        return (self.source_array[idx], self.target_array[idx], self.valid_len[idx], self.label_target_array[idx])\n",
    "    \n",
    "    def get_dataloader(self):\n",
    "        return DataLoader(self, batch_size = self.batch_size, shuffle = True)\n",
    "    \n",
    "dataset = SpanishDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['put', 'it', 'on', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.source[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = dataset.source\n",
    "target = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', '.', '<eos>'] \t ['ve', '.', '<eos>']\n",
      "['go', '.', '<eos>'] \t ['vete', '.', '<eos>']\n",
      "['go', '.', '<eos>'] \t ['vaya', '.', '<eos>']\n",
      "['go', '.', '<eos>'] \t ['váyase', '.', '<eos>']\n",
      "['hi', '.', '<eos>'] \t ['hola', '.', '<eos>']\n",
      "['run', '!', '<eos>'] \t ['¡corre', '!', '<eos>']\n",
      "['run', '!', '<eos>'] \t ['¡corran', '!', '<eos>']\n",
      "['run', '!', '<eos>'] \t ['¡huye', '!', '<eos>']\n",
      "['run', '!', '<eos>'] \t ['¡corra', '!', '<eos>']\n",
      "['run', '!', '<eos>'] \t ['¡corred', '!', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for s, t in zip(source[:10], target[:10]):\n",
    "    print(s, '\\t', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3966,   91,  222,  223,  223,  223,  223,  223,  223,  223,  223,  223,\n",
      "         223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,\n",
      "         223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,\n",
      "         223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,\n",
      "         223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,\n",
      "         223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,\n",
      "         223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,\n",
      "         223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,  223,\n",
      "         223,  223,  223,  223])\n",
      "[3966, 91, 222, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223]\n",
      "[\"i'm\", 'warm', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(source_array[0])\n",
    "print(source_array[0].tolist())\n",
    "print(source_vocab.to_tokens(source_array[500].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish Dataset Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F...\n",
      "======================================================================\n",
      "FAIL: test4 (__main__.SpanishDatasetTest.test4)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/_s/m9kt_szd4qq9brspl9hd44mc0000gn/T/ipykernel_51936/1831953952.py\", line 15, in test4\n",
      "    self.assertEqual('foo', 'foo1')\n",
      "AssertionError: 'foo' != 'foo1'\n",
      "- foo\n",
      "+ foo1\n",
      "?    +\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x15f37ca10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SpanishDatasetTest(unittest.TestCase):\n",
    "\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "    \n",
    "    def test4(self):\n",
    "        self.assertEqual('foo', 'foo1')\n",
    "\n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
